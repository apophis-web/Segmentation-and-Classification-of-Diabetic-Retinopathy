{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATA MINING PROJECT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMFjdW1okThq",
        "outputId": "af621a3a-36ac-4d6e-a4f3-ae977adcc8c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from skimage import io\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.datasets import make_blobs\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "a0ebbYBHmBkt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "CpcoFHuQ4ppR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Data_set_DataMining/Data_labels.csv')"
      ],
      "metadata": {
        "id": "5AB3OrwbleqL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "aSzbl_GPlyZZ",
        "outputId": "e7fa4ee9-c21a-4afd-e6fb-7d0a45a81de5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Image_name  level\n",
              "0        IMG_1      4\n",
              "1        IMG_2      4\n",
              "2        IMG_3      4\n",
              "3        IMG_4      4\n",
              "4        IMG_5      4\n",
              "..         ...    ...\n",
              "511    IMG_512      2\n",
              "512    IMG_513      2\n",
              "513    IMG_514      2\n",
              "514    IMG_515      2\n",
              "515    IMG_516      2\n",
              "\n",
              "[516 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec2a048e-d6b4-42da-b131-1eb2cb0d941e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_name</th>\n",
              "      <th>level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IMG_1</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IMG_2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IMG_3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IMG_4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IMG_5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>IMG_512</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>IMG_513</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>IMG_514</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>IMG_515</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>IMG_516</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>516 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec2a048e-d6b4-42da-b131-1eb2cb0d941e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ec2a048e-d6b4-42da-b131-1eb2cb0d941e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ec2a048e-d6b4-42da-b131-1eb2cb0d941e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k_list = []\n",
        "for i in df['Image_name']:\n",
        "  image = cv2.imread('/content/drive/MyDrive/Data_set_DataMining/Images/' + i + \".jpg\")\n",
        "  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  k_list.append(image.flatten())"
      ],
      "metadata": {
        "id": "iyeZbMM69fEW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = KMeans()\n",
        "v = KElbowVisualizer(m, k=(2, 20))\n",
        "v.fit(np.array(k_list))\n",
        "v.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "JE4xjAfe9HJO",
        "outputId": "3adc4b5a-fd5a-499f-9c7c-117af147772e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhIAAAFnCAYAAADzOqBQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd1iV5f/A8ffZh72UIYgKiuZEAclRiqKoaWlaWqm0M8t+TUu/uUdm+a2vZtqwNDWz3OY2G+YKETBzoLgAEWVvOOP5/UGcJEAQOB7G/bqurqvzzM/9eDjP57mfe8gkSZIQBEEQBEGoBrmlAxAEQRAEof4SiYQgCIIgCNUmEglBEARBEKpNJBKCIAiCIFSbSCQEQRAEQag2kUgIgiAIglBtIpEQaqxt27YMGDCAsLAw7r//fl544QWioqJM6xctWsS6detue4yDBw9y7dq1Oz53TEwMZ8+eBWDNmjV8/PHHd3yMily9epUJEyYQFhZGWFgYw4cPZ//+/bV2/Ko6duwYHTt2ZNCgQWX+A1iyZAn/+c9/AOjXrx/Hjx83azzx8fEMGDCAhx56qEbH+Xesf/zxB/fddx9xcXGm9Q8//HCZ/T799FPatm1LQkJCjc5fU8nJyUyePJkBAwYwcOBAhg8fzqZNm0zr27Zty/Xr16t17IsXLxIREXHH+1Xlb00QapvS0gEIDcPq1atxd3dHkiR2797NxIkTWbx4MUFBQbzxxhuV7r9y5UpefPFFmjVrdkfn3bhxIwEBAbRr146xY8dWN/xyvfnmmzz00EMsX74cKE5awsPD2bVrFx4eHrV6rsp4eHiwe/fuu3rOikRGRtK0aVO+/fbbWjvm+fPneeONN1iyZAm+vr6m5WlpaVy6dIlWrVqZlu3fvx9nZ+daO3d15ObmMnbsWIYNG8Z7772HQqEgLi6O559/Hr1ez6OPPlqj4+/fvx+9Xk9QUNAd7VeVvzVBqG0ikRBqlUwmY/DgweTk5LBo0SK+++473nnnHby9vZk4cSJr1qxh7dq1SJKEra0t7733Hjt27ODo0aNcvHiRt956i/79+zNv3jyOHTuGXC6nT58+vPXWWygUCtNT6vbt2xk2bBhbt27lwIEDpKWlkZOTw/Xr15k3bx7Xrl1j2rRpJCQkoFKpePbZZxk+fDgJCQmMGTOG559/nh9++IGMjAymTJnCkCFDypQlNjaWLl26mD536dKFPXv24OrqCsCWLVtYtmwZAJ07d2bevHmo1Wp27drF0qVL0ev1uLq6MnfuXLy9vVmyZAnJycmcPXuWoUOHEh4eztKlS9m+fTtFRUX079+fKVOmoFAoavRvcPToUebOnUt6ejrDhw/ntddeAyg3rsTERBYvXmx6in3uueewt7dn0aJFAAwbNowFCxbQoUMHAKKiovjwww/JycnhwQcfZNu2bVUu75NPPlluvMnJybz44ovMmTMHf3//Uuvuv/9+duzYwcsvvwzAuXPnsLe3JyMjw7RNZGQk8+fPJysrCycnJxYtWkTz5s0xGo3MmTOHw4cPo9PpCAgIYP78+ahUKt555x2aNWtGVFQUly9fpmXLlnz66adYWVmV+x1t06ZNqbi2bNmCi4sLr7zyimmZr68vS5cuRaVSldp206ZNbNu2jZUrV5b5/Mcff/Dee+9RWFiIJEm88soraDQaPvvsM1QqFVlZWbzzzjusX7+er7/+mqKiIvz9/Zk/fz5arZZ33nkHBwcHDh8+zMSJE/n1119Nf2v9+vXj+eefZ8OGDVy/fp2hQ4fyzjvvALB8+XJWrVpFs2bNePjhh1mxYgUHDhyo8ndMEEqRBKGG/Pz8pKSkpFLLUlJSpHbt2kn5+fnS22+/LS1dulTKzs6WAgMDpezsbEmSJGnnzp3S559/LkmSJIWEhEgRERGSJEnSZ599Jj333HOSTqeT8vPzpZEjR0pbtmwxbffuu++azjN27FjTusWLF0tTp06VJEmSnn76aWn58uWSJElSQkKCFBAQIMXHx0vx8fFS+/btpdWrV5tiGDBgQLnlmjRpkhQSEiKtWrVKunDhQql18fHx0r333itdv35dMhqN0ksvvSR98cUXUmJiohQQECBdvnxZkiRJWrFihRQeHm6Kr3fv3lJqaqokSZK0efNm6YEHHpCysrIknU4nPf/886a4bnX06FEpNDS0wut/a7lDQkKkCRMmSHq9XkpJSZGCgoKkM2fOVBhXfn6+FBAQIBUVFUl6vV56+OGHpeHDh0uSJEmZmZlScHCwZDAYSp1v48aNpjLdSXn/LSQkRPrll1+kBx98UPr+++/LXX/o0CEpLCzMtGzRokXSunXrpJCQECk+Pl7Kzs6WgoKCpN9//12SJEnavn27NGLECEmSJGn37t3S0KFDpaKiIqmgoEAaPHiw6bvy9ttvS4MHD5bS09MlnU4nPfjgg9LWrVtv+x291SuvvCJ9+umnFf6bSNI/fxe3Xq9/X7+HH35YOnbsmCRJknTp0iXp9ddfN8W3dOlSSZIkKSIiQurRo4d0/fp1SZIkadq0adKCBQtM2w0bNkwqKCgos19ISIj0+uuvS3q9Xrp+/brUoUMHKSkpSYqNjZUCAgKk5ORkqaCgQBo7dqwUEhJy27IIwu3UuzYSsbGxhIaGsmbNmttu99133zFy5EjGjBnDnj177lJ0QglbW1uMRiO5ubmmZRqNBplMxoYNG0hJSWHw4ME899xzZfb95ZdfePTRR1EqlWi1WoYNG8ahQ4dM6/v27Xvbc+t0Og4fPszjjz8OgKenJ8HBwRw9ehQAvV5vevfeoUOHCttmfPDBBzzxxBNs376doUOH0q9fP9OT+6FDh+jatStubm7IZDIWLVrEk08+yaFDhwgODqZFixYAPPLIIxw7dgy9Xg8U12qUVMv//PPPjBw5Ejs7O5RKJY888gh79+4tN5akpKQy7SMWLFhQ7rbDhg1DoVDg4uJCUFAQUVFRFcalVCpp164dZ86c4ezZs/j4+ODo6EhycjInTpyge/fuyOUV/0zcSXnLM2vWLDIyMkhNTS13vbe3NzY2Npw6dQqAPXv2MHDgQNP6yMhI3Nzc6NWrFwBDhw7l6tWrXLt2jbCwMDZu3IhKpUKj0dCpUyfi4+NN+/bp0wdHR0eUSiV+fn4kJSVV+TuamZlJkyZNKixXVbm4uLBlyxbi4uJo2bKlqSboVgcOHGDIkCG4ubkB8Nhjj5X6nvTo0QONRlPu8Uu+C25ubri4uJCUlERERATdu3fH1dUVjUbDyJEja1wOoXGrV6828vLymDNnDj169LjtdqmpqXz11Vds374dgPDwcPr06YNWq70bYQpgeqVgZ2dnWqZSqVi5ciXLly9nyZIltG3blhkzZtC2bdtS+6alpeHg4GD67ODgUOpGc+u68mRkZCBJUqlz29vbk5aWBoBCocDa2hoAuVyO0Wgs9zgajYZnnnmGZ555hqysLHbv3s38+fPx8vIiPT0de3v7UtsCZZbb2dkhSRLp6ellYs/OzmbFihWsX78eAIPBUOFN907aSNx6DDs7O7KyspDJZBXGFRwcTFRUFJIk0bVrV27evElkZCSnT5/m3nvvve257qS85XnmmWfo27cvo0aNolOnTqaE4FZDhw5lx44dGI1GvLy8SpUvKyuL+Ph4U8NTALVaTVpaGlqtljlz5nD69GlkMhkpKSmEh4eXirWEQqHAYDBU+Tvq5OREcnLybctWFfPnz2fZsmU89dRTaLVaXn/99VJlgeLvyb59+/j9998BkCQJnU5nWn+7a2xra1umjFlZWaX2KUlQBKG66lWNhFqt5osvvjC9owa4cOEC48ePJzw8nIkTJ5KVlUViYiI+Pj5oNBo0Gg3t2rUjJibGgpE3Pnv27KF79+6o1epSy9u3b8/ixYs5cuQIvXv3ZsaMGWX2bdKkSal34BkZGXf09Ofk5IRcLiczM7PUMVxcXKp8jLS0tFK1IPb29jz66KPcd999xMbG4uTkZLpZAuTk5JCSkoKLi0up2DMzM5HL5Tg5OZU5h6urKxMmTGD37t3s3r2bffv2mZKKmri13JmZmTg4ONw2ruDgYKKjo4mMjKRbt2507dqVEydOEBkZWWnSfiflLU/btm3x9PTkvffe48033yy3dmjIkCHs3buXXbt2lWnL4urqio+Pj+ka7t69m8OHD9OxY0c++ugjlEol27dvZ/fu3fTp06dKMVXlOxocHMzevXuR/jXn4YkTJ9i2bVupZXK5HIPBYPqclZVl+v8mTZowbdo0fvvtN6ZPn86UKVNK1eKVlHHEiBGm8u3Zs4fffvutSmUpj62tLXl5eabPN27cqPaxBAHqWSJRUtV9qzlz5jB79mxWrVpFr169WLt2Ld7e3sTGxpKWlkZubi5RUVEVVp0KtUv6u9fGqlWrTI38Spw7d45XXnmFoqIi1Go1HTt2RCaTAcX/ttnZ2UDxq4sNGzZgMBjIy8tj69atFd4Ebt3v1mW9e/c23ZSvXr3K8ePH6dmzZ5XLUVBQwCuvvMLBgwdNy65cuUJMTAyBgYH06dOHEydOkJCQgCRJzJgxgw0bNtCrVy+OHz9uqkL/7rvv6NWrF0pl2cq//v37s3XrVvLz803bbt68ucoxVqTk6T01NZXIyEgCAwNvG5e/vz9nz54lNjYWPz8//P39OXHiBKmpqaV6S5TnTsp7O3379mXkyJFMmjSJoqKiUuvc3Nzw8PBg165dDBgwoNS6Ll26cPPmTdODQnx8PG+99RaSJJGamoqfnx9qtZqzZ88SFRVV6gZantt9R281fPhwdDod8+bNM8V74cIFU6PgW7m6unLp0iUKCwvJz8831SzpdDrGjRtnupF36NABpVKJXC4v9b3u168fe/fuNdWo7d+/n88//7xK17U8nTt35tixY6SlpVFUVMSWLVuqfSxBgHr2aqM8J0+eZNq0aQAUFRXRqVMnHB0deeutt5g4cSJNmzaldevWZZ4chNo1btw4FAoFOTk5+Pr68vnnn9OpU6dS2/j5+eHl5cXQoUNRqVTY2Ngwffp0AMLCwnj99dd55ZVXGDduHPHx8TzwwAPIZDIGDRrE4MGDyz1vaGgoH3zwAfHx8aWqcWfNmsW7777Lpk2bUKlUzJ07Fw8PjyqPPdCsWTOWLVvG4sWLmTt3rqkF/5QpU0w9OWbPnk14eDgKhYJOnTrx1FNPodFomDt3LhMnTkSn0+Hl5cWcOXMqjP38+fOMGDECKG4PMG/evHK3LWkj8W8LFy4ss6xTp06MGjWKtLQ0wsPDad26NUCFcanVatzc3FAoFMjlcuzt7SkqKqJr166VXid3d/cql7cyr732GtHR0cyePZu5c+eWWvfAAw/w66+/lnqNAqDValm8eDFz5swhNzcXlUrF//3f/yGTyXj66ad5++232bRpE4GBgbz99tv85z//oXPnzhXGcLvv6L/Pu3r1aj744AMGDRqERqPB3t6eqVOn0r9//1LbBgcH06VLF8LCwvDy8qJ///4cOnQIlUrFqFGjTL1Z5HI57777LlZWVoSEhPDmm2+aetVMmDCBcePGYTQacXFxYdasWdW6xlCcSIwYMYIRI0bg4eHBkCFDTD1KBKE6ZFI9vMMuWbIEJycnxo4dS8+ePTl06FC5Tw0lXn/9dcaPH1+ma5kgCEJjJEmS6Tfzl19+4eOPPxY1E0K11atXG+Vp166d6X3hjh07OHLkCHq9nnHjxlFYWMjNmzc5c+YMHTt2tHCkgiAIlpeWlsa9995LYmIikiSxa9cu8ZAl1Ei9qpE4deoU77//PomJiSiVStzc3Hj11VdZtGgRcrkcjUbDokWLcHR0ZO3atfzwww/IZDImT55caaMxQRCExmLdunV89dVXyGQyfHx8mDdv3h01RhaEW9WrREIQBEEQhLql3r/aEARBEATBcupFrw29Xk9qaiparfa2o+wJgiAIQkNhNBopKCjAxcXljrtU3011N7JbpKamWnzKYEEQBEGwlLo8Amm9SCRKBqHy8vIyDW18t5UM1tNY1OfylvTLv5O+8fW5vHeqMZUVRHkbuoZc3ry8PBISEur89A71IpEoeZ1hbW1danz8u82S57aE+lrekgGR7jT++lre6mhMZQVR3oauoZe3rr/SrxeJhCDcifbt21s6BEEQhEajbqc5giAIgiDUaaJGQmhwSubCMNeMr3q9vsKpx+uLf0+M1dCJ8jZs9bW8MpkMpVJ52yke6gNRIyEIdyA7O7ve/miV8PX1tXQId5Uob8NWn8trMBjIzs6udFbauk7USAhCFen1ehQKhcV6DtUWnU6HWq22dBh3jShvw1bfy6vVasnJycFgMJSZgr6+aJQ1ErP2xDBrj3mqvYWGy2g01ulBYQRBqJ9UKhUGg8HSYVSbWX8VFy5cSGRkJHq9nhdeeIGBAwea1h09epT//ve/yOVyWrVqxbx58+5KF5dZe2KYvfek6fOMsC5mP6cgCIIgVKS+t5EwWyJx9OhRzp8/z/r160lPT2fEiBGlEonp06fzzTff4O7uziuvvMLBgwfp06ePucIByiYRJf8vkglBEAShupIyLnDxZjQ5BenYap3waeqPh2NrS4d115gtkQgKCqJz584A2Nvbk5+fX+od0KZNm7C1tQXA2dmZ9PR0c4UClE0iSohkouGZNGmSpUMQBKEOMeeNPinjAjHxB0yfswvSTJ8bSzJhtkTi1kZpGzZs4P777y/VkKQkibhx4waHDh3i//7v/8wVSoVJRAmRTDQszz77rKVDEAShjqjOjV6SJIySAaPRgEHSo5SrUCqKG3Rm5adQpC/AKBkwGPX8mfArBbpc5DIFauU/Q1lfvBktEonasn//fjZs2MBXX31VZl1qaioTJkxgxowZODk5VXqs2NjYasVw7dqNKmxzjchI/W23iYyMrNb56ytR3rJ8fX3R6XR3IRrzys3NtXQIFRo8eDAfffQR7dq1q7Vj1uXymoMo7z/OXTuOwVB80y/U5yJJRiTg8PktNLX1poNHX7QqG/RGHUcvbSxOIKTSDR/bNO2Op2Px9zEm/meyClJM6zLykwEJlVyDQqYyLc/MTa3yv4NOpyMuLq7qBa5jzJpIHDx4kOXLl/Pll1+WGQs9JyeH5557jldffZXevXtX6Xh+fn7VGlP9swBodptaiekDO1daGxEZGUlAQMAdn7u+qs/lnTBhAgDLly+v8j5VKW/J+BH1ravZsWPHePfdd9m3bx/Hjh1j6tSp/PTTTxaL5/vvv+fRRx8td11mZiYpKSl07NgRjUZTK+fz9/cv1ZitoKCAxx9/nGnTptXK8eua3NxcbGxsWLNmDZs2bSI2NpahQ4eyYMECoPh7PHPmTI4cOUJGRgbe3t68/vrrZdqo7dixg08++YSkpCSaNGnCggULCAwMBODNN9/k6NGj5OXl0bRpU5599lkeeeQRALp27VrqOP++3uPGjSM6OtrUA8rV1ZU9e/aUKcfly5cZNmwYYWFhfPjhhxXGHRgYiI2NTYXHfv3DUUgYyS5MJT8/D73egEwmQ6vRYGfljEarwkZrg1EyYGvlgFymQC5ToJAruRh3iWPH/mDJH9+hz1WxYMECWvl0Ircgm40bNnH2zDl8uznj4GyLq6sbCuviWvcLFy6QePkG7z79WZnrU56ioiI6depU5rclOzu72g/Qd5PZEons7GwWLlzIypUrcXR0LLN+wYIFhIeHc//995srhFJKEoV/JxNPd/cVrzQamCNHjlg6BKECBoOBhQsXVphIxMbG4u3tXWtJBMChQ4dMN5rc3Fx69+7NoEGDau34NWHOsQNcXV2ZOHEiBw8epLCw0LRcr9fj4eHB6tWradasGb/++iuvvvoq27dvx8vLCyi+Zh9++CEfffQRnTt35ubNm6WO/cILLzB//nzUajVxcXGMHz+ee+65h44dOxIVFWXarqLrPX369NveWAFmz55Np06dKo37+++/p02bNhUe+9D5DWTlp5KbnY++CLw9fcnLyyPiSAw9H3oSW21xbbhcpqBXm1H/7HfoEMvmbSi+BlP+uQZuzm7k5eVhJzvK/P+8hMwmn9/+3MSF83HYdLZFo9HQrJknD973NO+91q7M9WmIzNbfcufOnaSnp/Pqq68ybtw4xo0bxyeffMK+ffvIz89ny5YtbNiwwbRu/fr15grFZEZYF6YP7FxqmaqeDgAiCOXZv38/w4YNo3///jz99NOkpaWVu937779PWFgYgwYN4sSJE6blu3btYujQoQwaNIjx48dz9epV+vTpw5UrV4Div+uOHTuSn58PwNdff83cuXPLHP+HH35g8ODBDBw4kCeeeILExEQAnnrqKbKzsxk0aBDx8fFl9jt37pxpSuj8/HzeeOMNXn755Vqrqt+7dy/Ozs6mJ+vKxMfH8/zzzxMcHEy3bt146qmnTOt+/PFHHnjgAbp06UJoaCjHjh1DkiQ+//xzQkJCCAwM5P/+7//Izs427fPDDz/w1FNPMXXqVIKCgvj666+B4lqaIUOGEBAQwLPPPktqamqNyzpw4EBCQ0PLPMhZW1szadIkvLy8kMvlhISE4OXlxV9//WXaZsmSJUycOBF/f3/kcjlubm64ubmZ1rdp08b09CyTyZDJZFy9erVMDHd6vUvs2LEDOzs7evToUWncZ86cKfcYRslAak4iPk39MRqNXI9PxcPVC6VSib29PTYGd7Zu3VphDLe7BrfG4unUhr6dR6IvkJGbm4ed1pkebR/Au0m7Sq9PQ2G2GonRo0czevToCtefOnXKXKe+rZLaB6Mk8dWxC3wXdYlFDwVgpRIDDQnVUzK3x79NmjTJ1PBzwoQJ5daUBAYGsmLFCgBWrVrFf//73zLbVHXOkPj4eCZPnsx3332Hn58fn332GTNnzuSJJ54otV1SUhIdO3bk7bff5vvvv2f27Nls2bKFa9euMW3aNDZu3EiLFi346quvmD59OsHBwURFRdGiRQsiIiLo0KEDJ0+eJDg4mOPHjzNixIhSx09NTWX27Nns27cPd3d3pkyZwqeffsq8efOYP38+AwcOZPfu3eWWITY2Fj8/P+Lj45k0aRKhoaG89NJLpV5NvPDCCxW2ZwkICOCzzz6r8Bpt3ryZ4cOHV7nf/uTJkxk6dCjLli1Dr9ebfre++uorNm7cyMKFC7nnnns4f/48NjY2fPzxx5w4cYL169fj4ODAq6++ytKlS3nnnXeA4kQpOjqasWPHMnfuXPR6PcuXL2fv3r0sW7YMDw8PZs2axccff8ycOXNqpcyVSUlJ4fLly7RuXdww0GAwcOrUKfr168eAAQMoLCwkNDSUyZMno9X+05hw5syZbN68mYKCAtq3b19u9/2KrveiRYv48MMPadWqFa+99hrBwcGmdTk5OSxevJhVq1bxww8/VBq3j49PmWMv/uQj+j/qT3t/Xwb4P4GLzI+s1N+xtrLGVuuIT1N/EpsoiIiIKPfYVb0GJVR6RzYtO8yWLZNNQ3ZX5fo0FI3y7lmSTOgMRt4/8Bdb/oznsW6tLByVINTMb7/9Rvfu3U1P9GPGjKFXr1489thjpbZTq9UMHjwYKG7YOG3aNAoLCzl06BDBwcG0aNECgEceeYQPPviAWbNmER0dzfDhw4mJieGxxx7jxIkTBAcHExMTY3r3XsLFxYXIyEjTE2tgYOBtn/xude7cOQDCw8OZOnUqoaGhZbap7k0zMTGRiIgI5s2bV+V94uPjixvqGQxoNBoCAgJIS0vjk08+4dtvvzU1CG3bti0pKSmsWbOGnTt34urqCkBYWBgbNmwwHe/s2bM888wz9O/fHyh+Bbx8+XI2b95suu6jRo1i1qxZtVLmyuh0Ot58801GjBhhugGmpKSg0+nYvXs3a9euRalUMnHiRJYtW8Zrr71m2nfmzJlMmzaNqKgo/vjjjzLv9yu63m+++Sa+vr6o1Wp27NjBhAkT2Lp1K97e3gB8/PHHjBw5End39yrF3arVP7/db775Ju7NXTh9/VcSrl/mwI4jdPd6GKXOgSNbL/Ph5H96dNnZnaiwpquq16Cia1iV69OQNMpEosST3Vvz/oG/WBURJxIJodqqUmNQlYaf4eHhhIeHVzuO7Oxsjh8/Xup9tK2tbZkxWhwcHEyjyJZ0w87MzCQ9PR17e3vTdnZ2dkiShJ+fH99++y2ZmZmoVCruvfdeZs+eTVxcHB4eHmUaQBsMBhYvXsyBAwcwGAzk5uaW+rGviCRJxMbGEh8fz5NPPlluElETW7duJSAggObNm1d5nw8++IDly5ezdOlS+vfvz+TJkzl8+DB+fn5lepUcP34cPz+/Uq8AMjIyaNq0qenzuXPnmDlzpunzkSNHKCoqKvVOX5Ik2rdvX40S3hmj0cjkyZNRqVSlGp6WPHGPGzfOlBA99dRT5d5EFQoFgYGBbNu2jXXr1jF+/HjTuoqu9601eCNGjODHH3/k119/Zdy4cZw5c4YjR46wefPmKsd96yR6nj7ORF/dh96o494OYfyy9iIHf/udgIAAcnJySh0nJyfH1Hbm36p6DSq6hlW5Pg1Jo04k/Jra07NlU/afTyI+PZfmTuV/qYT65U7fxzYUrq6u9OzZk8WLF5dafuzYsVKfb31nn5WVBYCjoyMuLi6lGsplZmYil8vp2LEjeXl5HDx4EH9/f5o3b05CQgKRkZGl3mGX2LlzJwcOHGDNmjU4Ozvz/fffs3379krjT0hIAIrbXTz55JP06NGjVGO7Es8+++xtq/m//PLLctdt3bqV5557rtI4btWjRw969OhBamoqzz33HJs3b0atVpdKuEqkpaWVSap++uknwsLCgOIndL1eX6oqPjMzk9DQ0DL/Zv9W3TJXRJIk/vOf/5CSksIXX3yBSvVPt0UHBwfc3d1LvY6o7FWQwWAo0wagqtdbJpMhSRJQ/F1NTEwkJCQEgLy8PAwGAyNGjGDz5s3lxl2SSNzMvsqJy3uQyeR0bt6PZo6tTcdu2bIlBoOBy5cv07JlS6C4dqjkdc6/VeUa3O4aVuX6NCSNctKuW40P8kWSYHXkRUuHItSSFStWmNodNCa9e/fm+PHjpkaMJ0+eLLchZEFBAfv27QNgz549pm5nvXr1KrX/d999R69evVAqlQQEBPDNN9/QrVs3AHx8fF5qr3AAACAASURBVNi4cWO5iURqaiqenp6mEWt37dplqkJWqVQYjcYyT4dQ/LTetm1b2rZty5w5c3j55Ze5caPsGDBffvklUVFR5f5X0Q31xIkTJCcn31Fvjb1793L58mUkSSI3N5esrCzatWvHPffcQ2RkJGfPnkWSJC5fvkxcXBydOnUiOjqaq1evkpuby//+9z9SUlIYOXIkUHzj8vPzKzWnUPv27Tl27JipoWNOTg779+833VhrUma9Xk9hYSFGoxGDwUBhYSF6ffFYOTNmzCAuLo7ly5eX+87/4YcfZvXq1aSmppKZmcnKlSvp27cvUPzvu2PHDnJzczEYDBw8eJAdO3aU+i5UdL2zsrJMvUj0ej3btm3j+PHj3HfffUBx27p9+/axZcsWtmzZwpgxY+jbt6/p7/l2cSsNtuRnSPh7DcTVtmWpY1tbWzNgwAAWL15MXl4ekZGR/PTTTzz00EMV/vvf7hrcLpaqXJ+GplHXSACM9m/Ba1siWBURx5T+Hev95ClC4+Xq6sqcOXN46aWX0Ol02NjYMHXq1DIDaLVs2ZKoqCgWLVqEXC43tXFwd3dn7ty5TJw4EZ1Oh5eXl6nBX3BwMJs2bTKNEdC1a1f+97//mRKLWw0dOpQdO3YwYMAAmjdvzquvvsqLL77IggULmDx5MgEBAYSEhPDZZ5+V2r8kkQAIDQ3l3LlzvPTSS6xZs6bG3UG3bNnCgAEDTK9ybhUeHs7kyZPp0KFDqeWRkZHMnj2b3NxcXF1def755003gxdffJEXXniBrKwsPD09ef/99+nUqRMTJkzg8ccfp6CggJ49e7Jq1SqsrKyA4kTi369DunbtyksvvcSkSZNIT0/Hzs6OkJCQWnmts2zZMj755BPT523btvHyyy/z8MMPs379etRqdakxfGbNmsWDDz4IwMSJE0lPTycsLAyNRsPgwYN58cUXgeIn83Xr1jFjxgyMRiOenp5MnTrV1O4DKr7eer2ejz/+mIsXL6JQKPDx8WHp0qWmV19WVlam6wXFvSPUajXOzs4kJiaWiVulVvD6O5MYMyIcyQirP97DrIvLyj32jBkzmDp1Kj179sTR0ZGZM2eW6jb67LPPEhgYaBqH5nbXoLxYSq5h7969K70+DY1M+nfqWweVDMpR3QGpKjP+299ZG3mJX18Ko7ePa7nb1OcBmqqjPpd31apVAHfU3qAhD0j1byUDFjUWlZV3+fLlhISEmJKY+q6x/PvmFWYReWU3OfkZ9G47Cjuts6VDqraKflvMfe+rLY2+RgIgPNCXtZGXWBlxocJEQqg/SrpQ1qThotB4eHp6mnq6CPVDWm4SUVf2ojMU4unQFltN2UEPGxO9QcfWqI/o3Lwfbdz+aSP2Q8QCbDSOyCiuab+/7RhsNA61fn6RSAAhrd1p4WTDDzFX+N/wIGw0FTeaEQShYRk2bJilQxDuQELaOf66dhAk6Oh5P06a5shkjbu538n4A6iV1uWuG9DhKVSK2hsptjyN++r/TS6XMT7Ql5xCPRv/bLgtawVBEOqz+LSznEr8FaVcRWCrIXg5197EbvVVRt4NMvKS8XKy3Ks5kUj8bXxQcZesVX/U3xnYBEEQGjJ3+1Y0tfPmXt/huNg2s3Q4taYmTRWPX9pBkM/QCtcfubCZnSeXcfzyrhqd53ZEIvE3Hxc7+vi68UtcMhdTsyvfQWh05HK5qfucIAh3R35RDmk51wBQKTUEtBxklvf8lqTT6ao1eduF5Eia2nlX2NC0q/cAgloNZVCn58nITeZKqnmmphBtJG4RHuTLr3HJrD5+UcwIKpShVCrJz88nLy8PhUJRb7sK63S6UqMBNnSivPVXZv5NYhL2YzTq6eEzEo2qbDuA+lpeSZIwGo3odDqUSmW1EomE9HNkF6QRn36WvMJMFHIlNhoHmjkWd2tt7fZPTzQv57ak516nZZOyg7zVlKiRuMXIzt7YqJWsiojDaKzzvWKFCkRERFQ4GU9N2dnZoVar620SARAX17he34ny1k/Xsy4SeWUnOn0Bvk0DUCutyt2uvpZXJpOhVCqxs7PD2rr8hpKV6dvucYb5v8zQLi/Rxi3o7xE9i5OIIn0Be0+twGAsrkW9nnkJJxu32x2u2kSNxC1sNSoe6dKClRFx/BJ3nX5tPCwdklAN5h7nQams/3829X0sjDslylv3JWVc4OLNaLIL0jFKBnSGQmzU9nTxDqWp3e3nR6mP5TWX88nHUSu0tGjSES+ntuyI+RSlXIWzbTNauNR+bQSIRKKMJ7v7sjIijlURF0UiUU/FxsYCiLEBBKGeSMq4QEz8AQDyirIpKMpGLlfQ0fP+SpMIoVjXFgPKLGvv2Zv2nr3L2bp2iVcb/9K7lSu+LnZsPHmFrIL6995NKJ7++tbZFAVBqNtik48jSUYAtEpr1Eor7K2acj2zfr62aGxEIvEvMpmM8CAf8nUGvo++YulwBEEQGhydoYjkrMucuXaIg7Hfk5h2liJ9AQByuQJbrRNymZycggwLRypUhUgkyjEu0BeZDL6JENmwIAhCbTFKRo7GbeWn0yuJurKXK6l/kV+Ug7XGodzRKW21jXvo6/pCtJEoh7eTDf1au/PT+evE3szCr6m9pUMSBEGoN4ySkaz8FFJzEknNSaRlk0642rdA/ney4GjthoutJy62njhau5KcecnURuJWPk3973boQjWIRKICT3ZvzU/nr7MqIo55Q7paOhxBEIQ6TZIkrqb+RWpuImk5SeiN/7Qxc7Fthqt9CwCCfYaVqX3wcGwNwMWb0eQUZGCrdcSnqb9puVC3iUSiAiM6Ncdeq2L18YvMHiQGpxIEoXEr6Z6ZU5COrdYJT6e2KOVqHKybYqd1RiaTcSX1L/KKMrFW2+Nu44OLrSfOts3Q3DIGREUTbHk4thaJQz0lEokKWKmUjPZvyRdHz7M/9jpNLB2QUGX/+9//LB2CIDQoSRkXiLl6gCJDATpDAel5yVxJOYWN1ol7PHpg594dgE5efdGorLFW21k4YuFuEo0tb+PJ7r4ArIy4YOFIhDvRt29f+vbta+kwBKHBuHgzmgJ9HjkFaRTq8pAkAyqlFo3SGs9bZp10snETSUQjJBKJ2wj2bkI7V3u2noonq8hg6XAEQRAsIqcgHaOkRyaTY2fVBEdr9+LXGcga3ARawp0TicRtFI8p4Uuh3sjeK5mWDkeootDQUEJDQy0dhiA0GLZaJ6zV9jhYu6JS/DPXjOieKYBIJCo1NsAHuUzGj3FiYJT64ubNm9y8edPSYQhCvSdJEsmZl2nVpLgbpvxfDSVF90wBRCJRqWYO1oS1a8bptAL+ui6SCUEQGo8rqX8SdXUveUWZdGne7+/XGXLstM50ad5P9LIQANFro0rCg3zZdSaRVRFxLBwWUPkOgiAI9Vx67nXOJR1Do7SiufM9aFTWInEQyiUSiSp4sIMX9moFayIvMm9IV1QKUZEjCMKd+fc4DHV5wKVCfT7RV/cD0MU7FI3K2sIRCXWZuCNWgUapIKylPcnZBew5d83S4QiCUM+UTJOdXZCGhER2QRox8QdIyqh7XcslycjJqz9RqM+jjXt3nG08LB2SUMeJRKKKhvoUt05e+YeYyKuuGzNmDGPGjLF0GIJgcvFmNJJkJLcwg7yiLJD+WV7XXM+8RGruNVztWtCqSWdLhyPUA+LVRhW1c9LSycORH08nkJJTQBNbraVDEiowZcoUS4cgCKWk5yaTU5iG0Vg8Ho3RaMBG41gnp8l2d/DBIOlxs2tp6uYpCLdj1hqJhQsXMnr0aEaOHMnevXtLrTt8+DCjRo1i9OjRLF261Jxh1AqZTMaTQb7oDEbWRV2ydDiCINQTWfkp5BZmYDQa0KrtUCrUFOnzKdTn1alxGAxGPVD8W+fl1BaVUmPhiIT6wmyJxNGjRzl//jzr16/nyy+/ZP78+aXWz507lyVLlrBu3ToOHTrEhQt1713hvz3erRVKuUy83qjjpk2bxrRp0ywdhiAAYKd1oUWTjthZNcFabYed1gUrtR0apXWdGYfBKBmIuPQjfyUexCgZLR2OUM+YLZEICgoyTZ5kb29Pfn4+BkNxtV58fDwODg54eHggl8vp06cPR44cMVcotcbVzooh93gSfS2d6MQ0S4cjVGDbtm1s27bN0mEIjVhy5mXOXT8GFD/h3+v7EIEtB2GndUYuU+Bq3wJ/7/54OLbmRtYVdIaiSo5oXueSjpGRdwODUY8M8TpDuDNmayOhUCiwti7uMrRhwwbuv/9+FAoFUDzyoLOzs2lbZ2dn4uPjzRVKrXqye2u2/ZXAyog4PvZ0rnwHQRAaDYNRz9mko8SnnUYuU+Dt3B6rvyexKm+a7PTc65y4sgd7qyYEthyCWnn3214lZcRxJfUUNhpH2nv2Fu0ihDtm9saW+/fvZ8OGDXz11Vc1PlZsbGwtRFR9kZGRuBslnDQKvvnjPGOayVApGu4fXWRkpKVDqJaiouKnuzuNv76WtzoaU1nh7pS3yJjLDf1piqRc1DIbXJXtOP3n7X+zJEkCvQ2J2Ze4eXMl7qrOKGU1b5tQ1fIWGfNI1BVv61DQjpiokzU+tyU0tu9zXWPWROLgwYMsX76cL7/8Eju7f6aWdXV1JSUlxfQ5OTkZV1fXSo/n5+dX6jh3U2RkJAEBxaNahl+Dj387wzWtKw939rZIPOZ2a3nrG7VaDXBH8dfn8t6pxlRWuDvljU87w5lr0WgkOa2dg2nncS8KedV+XiUpgLNJh7mS+heF6qt0bvWAqRajOqpaXoNRz5ELm7EttKZz8340q6ODY1WmIX+fs7OzLf4AXRVmayORnZ3NwoUL+eyzz3B0LN0y2cvLi5ycHBISEtDr9fz888/06tXLXKHUuie7+wKwMqLuNxAVBMH8CnS5KORKunoPpINn7yonEVDchqKdR098mnYlryiLYxe3F481YWZymRw3h1Z4u3Sot0mEUDeYrUZi586dpKen8+qrr5qWBQcH07ZtWwYMGMDMmTN54403ABgyZAitWrUyVyi1rpOHEwFezuw+e43rWfm421tZOiThFi1atLB0CEIjkJWfUjyJlUyOr2s3mjvfg1ZlU61jyWQy/NyDUCpU3My6ilpp/t8UmUxOG7fA4tcrglADZkskRo8ezejRoytcHxQUxPr16811erMLD/IlcnMEayMv8kZIB0uHI9xC9NgQzMkoGYm7cYK4Gydo7RpAa7cA5DJ5tZOIW/k09adlk07IZcUN0/UGHUqFqsbHvVVWfgo3sq7g69oVmUwuGlcKNSaGyK6mx7q1Qq2QszIiTmT0gtBI5BflEHHxR+JunECrssXF1rPWz1GSRKTkJPDruXWk5dTe/D46QyHRV/dz4UYkGXk3au24QuMmEolqcrbW8GDH5pxOzuR4fKqlwxFusXPnTnbu3GnpMIQGJjnzEocvbCQ97zruDj70ajMSJxt3s53PYNChNxZx/PIubmbXvHu8JEn8mfAreUVZ+DT1N2vsQuMiEokaCA8qaXQpRrqsS6ZMmSLm2xBqVWb+TaKu7sNgNNDB8366NO+PSmHeIaTdHFrRrcVAAE5c2cP1zIs1Ot7llD+5kXUZZ5tmtHYLrI0QBQEQiUSNDPTzwMPeiu+iLlOgM1g6HEEQzMTBqilt3ILo2XoEzZ3b3bV2BU3tvAlsORi5TEH01f0kplevK2B67nVirx9Do7SiS/N+yGXip1+oPeLbVANKhZxxAT5k5Bex9VT9GJlTEITyJWVc4ND5Dez58wt+j91A9NX9nIz/xdQGyte1K7Zap7sel7NtM4JaPYBKoeFyykmM0p0/tKTkJADQxTsUjcq6tkMUGjmRSNSQeL0hCPVfUsYFYuIPkF2QhkEykJR5gdOJvxOfdpoCXa6lw8PR2pXuPkMJ+Lt24k61cQukV5tRONt4mCE6obETiUQNtXNz4N4WTdgXe42EDMv/4AiCcOcu3owGQG8oIiv/Jjp9AUqFBlutM1ZqWwtHV8xO62LqYpqZf5O4Gycq7TGWkXfDtI0lalOExkEkErUgPMgXSYI1kTVrDCUIgmXkFKSj0xeSVZCK0WjASm2PvdaZgqIcS4dWhiRJnE48xPnk45xNOlJhMnEzO56jcVtMs5AKgrmIRKIWjPZviVapYOUfYkyJukB0/xTulK3WCb1RBxQ/+VupbUEmw1brWMmed59MJqNriwHYapy4knqKvxJ/Q5KMpbbJL8rhZPwBZDI5Ho6+FopUaCxEIlELHKzUjOjUnPMp2Ry+fNPS4TR6np6eeHrW/kBBQsPl09QfK7UtDlZNUSk1pZbXRVqVDd19hmFv1YSE9HPExB8wNcKUJCPRV/ejMxTS3qMnDlZNLRyt0NCZfRrxxuLJ7q1ZF3WZVRFx9GpV+UymgvlkZGQAlJksThD+7Wb2VdJykvBz7w4Ut5XIKcjAVuuIT1N/POrwZFZqpZagVkM5cXk31zMvkl+UjVEycKXwLIpUGZ5Ofng532PpMIVGQCQStSSktRvNHa35PvoKHz0UiI2mdsfHF6quT58+AMTExFg4EqEuu5F1hair+5Ahx8u5HR6Oret04lAelUJNQKvBHLuwlbTcJCTJiF4qRI6G7II0rmfG1bsyCfWPeLVRSxRyOeMDfcku1LHpTzGmhCDUZbcmEQEtB2GjcbB0SNWmlKtABgq5EqVcjUpm9fespDJTbxRBMCeRSNSikjElZu6OZtYe8TQsCHVRcuZlUxIR2HIQLrbNLB1SjeUUpBf/jwyUMg0KufLv5RkWjEpoLEQiUYt8m9jh7WjN5fRcZu89KZIJQahjsvJTiL66H7lMTmCrwTg3gCQCKh4joi72OhEaHpFI1KJZe2K4mpFn+iySCUGoW+y0LrRo0oGAloMb1CiPFfUuqau9ToSGRTS2rCWz9sQwe+/JMstLls0I63K3QxIE4W+5hZnYaByQyWS08+hh6XBqXUmDyos3o8nOzsFO61zne50IDYdIJGpBRUlECZFM3F3Tpk2zdAhCHZJjuMHvsSdo59GDFk06WjocsynpdRKZFUlAmwBLhyM0IiKREBqcUaNGWToEoY5IyrjADf0ZnOROOFiLgZkEwRxEIlELSmoaKqqVmD6ws6iNEIS77FrGBU7GH0COnMBWD+BoLQaKEwRzEIlELakomWhqo+HdAZ0sEVKjNWbMGAC+++47C0ciWEpJEqGUq3FXtRFJhNCg6Q06tkZ9ROfm/WjjFmhafi3jPCcu70Emk+Pl1JYu3v3Ncn7Ra6MWzQjrwvSBnU2fuzRz4mZuId9HX7FgVI3PmTNnOHPmjKXDECxEkiSSMy+ilKsJavUAWrm9pUMSBLM6GX8AtdK6zPJjcdsJuWcsQzpPIDHjPBl5yWY5v6iRqGW3vsIYH+hDuwVbmbUnhke6tECpEHmbIJibTCaji3d/8gqz/h5f4aqlQxIEs8nIu0FGXjJeTm1LLc8uSEWjssJGUzyWiJdTW5IyLuBo7VbrMYg7mxnMCOvCjLAutHKx45ngNpxPyeab4xctHZYgNGiJ6bEkZVwAQC5TVDhIkyA0JMcv7SDIZ2iZ5flFOWiVNqbPWpUteUXZZolBJBJm9p8BndAo5czZd5JCvcHS4QhCg5SQdo4/E37hzLXD6A1Flg5HEO6KC8mRNLXzxk7rXIWtJbPFIV5tmJmngzUv9mzLx7+d4atjF3ixV9vKdxIEocri087yV+JvqBQaAls9gFKhtnRIgnBXJKSfI7sgjfj0s+QVZqKQK7HRONDMsQ1WajvydTmmbfOKsrBWm6e9kEgk7oK3+3Xgi6Pnmbf/T8KDfLFWi8tuTv37m6dlslD3xKed4a/Eg6gUWrr7PICd1sXSIQnCXdO33eOm/4+6sg9brRPNHNsAYKd1RmcoILsgDRuNA/FpZ7m/7WizxCHuaHeBq50Vr9zXjvd+OsXyw7G83re9pUNq0P773/9aOgThLkjOvCSSCEH4l/PJx1ErtLRo0pF7fYfz27nibvCtmnTGwco8g7KJROIueaNvez49dI73D5ziuXvbYKdVWTokQahXkjIucPFmNDkF6dhqnWjh0hFXuxa0cQ+q4jtiQWi4urYYUGaZu4MPD3SZaPZzi8aWd4mTtYbX+7YnJbeQxQfFGAfmtHjxYhYvXmzpMIRalJRxgZj4A2QXpGEw6skuSONU4m94OPqKJEIQLEwkEnfRK/e1w8Vaw6JfTpOeV2jpcBqsFStWsGLFCkuHIdSiizejASjQ5ZGRfwOdobDUckEQLEckEneRvVbN2/06kFmg47+/nrZ0OIJQb+QUpKMzFJFXmIkMGXKZ4u/lGRaOTBAEsyYSsbGxhIaGsmbNmjLr1q5dy+jRo3nssceYN2+eOcOoU17s1RZ3Oyv+99tZbmTnWzocQagXtCpbcgrSALDVOqOQK//+f0dLhiUIAmZMJPLy8pgzZw49evQosy4nJ4cVK1awdu1a1q1bR1xcHNHRjaOK0lqtZGpoR3KL9Cz8+S9LhyMIdZ7BqEdnKESSjFhr7FHdMk6ET1N/C0YmCAKYMZFQq9V88cUXuLqWnXVPpVKhUqnIy8tDr9eTn5+Pg4ODuUKpc569tw3eTjYsOxRLYmaepcMRhDrtfPJx9MYiWrh0ooltc2TIsdM606V5PzwcW1s6PEFo9MzW/VOpVKJUln94jUbDSy+9RGhoKBqNhgceeIBWrVqZK5Q6R6NU8O6ATjz//VHe2/8nn4wMtnRIDYq1ddlZ8IT6q1XTzhiMetp53Gt6pSEIQt0hkyTJfANwA0uWLMHJyYmxY8ealuXk5DB69GhWr16Nra0t4eHhzJgxg3bt2pV7jOzsbGJjY80Z5l2nN0qM/jGOpLwiNgxtTTNbMayvINxKkiRkMpmlwxAEi/Pz88POzs7SYVTIIul9XFwczZs3x9m5uP93YGAgp06dqjCRKGHJixkZGUlAQECtHnO+woVxa39nS5LEijG1e+yaMkd567LGVN76UNYCXS7HL+3knmY9cbH1rNGx6kN5a5Mob8NRXx6iLdL909PTk7i4OAoKCgA4deoULVu2tEQoFjXavwUd3B345vhFYm9mWTqcBiMiIoKIiAhLhyFUk9FoIOrKPnIK08n+u6eGIAh1l9lqJE6dOsX7779PYmIiSqWSPXv20K9fP7y8vBgwYADPPPMM48ePR6FQ0LVrVwIDA80VSp2lkMuZGebPI6t+ZebuGL4dd5+lQ2oQnn32WQBiYmIsHIlQHaevHSIz/wbNHNvQwqWjpcMRBKESZkskOnbsyOrVqytcP2bMGMaMGWOu09cbIzo1p5uXM+ujL/NO/450buZk6ZAEwWLi086QkH4We20TOnjeJ9pICEI9IEa2tDCZTMbsQcV94WfuEU/QQuOVnpvM6WuHUCk0+LcYIHpoCEI9IRKJOmBQu2b0bNmUrafiibiaYulwBMEiNCor7DTO+HuHYq2uuy3UBUEoTSQSdYBMJmP24OJaiem7Ra2E0DhZq+3p0XpEjXtpCIJwd4lEoo4Iae1O/zbu7D13jYMXky0djiDcNReSI8nKL66JE20iBKH+EYlEHTLr77YS03dFY+Zxwhq0VatWsWrVKkuHIVRBQtpZLtyI5PS138V3XhDqKZFI1CE9WjZlyD2e/HbxBvtjkywdTr3l7++Pv7+YzKmuy8i7wV/Xfkel0NC5eT9RGyEI9ZRIJOqYkh4c03eLWgmh4SrU5xF1ZR+SZKRL8/5Yq+0tHZIgCNUkEok6pquXMyM7e/PH1VR+PJ1g6XDqpcDAwEY5wFl9YZQMRF/ZT6E+Fz/3YJrYeVk6JEEQakAkEnXQzLAuyGQwfVcMRqOolbhTOp0OnU5n6TCEChTq8ijQ5+Lu4EOrJp0tHY4gCDUkRnypg9q7O/J4t1asjbzEhpNXeNS/paVDEoRaY6W2o6fvw8hkctEuQhDqCEkykpKTSE5BOgC2Wiea2Hoik1Ve3yASiTpqxsAufBd1mZm7Y3i4kzdKhag8Euq3rPwUFHIVNhoHVEqNpcMRBIHiBOJU4kH+SjyIrcYJG40jALmF6eQWZtLBs/ffw9VXfA8SiUQd5dvEjqe6+/Ll0Qt8G3WJ8YG+lg5JEKqtUJ/PiSt7MRh13O83RiQSglBH7D+9EhdbT4Z3ew2tyqbUugJdLqev/c7+06sY0OGpCo8hHnPrsP+EdkatkDN7z0mK9AZLhyMI1WKUDERf3U+BLoeWTTqLJEIQ6pCu3gPo1iKsTBIBoFXZ0K1FGF29Q297jColEr/88gtr1qwB4OrVq6Jb4l3i7WTDCz39uJSWw9cRcZYOp96YMGECEyZMsHQYwt/OJR0jPTcJN/tW+DQV43sIQl3SxK45AAlp54i7EQXAb+e+Y+PxD7iScqrUNhWpNJH44IMP2LBhA5s2bQJg+/btzJ07t0aBC1X3Tr+OWKkUzNv3JwU6UStRFS+++CIvvviipcMQgMT0WK6knsJG40gnrz6icaUg1FEx8T/h6eRHQto5jJKRB7u+wulrh6q0b6WJREREBJ988gk2NsXVHi+99BJ//fVXzSIWqszd3oqXe7cjMTOPz47EWjocQagyo2TgQnIkSrmabi3CUCrUlg5JEIQKKOUqtCobEtLP4uvaDZVCg7wKPTagComERlP8PrPkScJgMGAwiCfju+mtkA7YaVQs+OkUOYVifITKTJo0iUmTJlk6jEZPLlMQ7Psg3VoMxEbjYOlwBEG4DYNRz6mEX0lMj8XD0Zes/BSKDAVV2rfSXhvdunVjypQp3Lhxg6+//pq9e/fSvXv3GgctVJ2LjYbX+tzD7L0nWfr7Od7u39HSIdVpv/32m6VDaJSSMi5w8WY02QVpWKvtaeMWiIdj63IbcQmCULf0aP0wscl/0LvNKJRyFYnpsQS0HFSlfStNJF577TV2796NVqvl+vXrPPXUUwwcOLDGQQt35tX772HJwbN88PNfZOQXoVUpmBHWxdJhCQJQnETExB8AIK8wlVqyJwAAIABJREFUi/Tc62QXpNIN8HBsbdngBEGoUE5BBgAqhYYOze4zLWvu3L7Kx6g0kfj88895/vnnGTSoapmJYB4OVmreCunA1J1RLPz5nzYqIpkQ6oKLN6OB4vEiCnQ5yOVK5DIFF29Gi0RCEOqwnSeXIZOBJEF+URYqpRZJMqI3FGGrdWZk4FuVHqPSRCI2NpYrV67QokWLWglaqL6sgqJSn2fvPQmIZEKwvJyCdPQGHbmFGchkMuy0zshkctPTjiAIddOj3acAcOzidlq7dsPF1hOAm9lXuXgjukrHqDSROHfuHEOGDMHR0RGVSoUkSchkMn755ZfqRy7csVl7YlhwoGxvGZFMCHWBVm1HWvp5kCRstM4o5MU/LbZaRwtHJghCVaTlJOLiM8z0uamdNycu763SvpUmEsuXL69+ZEKtmLUnxpQwlEckE6V16SKuw90mSUYkyYCV2h61UmtaLgagEoT6Qkbk5d242rdEhowb2VcwSFXrJVhpIuHu7s727ds5dap4hCt/f3+GDh1as3gFwYy++eYbS4fQ6AS1eoDTansK9QXkFmZgq3XEp6m/aB8hCPVE33ZPcObaIWKvH0OSJByt3ejb9okq7VtpIjF37lxSU1MJDg5GkiR27dpFdHQ07777bo0DF6qmpKaholqJ6QM7i9oIwSKMkgG5TIGNxoEgH/GAIQj1lZXalm4tw/6eAuPOpsGoNJE4f/68aZ4NgLFjx/L444/fcZBCzVSUTKgUMp4MEjOD3urbb78FEN9TM7uZHc+Za4fp2mIAdlpnS4cjCEINnEr4lZj4n9EbCoHiVEIGhPd+r9J9K00kdDodRqMRubx4EEwxsqXl/DuZeLCDF9v+SuC574+w54VQMY/B395//31AJBLmlFuYQczVnzBKBgxGvaXDEQShhs4nH+ehrq9Wq4F0pYlEnz59GDVqFEFBQQAcO3aMIUOG3HmUQq249RXG9IGdGbbiZ3adSeTLYxd47t42FoxMaCx0+kIiL+9Bbyyic/MQHK1dLR2SIAg1ZG/VpNq9rCpNJCZOnEjPnj2JiYlBJpMxe/ZsOnfuXK2TCbXj1mTis0fupdPCbby1LZKwts3wdhLDEQvmY5SMxMT/RF5RJq2adqGZo0heBaEhcLJ259dz63B38EF+yzRcbdyDKt230km7bty4wcmTJwkPD2f8+PH89NNPJCcn1yxiodZ4Oliz6KFAsgt1/D979x0WV5k+/v99pgNDL6EFAgRIJ8X0mMRoosasZXUta9S1flbdVbdcu+p3rdmPZdX97a6fXV3LFmOPRleNbhKNLT0hCQlplBBC722AAWbm/P4gYDDADDAwDNyv6+IKnPKc+4Rk5p7nPM/93PHujtMDZYQYHNmle6i0FBLuH0fKGOcvMEII79DUWo9W0VFRf4qy+pOdX65w2iPxwAMPcOWVV3b+nJqayoMPPsirr77a74CFe/1kdhLrMvLZeKyYf+zO4da5w/dTYsfCThZrDWZTsEwR9DKRQYnUW6tIG7sMxcUlhoUQw9+ilB8B0NLWBAoYdb4un+s0kWhtbe0yJmLlypW8/fbb/QhTDBZFUfj7VfOY9uzH/PqjdFakRDN2GD7iOHNhJ4AGa3Xnz5JMeIdAn3BmJ8gYKSFGmvL6fL7Neoc2ewuqCka9L4tTriHMP9bpuS59pPjmm2+wWq00NTWxceNGlwPLysriggsu6DJ9tENJSQnXXXcdV111FQ8//LDLbYrujQ3249lLZ1FvbeN/3ts5LB9xnLmwU6ut+azt7rJt2za2bdvm1jZHs+ZWC3vyPqWptcHToQghBkn6yc9YNvFGrp37ENfNe4glqdeyJ+8Tl851mkj8/ve/5x//+Afz589n0aJFrFu3jjVr1jhtuKmpiTVr1jB//vxu9z/11FPccsstvPfee2i1WoqLi10KWPTsljnjWZ4SxcZjxfxrT66nwzmLxVqDqqo0ttRiaalFVR2nt7t3YSez2YzZbHZrm6OV3WFjf/4mqiyFVDac8nQ4QohBoigagv0iO38ONce4/PjS6VHx8fH861//Yv/+/ezbt4+XXnrJpZVADQYDL7/8MhERZ08NczgcpKens2zZMgAeeeQRoqOjXQpY9ExRFF66ej7+Rj2/+s9eCmsbPR1SF2ZTMG32lvb1alWVVpv19Hb3Lux08uRJTp486dY2RyNVVTlU+BX11kpigycwNmSSp0MSQgwSBYX8ykxabVZabVbyKg66nEg4HSOxfv16mpubufbaa1m9ejWlpaXcfvvtTov96HQ6dLrum6+ursbPz48nn3ySw4cPc8455/CrX/3KpYBF7+KC/Xjm0ln8dN1OfvreLj6+9bxhU6gqMXw6pXV5nT+32Jow6n3dvrDTZZddBkBGRoZb2x1tTlQcoLTuBMG+kUyKWThs/h0JIdrZ7K1szV5Hc6sFu8NGWtwyxoZM7Ny/bs9T+BmDUGj/v7s49Vr8jIHdtjV//BXszP2IbdnvoSgawv3HMj/pCpficJpIvPPOO6xdu5bNmzeTnJzMG2+8wU033TSgqoGqqlJWVsaNN95ITEwMd9xxB1999RVLly7t9bysrKx+X9Md0tPTPXp9V83QqcyJ9OOzo0U8/t4WViX27xP/YNxvqH0y1WouVrUORTViaoqlOLeOYtx3rdbWVqDv8XvL79cdnN1ro72SMlsmOsVISEs4+/e5dxzLUBtNv1uQ+x0tCqqPEmqOZWrsEizWGjZlvtolkQBYPvlm9Fqj07YCfMJYOuHHnav3Nrc24GPwdykOp4mE0WjEYDDw9ddfc+mll3aWyh6I4OBgoqOjiYuLA2D+/PlkZ2c7TSRSUlLw93ftxtwtPT2dWbNmeeTa/fF24gSmPfsxfz5QwW0r5hMT6PpUHvC++z2TwWAA6FP83ny/feXKvVZZinEUVjEzfgUBPmFDFNngGE2/W5D7HUkaGhp6/QCdEP5dccLGljp8jQH9vtbR4h0U12Zz/qQbAfj6+FvEh05hYvQCp+e6lBU89thj7Nu3jzlz5rB///7OT3z9pdPpGDt2bOdz7MOHD5OQkDCgNkVX8SFm/vCDWdRZ2/jpuuExi6PKUkR9c2WXWBwO+7CITXQVao5mceo1Xp9ECDEabMj4G99kvcWcxB+ctW9Hzgd8evAF9p78rNfX2hMV+zlv4nfLhq+YcqvLM+qc9kg8++yzfPrpp9xwww1otVqKiop47LHHnDacmZnJ008/TVFRETqdjo0bN7Js2TJiY2NZvnw5Dz74IPfffz+qqpKSktI58FK4zx3zknk/I59PjxbxenoeN5yT6NF4jhRvo7m1gfMn3YRW0VFcm83R4u1Mj1tOqNk7BtuO5IJaDtVOVuluEsLTMOp80ShaT4ckhHDBJWl3UWUp5tvj73DpjHs7xzPNiFtOTHAqRr0PW46sJb8qk3FhU7ttQ1Ud3/s/7/qYKKeJREREBD/5yU86f161apVLDU+ZMoW1a9f2uD8+Pp633nrLpbZE/3TM4kh79mPu+3APF6REEhXQt0cc7tLUUk9jSy0R/vFoNe3/7Ew6P9rsLRTVZHlFItFRUMvuaEOjaEdcQa2jxTsoqD6C3WFncswiT4cjhHCi0lKIj96MnzGIUHM0DtWBta0RH0P79PfxY7575BMbkkpNY2mPicTYkIlsyPgbYwISUFEpqc0hPmyKS3FIjdsRblyImadXzaK2uZWfrtvlsccI5Q35AIQHxHVuC/aLwsfgT1n9CWz2Nrdd69lnn+XZZ591W3sdTlQcwGZvpa6pgrrmCuyOts7t3u5U1WEKqo/gbwohNXKup8MRQrigrC6PzKJvgfbBkTZHCyZ9+4fFVpuVTZmvYnfYACityyPYb0yPbaXFnc8541Zi0vvha/BnXtLlpI117UmB0x4J4f06HnF8cqSQN/flcf2soX/EUV5/OpHw/y6RUBSFmKAUcsrTKavPIyY4xS3XWr58uVva+T6LtYaW0xU5HQ47dc2VmI3Bbi+oNdSqLMUcLd6BXmtiRvyF6LR6T4ckhHBBauQ8tuW8x6cHX8TuaGNe4mXklO/DoDURHzaF2OBUNmT8DZ1GT4g5mvjQ7nsjOrTarWg1OiZGL6C+uQpVVV2a9u1SInHs2DFqa2u7fJrtqWKlGH40GoWXrp5H2rOfcO8Hezg/OYrIAJ8hu36bvYWaxlICfcIx6buuARIdnExOeTpFNcfdlkgMFrMpGFV1YNCaUHFgaanFYq1G7+d8atVw1dRaz4FTmwGYEb8cXxenewkhPE+n1bMk9boe90+KWcQkFx9T7s37jHprJY3WWiZGLyCv4gDNbRbmJV3mPA5nB/z85z/n2LFjREZ+VzpTURRJJLxMQqg/T6+ayc/W7+bO93ay/ualQ1ZgyGKtQaPRdumN6OBrCCDYL4rqxhKaWy2dz/YG4uKLLwbgs88+G3BbZ0oMn05GwRb0uvbEIUDRYWmpJiLAeaXX4aqppR6HamdSzEJC/KI8HY4QwkNK60+wKu1u/nvoJaD9UcenGS+4dK7TRKKoqIjNmzcPLEIxLPzP/BTey8jno8OFvL3/JNfNHJopt8F+kZw/8UYcqr3b/Slj5qAoylm9Ff01GOu2OBx2mlrrmRi1gMKaY1istQT7jSEtbhmxwROA9lHPLbZmt93HUAjzj+XclGu8KmYhhPvpNF0faTpUB47T6yE5PdfZAQkJCbS2tnYW+RHeS6NRePma9lkc93ywm2XJkYzxH5pHHBqNFg3dTyfsbQDQcFFSl0t22V7GhU1jYfJV3R5zvHQ3xTVZzIhf0WXxm+GiY+pqccspqjMzmBC9gLEhEySJEEIQ4R/P1qx1NLU2cLjoW/IrM4kMdG08ndNEQqPRcMkllzBt2jS02u/eCP7whz/0P2LhMYmh/jx1yUzu+WAPd72/i/duWjKojzjqm6uobSpjTGACRl3vSUuDtRqNou2xFrynqKpKflUmAHGhk3s8ztfgT5u9hd15nzApehFjQyYMVYhOdUxdBbCrLVQ0nKI2txyd5qoRMXVVCDEwM8ddyMnKQ+i0ehpb6pgcc67L0z+dJhILFixgwQLnJTKF97hzQSrvZeTz4aEC3jlwkmtnDN4jjuLaLE5WHsLX4I/Rf2yPx9U2lbMz90NiglOZGrtk0OLpj9qmcuqbK4kIGNfrYMS40Mn4GYM4cOpzDhd9g8VaRWrUvGFR2OlExQFUVaXV1kyb2oRW0eFr8OdExQFJJIQQOBx2wv3jGBc2lWpLMTVNpdjsrei0zp9GOK0jccUVVzB79mz8/Pwwm83MnTuXK65wbUUwMTxpNAqvXLMAH72We9bvoayheVCuo6oq5fX5aDV6Qvx6LzjVPqPDTGndCWwO99WUcIdTp3sj4nvpjegQao5h/vgrMBuDya86zN68z3ocGzJUVNVBlaWIuuYKGlvap6qajcFoNXqvn7oqhHCPb7PfpaLhFI0tdXx57A1qGkvZmr3OpXOdJhJvvfUWN954Ixs2bODjjz/mhhtu4IMPPhhw0MKzksL8efKSGVQ1tXD3+7sHpVBVY0sdTa31hJlj0Wh6/1SuKAoxwSnYHW2UnbHUeH9ceeWVXHnllQNqo4O1rZHSujzMxmCnyVAHX0MA85IuIyIgngCfUI/3SKiotNqsOFQ7Rr0vRk1A5wp/ZlP/VoYVQowsTS31jAubysnKg0yImsc5CStpaXPtQ6bTRxv/+c9/+OyzzzAa26e8NTU1cfPNN0uvxAhw98IJvH/wFB8cOsW6jHyunj7Ore13VLN0dXpkTFAKueX7KK7JGlBNiYcffrjf536f3WEjIiCOcP/4Po0l0WkNzIhbAbQnaKqqUtdcQZBvhNti64mqOiipy8XhcBAbkopG0TJ17HnklqWj0WhpaG3oPDYxfPqgxyOEGP7squ30eLDDLExu/yDW5mhx6VynPRI6na4ziQDw9fVFr5fKdyNB+yOO+fjotfx8/W7K3fyIo+J0NcuwXsZGnMnXGECwbyRVjcU0n/Fm50l+xkBmxK8gNiS1z+cqioKitP8Xy6s8yM7cD8kt3z9oZcpV1UFxbTZbs9ZxsOBLsst2dz5WSY2cw4z45fibQgAFf1MIaWOXyfgIIQQAkYGJvLnzUXwN/gT6hHO4aCuBPuEuneu0RyIyMpI1a9Z0DrjcunUrUVFSuGakGB8WwBMrZ/CL/+zl5x/s4Z0bF7ulXYfqQFE0BPtGOp2tcaaY4BTqrZU0WKvx6WeVxccffxwYeM+E3WHrXGBsoMLMMZzSm8ku24PFWs2U2CVua7u9B+IEuWXpNLbWoaAhNngCiREzujxWiQoaT1TQeNLr05mVPKuXFoUQo0WLrQmjzpdzxl3M1Nilna/XcaGTmBg1v8sxPXH6SrZmzRrWrl3L+vXrURSFtLQ0brjhBjfdghgOfrao/RHHexn5rMvI50hpLcXF5fx9AO81GkXDnMRVLhc06RAVNJ7IwKQBrffw/vvvAwNLJFRVZdeJjzDp/JgRv7yzZ6G/AnzCmD/+cvbnb6akLpfG1jpmxq/ApB94Jc/qxlIOFmw5I4GYjq8hYMDtCiFGvi8O/5vp8RcQHZTc5UNfe+8lFNdkc6Dgc1ZOu7PHNnpMJDoW6zAajdx2221uDFsMNx2POKY/+wm3vLWNprb27vDojRk8cmHawNru4xuwuz6lD1RtUxn1zZX4BJgHnER0MOp8mZOwiiPFWymsOc72nA9YMP6HfS4IpaoOSuvyCPSNwNfgT4hfFMljZhMVlCQJhBCiT5ZOvJ7t2evZc2IDMcGpnXV8GlvqKKo5jp8xiKUTVvfaRo+v2jfddBOvvfYakyZN6jLIrCPBOHr0qJtuQwwHyeEBLE6MYFNWSee2xzcdBOhzMuFw2DlSvI3IwETC/GP7HItDtVNUk43N3kJC+MASmf7K75zy6VpBFldpNFomxyzG3xRKg7Wq1+7C71NVldK6E+SUp9PYUkts8ASmxC5GURSSIma4NU4hxOjgawjggsk/ocpSTFFNFnXN5ae3B7Eo5WpCzc5nq/WYSLz22msA7Nq1i8DArpUGCwoKBhK3GIYe25jRJYno0J9korqxhMKaY+i0+n4lEgoKueX7sNlbiAudPOS9FM2tFsrq8vA3hRA8CAtZKYpCfNiULkv0ltXlER4Q320PzvcTCAWFmOBUEiNkxoUQwj1CzdEuJQ3d6fUV2uFw8LOf/YzXXnut80Wvra2Nu+66i48//rhfFxTDz2MbMzoThu70NZnomPbZ3WqfrlAUDdFByZyo2E9ZfR7RQcn9aqe/CqqPoKISHzp1UMuHd7RdXJvDwYIthJpjiQxI4FT1YSzWGsymYBLDp1PfXEVeZUZnApEUPgNfozzCEEIMDz0mEp988gnPP/88+fn5TJo0qXPKmkajYdEi19Y3F6NPRzVLncYwoIWrYoJTOFGxn6KarD4nEtHR/cuqoX38QVFNNnqtkaigpH630xcR/nGE+8dRWHOc3PJ9+JuCcagOGqzVZBRsITVyDtFBKSRFzBh265AIIUSPicSqVatYtWoVzz//PD//+c+HMiYxxDp6GnrqlfjNeZNd7o2wtFRjbbMQFZg0oIqOfsZAgnzHUGUpornVgo/B9dkNn332Wb+vqygaFiT/EIu1esgeqei0BmbGr6CsLo9Gh426pgqgfaaHTmuguDanxxVHhRDCXaxtjVisNYT5x6KensLvCqdH5ebmDjg4Mfw9cmEaD6+Y1u2+nfkVWNtcWy+i/HQRqnAXq1n2pqO6ZXFt9oDb6gujzodQc8yQXlNRNGg1OvxMwSiKBoPOp/M/sayHIYQYbCcqDrAh42+d62vsOvERWaV7XDrXaSIRGxvLe++9R25uLgUFBZ1fYuT5fjLxu+VTuXJaHN+cKOf6N77F7nBeE0KvNWI2BhNudq2aZW8iA5MI8Yvqc3f+5s2b2bx5c5+vV99cSUltbp9rX7iL2RSMUedDsF8kZlNwZ4+IrIchhBhsh4u2ctmMezuno58z7hKySne7dK7TvttPP/30rG2KovDFF1/0MUzhDToeYRQXF/PYRdNpsdmpbvqCDw8VcNf7u3jxqnm9DkCMC51MnAurZLpCrzUwJ/EHfT7v17/+NQAZGRl9Oi+3vH1w51z9pQMa39FfieHTySjY0u12IYQYTAatqcuS4Tqt3ulii53HOjtgy5azX9jEyPbIhWmkp9sAMOq0rL95Kcv+tplXduYQ6e/DYxcN/RtbX57X9YdNtVJTn0eAKYwg3zGDdp3edKx7caLiABZrLWZTEInh02U9DCHEoDPqfckpS8fmaKPKUkRexUGXi+U5TSTKy8v505/+xKFDh1AUhenTp3PfffcREhIy4MCFdwgwGdhw+zLOfX4jv998iAizibsXTTjruKPF29AoOpIjz3Hr0tlZpbsprs3m3JRrBm0AZL29GIC4sMmDOuXTmY71MIQQYijNH38F+/M3YbO3sC37fcYEjGPh+CtdOtfpq/LDDz/Mueeey80334yqqmzfvp0HH3yQF198ccCBC+8xxt+Hz+44n3P/77/c++Eews2mLsuO2x02CqqP42Mwkxo11+3Xt7Y1UlZ/kuhBeJO1O2zU24sJ1gYTFTg0Uz6FEGI4Mep8mJd0Wb/OdZpINDc3c/3113f+nJKSIo87RqmkMH823HY+5/1tEze+uY0QXyMXpLRXfqyyFOFQbUS4YbbG97XXlDhAcU3WoCQSxbU5OLAxNmTisFnrQwghhlJu+T4OF22lzW7ldNkoAK6a/Run57qUSJSXlxMREQFAaWkpra2t/Y9WeLUZsSF8cMtSVr70BVf+6yu23LmCWWNDO6d9Rvi7P5HwMwYR5DuGSksh1rbGPi9y5YxOo0ev+DI2dJJb2xVCCG9x4NQXLEy+El9D34veOU0k7rrrLn74wx8SHh6OqqpUV1fzv//7v/0KVIwM542PZO31i7h27Tdc8soXfHP3hVQ05KPXmgjyjRiUa8YEJVPbVEZxTbbTNSb+85//9KntqKAkYvWz3Z6gCCGEtwjyjSAyMLFf5zpNJGbOnMnnn3/OyZMnAUhISKC8vLxfFxMjx1Vp8fxf41zufn8XN7y+gQfPs5AQNnHQZlZEBiVxtGQHRbVZJISn9Togcty4cS6361AdaBSNRwdYCiGEp6VEzmVT5quE+4/t8jo+Pe4Cp+f2+qrvcDi4++67MRqNpKSkkJKSgqIo3HXXXQOPWni9ny5I4ZEV08iraeaTo+DvM/AiVD3Ra41MjjmXtLHnO33Tt1gsWCwWp202tTbw1dHXKag+6q4whRDCK+3N24CvIQCV9g9YHV+ucGnRrokTJ6IoSucKoOeee667Yhde7qEV0yizWHlxexa5tbl8evs4THr3Tf08U0fJbGcWLlwIOC9IVVB1hFa71a1TVYUQwhv5GPxZlPKjfp3bY4/EqlWr2LhxI3fffTfHjh3j6NGjnX++9NJLLjWelZXFBRdcwOuvv97jMc899xw33HBD3yMXw4KiKPzlitlcOS2Or3PLXC6l3V+qqlLbVI7dYRtQOzZHGwXVRzFoTTLlUwgx6sUEp5Jdtpe65goarFWdX65wOkbiiiuuID09nVmzZvHuu+9y4MABbr31VpKSen/xbWpqYs2aNcyfP7/HY3JyctizZw96vd6lYMXwU1B1hOLaHP525Tyqm1r48FABd7+/mxeumjso4w7yKg+SVbqLtLHnD2iZ75LaHGyOVpIiZrpcBlYIIUaq4yU7u9mquDT90+nIuAceeAC9Xs+RI0d49913ufDCC/n973/vtGGDwcDLL7/cOW20O0899RS/+MUvnLYlhq+y+pPUNJXiZ/Bl/c1LmR4dzMs7s3l0Y9/WuXBVhH8cAEU1Wf1uQ1VV8iszUdAwNmSiu0ITQgivddXs33bz5TyJABcSCUVRmDZtGps3b2b16tUsWbIE9cxqFT3Q6XSYTKYe969fv545c+YQEzO0yzUL97HZW6lqLMbfFIKPwXy6lPb5JIaa+f3mQ/x16zG3X9NsCibQJ4JKSwHWtsZ+tVHbVI6lpYbIwASZ8imEGNUOFnwJwDfH3+Hbbr5c4fTRRlNTEwcPHmTjxo28/vrrtLa2Ul9fP6DAa2trWb9+Pf/85z8pKytz+bysrP5/CnWH9PR0j15/qDm730Z7BfW2OjTaoC7HPrsgkts253HvB3toKC9meXzfC5z0xmrX0mBrYNu+TQTp4s7a31Ewraf4VVXFrCbRZNWTXvHdMaPp9zua7hXkfke60Xa/7hRqbv8wP5CqwU4TiVtuuYWHHnqIq6++mpCQEJ577jlWrVrV7wsC7Ny5k+rqaq6//npaW1s5deoUTzzxBA8++GCv56WkpODv7z+ga/dXxziR0cKV+z1Y8BVNtf7MTlrcpRDVLCBufCrn/W0Tj+4sYfaUiZx/upS2O7TZpvDlsXIMhjZmJs88ayzGQw891B5HH35fo+n3O5ruFeR+R7qRfL8NDQ2D/gG6YzZcbXM554y7uMu+bdnvMX6M879bp4nEypUrWblyZefPv/zlLwc8iO6iiy7ioosuAqCwsJAHHnjAaRIhhhdVdVDRcAqjzpdAn/Cz9p9ZSvuHZ5TSdge9zkhEQDyVDe0ls30M5i77f/zjH/d4blldHmZTMH7GILfEIoQQ3iy/MpP8qsOU1ObQ3PLd0waH6qCsPs+lNnpMJO677z7+9Kc/sWTJkm4Th6+++qrXhjMzM3n66acpKipCp9OxceNGli1bRmxsLMuXL3cpODF8qapK8phzUFF7TCy/X0r7259dRHJ4AI+dHoj5yIVp/b7+hKj56GONfVpky2Zv41Dh12g1OpZM+DGaQarCKYQQ3iImOBWTwUyVpZCoMx5vKIriUlVL6CWR+N3vfgfAm2++2a/gpkyZwtq1a50eFxsb69JxYnjRaLTEubDI1ZmltC9+6QsunzqW/+/r7ypJ9jeZ6G2Q5I033gjAa6+91mV7cW0WNkcr48KRRcXKAAAgAElEQVSmShIhhBCATqtnTMA4fjDjHnSa/pVi6DGR2Lp1a68nymyL0c2h2l2uCPnTBSmUNzTz2KaDXZKIxzcdBPqfTLTZWyiuycZsCu4cMATdV7RUVZX8qsMoikz5FEKI7+tvEgG9JBLbtm0DoKamhmPHjpGWlobdbufgwYPMmDGDyy+/vN8XFd6tqaWebTnvkxQxk8Rw15IARw9ThgeSTFjbGjlasp1w/7guiUR3qixFNLbUEh2UjFHv2+drCSGE6F6PicQzzzwDwD333MPnn3/eWRPCYrF0PvYQo1N5Qz52RxsGrdGl4x/bmMGazYd63N/fZMLfFEKgTziVDe01JXp73JFflQlAXOjkPl1DCCFGg8NFW5kcs6jLtv35m5kR73xMo9ORasXFxV0KS5nNZoqLi/sRphgpyuvzAQj3P7uGw1CLDk6hrrmCktocEnroHVFVFaPOlxC/6C7TVIUQYrQrqc2lpC6XE+X7abU1dW63q3ZyytLdk0gkJydz7bXXMmPGDDQaDRkZGcTHxw8scuG12mwt1DSWEOgT7vIjgo6eho6eh+/7yeykfo+TiApM4ljJDopqshgXNq3bGSSKojAldrFLFVmFEGI0CfQNp7m1AQDljEHoekXLktTrXGrDaSLxxBNPsH37drKyslBVldtvv12WER/FKiwFqKhEBIzr03k9JRMK8Ma+PBYmRHDL3L5XVjPoTET4x1NWn0e9tZJAn3AWL17cuV9Vv5ueOhiLiAkhhDfzNQSQGDGdiIB4zKbgfrXhNJFQFIWFCxeycOHCfl1AjCwVpx9rRAT0/bHG95OJh1dMY3HSGK7+99fc/u4ODpfW8vSqmei0fZuaGRuSSpu9pbPH4fnnn+/cd6r6CCW1OUyOORd/U0ifYxZCiJHsq2NvsnTCj/n04It091nrR7MfcNqG69V8hAASI6YT4BOG2di/N+UzH2F0fL/z3pVc9o8v+dM3RzlSVsdbN5xLkI/B5TbD/eO6Ha+hqiqnKjNpamvAoOt5ATkhhBit5iS0L3mxJPXaflf8lao8ok/8TaEkhKcN6DHBIxemdUkoksL82X7PRVw8MYZNx4tZ8OfPyK7o38Jwqqrywgsv8MILL1BlKaSxtY6owCSMOpnyKYQQ3/fF0X9jd9jYf2ozfsYg/IyBXb5cIT0SwmXWtkaMOt9BGWsQYDLwn1uW8sCG/Tz31RHm/fkz3r7hXJanRrt0vt1hI7Pwa1RUXnzxRQBmXzwOgPiwKW6PVwghPM1mb2Vr9jqaWy3YHTbS4pZ1KbhXXJvNvpMbURQNscGppMWdf1Yb/qYQXt/+MCoqr237bs0rlfYxbDctetJpHJJICJeoqsruEx+jKBoWJf9oUJIJrUbDH34wi8mRQfx03U4ueWULf7z0HO5elOr0elqNjsaWOhqsVRh9dOiNOiobCgjyHdPtomJCCOHtCqqPEmqOZWrsEizWGjZlvtolkdiV+zErptyCryGAzw69RHzYFIJ8x3RpY+mE6wHYlv0+C5Ov7Fcc8mhDuKSxpZam1nrMxuBBn/1w0+wktty1gjA/I/d+uIefvreTVpvd6XkxwSmoqMRNCCNxanu9iPhQ6Y0QQoxMCeFpTI1dAkBjSx2+xoDOfQ3WKox6H/yMQZ09EiW1OT221d8kAiSREC4qb+iYrTE0NUTmjwtn170rmRETwis7c7jw759TabH2ek5U0HhabVYW/iCZlJmRKIoGh+o8ARFCCG+2IeNvfJP1FnMSf9C5rbnVgkn3XbVfk95M0+l6Ee4miYRwiSeqWY4N9uPru1dw5bQ4vjlRzrw/f0ZmSU2Px1dZCmmxNeHjb0Sn16KqDg4VftVrFi6EEN7ukrS7WDbxJr49/k4vhfcGryCfJBLCqRZbM7VNZQT7Rg75NEo/o563b1jMIyumkVdtYeHz/+WjzIJujz1RcaBzdobBpOuyXQghRppKSyGNLbUAhJqjcagOrG2NAPgY/Glus3Qe29Raj68hoNt2BkoSCeFUZUP7G3f4ED3W+D6NRuHhC9N4+8bF2B0qP/zXVzz9ReZZmbfFWoNeayQkKJTExMQzttcOdchCCDHoyuryyCz6FoDm1gZsjhZMp5cu8DeF0Ga30mCtxqHaKag+RnRw8qDEIbM2hFMddRjMpv4VK3GXH6XFkxRq5op/fMWDn+4ns7SWl6+ej0mvBcBsCqbBWk2AT1iX8zwdtxBCDIbUyHlsy3mPTw++iN3RxrzEy8gp34dBayI+bArzki7nm+NvA5AQNm3QZrBJIiGc0mi0hPnHejoMAGbGhrLrvpVc+a+veHNfHjmV9ay/eSlRAb4khk8no2ALFkt7d57ZbAYgMXy6J0MWQohBodPqe11YKzIwkUvS7hr0OOTRhuhVc6sF6xnP2YaDyAAfvrhzBTeck8juU1XM/dNn7C2oIipoPGljl/HNV3vIOHQYf1MIaWOXERXU98XAhBBCuEZ6JESvTlQcoKD6CPOSLifIN8LT4XQy6bX889oFTI0M4rcb9rHk/zbyj2sXcKy8kdde2AOALv4eHkmWJEIIIQaTJBKiR6qqUtFwCp3GQIBPqKfDOYuiKPzqvMlMGBPI9a9v5cevtw868j+9v2OV0TPX9RBCCOFe8mhD9KjBWoW1zUK4/1g0itbT4fTokkmxrJ6V0O2+xzcd5LGNGUMckRBCjB6SSIgedRShGqpqlv312MYMXtie1eN+SSaEEGLwSCIhelTRcAoFDWH+Yz0dyoDZHYNX1U0IIUYzGSMhumVX26hvriLYLxK91ujpcHrVMQaiY0xE05KbzjrmzX15JISauWFWIjqt5M9CCOEu8ooquqVV9Jw3cTWTohd5OhSXPHJhGg+vmAaAPWIc9ohxAPxi8UR+tiiVorombntnB5Oe/ojX9uZiszs8GK0QQowckkiIHhl0Jq+qCnlmMgHw8IppPHvZOfz5ijlkP3g5dy5I4VRtIze/tZ0pf/iIN9JPYHdIQiGEEAMhiYTooqQ2h2+z3iXbuplvj7/rdStnPnJhGgmb/kjCpj92mfYZG+TH/105l6wHLueO+cnkVVu48c1tTHvmY97en4ejxxXzhBBC9EbGSIhOJbU5ZBRsodVmpVVtpMJyisbW9gWvvKk6pJ+m516GuGA/XrhqHr9dNoX/3XyIf+/N5frXt5IYaORJfTg/nBqHRqMMYbRCCOHdpEdCdOpYbrvVbgXAoDV12T6SjAsx8/I18zn628u4aXYSJ+tbuOa1b5j1xw18cOjUWSuLCiGE6J4kEqKTxVpDq62F1rYmFDToNPrT20fuMtxJYf7849oFvHtJEqtnJZJZWstV//qa2f/fp3yUWdBtQvHYxgypSyGEEKfJow3RyaQ3U91YAoqCQfEBpb2L35sGXPZXXICRf583iwcvmMKaTQd5+8BJrvjnV5wzNpSHV0xj5cQYFEXhsY0ZndNMQcpvCyHEoPZIZGVlccEFF/D666+ftW/nzp1cffXVXHvttTzwwAM4ZPS8R6mqSpu9FVV14GcIRKN8l2OOpmW4UyMCeX31uRz89Q/4UVo8ewuquPTVL1nwl8+44Y2tXZIIqZgphBCD2CPR1NTEmjVrmD9/frf7H374YV577TUiIyO55557+Pbbb1myZMlghSOcUBSFcxIuJrtsL622Joqtp/A3hZAYPt2rBloC3HrrrQNuY1JkEG/fuJj/V1LD45sOsv7gKXafqjrrOFkYTAgx2g1aImEwGHj55Zd5+eWXu92/fv16zGYzACEhIdTU1AxWKMIJVVVRFIUg3whmJ6wEIL0+nVnJszwcWf/cc889bmtralQwUyKDWH/wVI/HSDIhhBjNBu3Rhk6nw2Qy9bi/I4koLy9n27Zt0hvhIdWWYnbmfkhTa4OnQxFCCOGFFHWQ57k9//zzBAcHs3r16rP2VVVVcfvtt/PLX/6SRYt6LsXc0NBAVlbPqzuK/rGpLRS17sWOjWj9dEyaQE+H5BZ/+tOfALjvvvvc1uZLB8t5JbOy230KcFVKMLdOCSfEJOOXhRDulZKSgr+/v6fD6JHHXvUsFgu333479913X69JxJk8+ZeZnp7OrFne2dXfHYfDzq4TH+HbbGJi1ALiw6Z02e/N93vwYPujhr7E7+x+/z4Lor83YwPgymlxZBTXsC6rhs/yG/j10sn8YslEzEZ9/4IfAt78u+0Pud+RbSTfr7d8iPZYHYmnnnqKm266icWLF3sqhFHtSPE26poriA5KJi50sqfD8QrdreXx7k1LyPzNpTx/xRx89Foe3ZhBypMf8uL2LNpkYTAhxCgwaD0SmZmZPP300xQVFaHT6di4cSPLli0jNjaWRYsW8eGHH5Kfn897770HwKpVq7jmmmsGKxxxhoLqoxTWHCPAFMbkmHNRFCkJ7aozB1R2fK/XarhrUSo3nJPIH78+wnNfHeHu93fx52+O8r8rZ3DF1LHydyyEGLEGLZGYMmUKa9eu7XF/ZmbmYF1aOOGj98fXEMiM+OVoNfJMv696mp3hb9LzyIVp/M/8FNZsPsjLO7P50b+/Zl58GE+tmsm5iWOGOFIhhBh8UiJ7FArzj+XclB/hYxi+g3e8WWSAD3+9ci6Zv7mUH06LY2d+JUv/uonLXv2SI6Ujt9y4EGJ0kkRilHCodo6V7KTF1gyAoozcX/3EiROZOHGip8MgJTyAdTctYds9F3FuYgSfHCkk7dlPuP2dHRTVNXk6PCGEcAvp1x4ljpfsJL/qMA6HjUkxrs2S8VZvv/22p0PoYl58OF/etYJPjhTy4Ib9/GN3Dm/tz+PexRP5zXmTCfQxdDm+o+y2FLgSQngDSSRGgeKabPKrDmM2BpMSOdfT4YxKiqLwg8ljuXhCDP/em8uj/83gqS8yeXlHNv9v+VR+uiAFo04ri4IJIbyOJBIjXH1zJZlF36DTGJgRvwKddvjWN3CXjplAV111lYcjOZtOq+HWuclcNyOB5789xlNbMvnlf/byl2+PMiMmhA8OFXQeK6W3hRDeQBKJEazVZmV//mYcqp3pcRfgZxwZlSudWbNmDTA8E4kOvgYdvz1/CrfNS+aJzw/xl2+PcrK68azjJJkQQgx3I3fEnaDBWkWLrZmkiJlEBMR7OhzRjVA/IwEmPY5eCtXLcuVCiOFMeiRGsFBzDIuSr5JpniNAVWOLp0MQQohuSY/ECFTTWIrN3gqArzFAqioOc98vvd2dv247zoV//5wPD53CJqW3hRDDiPRIjDAN1mr2nvwUszGYeUmXSxLhJTrGQHx/UbD/d8EUJkcG8+L243yeVcLnWSXEBvpyx/xkbp2bTGSAjyfCFUKITpJIjCBt9lb252/G7rAxLmyaJBFe5vvJxMMrpnVuu2bGODJLanhxexZr00/w8H/bp4n+cFocdy5I5dzECPl9CyE8QhKJEUJVVQ4VfElTax0JYWlEBSV5OiSP+frrrz0dQr91tyhYhylRwfzflXN58pKZvL7vBC9uy+LdA/m8eyCfyZGB3LkgletnJRBgMny/WSGEGDSSSIwQueX7KG/IJ9QcQ3LkbE+H41FBQUGeDmFAnE319DfpuXNBKj+dn8K2vApe2H6c9w+e4mfrd3P/hn1cPzOROxemMDUq+KxzH9uYQXFxOX+fNVjRCyFGG0kkRgBrm4Xciv2Y9GbSxp6PZgSvo+GKoqIiAGJiYjwcyeBSFIVFiREsSozgjw3N/GNXDn/fkdX5tSghgp8uSOGH0+LOqpoZvTFDalMIIdxCEokRwKQ3MydhFVqNDoPO5OlwPG7lypUAZGSMntoLY/x9eOCCqfxm2WQ2HCnihe1ZbDpezNa8ciL+YyIl3J+teRWdx0uhKyGEu0gi4cXsDhsKChqNlmC/SE+HI4YBrUbDpVPGcumUseRU1vPSjmz+uvVYlySigyQTQgh3kETCy5TU5nCi4gAN1hpabc34GgJZnHqN9ESIs4wPC8DPoMNq67nuxOObDqKqKo9eNH0IIxNCjCSSSHiRktocMgq2ANDcaqG5tR5rm4WKhlPEBKd4ODrhrf627Tg6rYbrZiSQFCZVUIUQfTO6R+V5mRMVBwBos7fQ3FqPomgxG4M5WXnQyZlitHJWNXPSmEAaW+088t8MUp78kIV/+Yy/bj1GeUPzEEYphPBm0iPhRSzWGlRVpbGlDlDwNwWj0WixWGs9HZoYxnqqmtlR8Kre2soHhwp4c18eW7JL2ZlfyS/+s5flKVH8eGYCl00Zi9k48pefF0L0z6hLJDrGGFisNZhNwSSGTycqaLynw3KJ2RRMRUMBDocNo94PndZwert3101wtyeffNLTIQw7vVXNDDAZuGl2EjfNTqKkvol3D+Tz5r48/nusmP8eK8bXoOWyyWP58axElqdEodf23JHZsUqpDOAUYvQYVYnEmWMMoH1dio6fvSGZSAyfTpWlGK1G32VFz8RwGSh3po7pn6Krjjf34uLiHt/oowJ8uXfxRO5dPJHj5XW8te8kb+7L4639J3lr/0nC/Iz8KC2eH89MYP648C5luc+sU3Hm9YQQI9uoSiQ6xhioqoq1rRFFaa/BcKLigFckElFB45mdsJLc8gM0ttRiNgV5VY+K8LxHLkwjPd3m0rGpEYE8elEaj1w4jd2nKnlzXx7vHDjJC9uzeGF7FgkhZq6bOY4fz0zk3QMnuyQRMrVUiNFjVCUSFmvN6e9UrG0WVFQMWh+vGGNgbWtEq+iIChoviYMTl156KQAfffSRhyMZGRRFYW58OHPjw3nu0nP4PLuEN/fl8eGhAp74PJMnPs/s9jxJJoQYHUZVImE2BdNgrUZRNPgaAmhsqaWxtY5oL3hjPlayg8qGQuaPvwI/Y6CnwxnW8vPzPR3CiKXTarhoQgwXTYihsaWNm97axgeHCno8XpIJIQbX3rxPKas/iUN1MC12KfFhUzr3rdvzFH7GIBTaH0EuTr12UN4/RlUikRg+vXNMhFHnS4utiTablRC/aA9H1ruaxjJK604Q6BOOryHA0+EIAYCfUc/UqOBeEwmAL7JKWJ4Sxdz4MLQamXEuhLuU1OZS01TGJWl3YW1r5OMDf+mSSAAsn3wzeq1xUOMYVYlExyOB9lkbtUQExFPfXElFwylSHHPQaobfX4eqqhwv3QnAhKj5XQa3CeFpPU0t7aBVFLadrODc/9tIuNnIyomxrJoUy4rUKJlSKsQAjQlMIMx/LAAGnQ82exsO1THkCzcOv3fOQfb9MQZHi3eQX3WIsvo8ooOSPRhZ98rq86htKmNMwDhZT0MMS73Vqfj10kl8kV3KJ0cK2XCkiH/vyeXfe3IxaDUsHR/JDybFsmpyLHHBfi5dS6aXCvEdjaJBc7oMQHbZHmKCU89KInbkfIClpYaIgHHMir9oUD6MjrpE4vuSx8wi1BxNuH+cp0M5i8Nh53jpLhQ0pETO9XQ4QvSotzoVHYuIORwqewur+ORwIZ8cKWTT8WI2HS/m5x/sJi06mFWnk4pzYkPRaM5+sZPppUJ071TVYbJL97Jiyq1dts+IW05McCpGvQ9bjqwlvyqTcWFT3X79UZ9I6LQGIgLiPR1Gt1rtVnz0/kT4x8sAyz7omLUhhtaZb+zdvclrNApz4sKYExfG4xdP51RNIxuOFPLxkUK+zC4lo7iG//38EJH+PqycGMOqybFckByJn1F/VhIhgziFaFdUk8XBgi9ZPvmWsxZvHD9mVuf3sSGp1DSWSiIxmFpsTRwv2UVkYBIRAcOjd8Kk92N2wiWo9Lx6ozjbmjVrPB3CqNWXN/a4YD/uXJjKnQtTsbS0sTmrhI8PF/Lp0UL+sTuHf+zOwaTTEhvkS05lw1nnSzIhRrtWm5W9eZ+yYsptGPW+Z+376tgbnD/pJrQaHaV1eYz73kBMd5FE4rRWm5WS2hxqmkoJNUd7fOBli60Zo84HRVFQ0Ho0FiEGm9mo54qpcVwxNQ67w8HuU1V8cqSQV3dmd5tEdJBkQoxmeZUZWG2NfHXsjc5tUUFJBPtGEh82hdjgVDZk/A2dRk+IOZr4UPf3RsAgJxJZWVncdddd/OQnP2H16tVd9m3fvp0//vGPaLVaFi9ezN133z2YoTjlbwohPmwKJysPkVeR0aVLaKg1tdSzNXsdCeFpJI85x2NxeKuOtTYeeOABD0ci+kOr0TB/XDjzx4Vj0Gp6nBHS4cNDp4jwNzEvLpypUUHoelkLRIiRJDVyLqm9jJ+bFLOISTGLBj2OQUskmpqaWLNmDfPnz+92/+9//3teffVVxowZw+rVq7nwwgsZP96zhaHGR8yipDaXExUHiA5O9ljNhuOlu3CodszGYI9c39u9/fbbgCQSI4Gz6aVGrYaDJbX87P3dAPgatJwTG3q6EmcYc+PC+nQ9mRUiRN8NWiJhMBh4+eWXefnll8/aV1BQQGBgIFFRUQAsWbKEHTt2eDyR0GkNTIiaR0bBFo4Wb2fWuIuGPIaaxjLK6vMI8o0gMjBxyK8vxHDT2/TSh5ZP43hFPTvzK9iVX8mu/Eq25lXwzYnyzuMiffWce7iRefFhzI0PZ0ZMCCb92Y8LZVaIEP0zaImETqdDp+u++YqKCkJCQjp/DgkJoaCg9+p4QyUyMImC6mNUWgppbKkb0tkS7cWndgCQGinFp4To0Nv00oljApk4JpCb57R/EGmwtrG3sIpd+RXszK9kW24J6zLyWZfRXjpdr9UwIya4vdciLoy58WG8tieXNZsPdV5Pxl4I4TqvGmyZlZU1JNdxqEEEEsixzJwu29PT0wf1uhZ7OeW2XPw04Zw4VggUDur1nBns+x0sra2tQN/j99b77Q9vvNdVYVA8Jez097Ze7yEAWB4Ey4MCUKf5U9zYRmZlc/tXVRP7CqvYfaqK53u53uObDlJcXMwd0yLceyNDwBt/vwMx2u53uPFIIhEREUFlZWXnz2VlZUREOP/PmpKSgr+//2CG1qP09HRmzRrcAZi55ftprShh4fjL8TV6dk2NobjfwWIwtFd660v83ny/feXN9/r3foSdnp7OpUvmc2Z1EWubnf1F1Ty+MYNNWSU9nvtKZiWGgBD+8sM5XtND6M2/3/4Yyffb0NAwZB+gB8Ijw5tjY2OxWCwUFhZis9n48ssvWbhwoSdC6VVtUzk7cz+ksaVuSK6XFDGDpanXezyJ8Hbh4eGEh4d7OgwxjJn0WuaPC2feOOf/Tv62PYuE36/ntne2887+k1RarEMQoRDeY9B6JDIzM3n66acpKipCp9OxceNGli1bRmxsLMuXL+fRRx/lV7/6FQArV64kISFhsELpt+bWBmqbyjlash3UwXtjsjtsaBQtiqKg1w3uKm2jweeff+7pEISXcDYrZNWkGHwNOj7PKuGfu3P55+5cFAVmxoSwPDWaC1KiWDAuHKPOtVovMitEjESDlkhMmTKFtWvX9rh/9uzZvPPOO4N1ebeIDEyksOYYlQ0F+DoGr/Mmq3Q3dc3lpI29AB+DedCuI4Q4W2+zQjr22R0O9hfV8HlWMZuPl7DtZAXphdU89UUmvgYtS5IiWZ4SxfKUKCaOCez2MYjMChEjlVcNthxqiqIwMWoh23Leo8qejd1xvtsrXja21HGq6ggmgx9GnY9b2x6tvvrqKwCWLl3q0TiE9+htVgi0F8k6Z2wo54wN5f7zp2JpaeObE+VsPl7M5qwSPjtaxGdHiwCICfTlgtNJxQUpUYSbTbJWiBjRJJFwwmwKIiFsGhn135Jbvp+UyNlubT+rdBcqDlIj56LRSClsd7j33nsByMjI8HAkwps4W3TsTGajnpUTY1g5MQaAwtpGNmeVsPl4CV9kl3Qulw4Q6W+itOHscRWSTIiRQhIJFyRGzOBw3m5K6nIYHzHTbW/4NY2llNWfJMh3DGMCht8YESFGm/6+qccG+XHznPHcPGc8DofKgeJqNh8v4aUdWZysaezxPEkmxEggiYQLdBo9Y3RTmTN+gduSCFVVOVbSXnxqQtQ8r5laJoTonUajMDM2lJmxoVhtdqdrhbx74CQRZhPLkiNJCQ+Q1wLhdSSRcJFRY0an1QPtScBA/7M3tzXQ3NpAZGAiQb5j3BGiEGKYcTYrJMCk51h5PT9b375WSGygL+clR7IsOZLzk6OICfTt9ryePLYxg+Li8n7V2xCivySR6KPC6uPkV2UyN+lSdBp9v9vxNQSwOPVa7KrNjdEJIYab3maFPLxiGrlVDWzJLmVLdilf5pSydu8J1u49AUBqeADLkiNZlhzF0vFjCPHteXr4mQM6ozdmyOMSMWQkkeijptZ6GqxVnCjfT0rknH610dGjodMa0GFwc4RCiOGmt1kh48MCGB8WwB3zU3A4VA6V1rAlu5Qvskv5JreMF7Zn8cL2rM76FcuSo1iWHMmihAh8De0v4TIrRHiSJBJ9lBQxg+LabPIqDxIdlILZFNSn81ttVnad+IikiJlEB3l2tdORat26dZ4OQYizuDIrRKNRSIsOIS06hF8smUSb3cHuU5WneyxK2JFfSXphNc98eRiDVsP8ceEowFe5ZWe1NRjJhBTUEt2RRKKPtBodE6MWsP/UJo6WbOOccSv7NF4itzydxpZaWm3Ngxjl6JaSkuLpEIToVl/fgPVaDQsTIliYEMFDK6bR2NLG1rwKtmSXsCWnlK+7SSDO9Pimg1Q1tvDEJTMwG/v/KBakoJbomSQS/RAREE+Y/1gqGwoorTtBVFCSS+e1F586iq8hgLjQSYMc5ejVsfpnx+JdQowUfkY9F06I5sIJ0QDc/0k6z3x5pNdz/rrtOH/ddhw/g46oAB+iAnwY49/+Z5T/Gd+f/gr1NaLRdP1wNFSPTqTHwztJItEPiqIwKWohWy3rKK8/6XIicfzM4lOKFJ8aLLNntxcNk4JUYqR7atUsfPS6HmeFLEoIJzk8gJL6ZkrrmyltsLItrwKHqvbYpk6jdCYXY/xNFNc1sb+o5qzj3J1MSI+H95JEop98jQHMH385ZmOIS8dXN5ZQfgCw4p8AAA3DSURBVLr4VETAuMENTggxariyVsiZbHYHFY1WSuutlDQ0U1Lf1JlkfJdwNJNZUsveAnuv135800FeTz/BsuRI4oPNxAX7ERfkR3ywHzGBvui0rq1RJINFvZskEgPgbwrt/N6hOtAoPf+nKa7JBmBC1HwpOCOEcCtna4WcSafVEBXgS1SALzN6aVNVVR7csJ8/fHm412ufqLJwoirnrO0aRSEm0Oe7BOP0V/wZyYafUX9WEtFBkgnvIYnEADlUO0eLd9DUWtfrwMvJMecSE5xCkG/EEEcohBgNOt5wi4uL3fLmqygKT66aiUmv7fHRycMrpvGrpZM4VdPIqdpG8msa278//ZVfY2H7yQq25pV3e76PTkuzredeD0kmvIMkEgOkoMHaZqHKUkRpXS5RPUzpVBSFYL/IIY5OCDGaPHJhGunp7i1y58qjk0mRQUyK7H4qvM3uoKiuqT3JqP0uwThV00R6QWWviQTAp0cKGePvw5TIIKZEBRHk079B1DKQc/BIIjFAiqIwMXoBVVlFHCvZSbh/HDrtd//QT1YepLnVwvgx56DXyiwCIYT36cujk+/TaTXEh5iJDzF3u//R/x5gzeZDPZ6/t7CavYW7On8eG+TL5MggpkYFMyUqCG2NlSk2O0ZdzwPYZSDn4JJEwg18DQEkhk8npzydnPJ9TIiaB0CLrZmcsnQURcP4MVL8fqj88pe/9HQIQow4fVlmvS8evWg6iqJ02+Pxm2WTOVpWx6GSWg6X1nb++d9jxfz3WHHnsdqNeaSEB5xOMII6ey8SQ/xZs/mgTF0dZJJIuElCeBpFtVnkVx4iJjgFf1MIueX7sDnamBi1AL225xr5wr1uuukmT4cgxIg0WG+SvfV4dKykeqbqphYyTycVWw5mU2bTkVlay9GyOt7LyO88TqdRsDnOnuoqU1fdSxIJN2mveLmQHbkfsD37fewOGw3WaoJ8xzA2dKKnwxNCiGGtLz0eIb5GFieNYXHSGOaYLMyaNQtVVSmsbeJQaS2HS2p5Y98JDpXU9tjG45sO8sK248yIDSU20JfYIF9izvwz0JcgH4PTWXYydVUSCbeyO1rRafSoqDS21mF3tNFia6KsLq/HQZjC/W699VYAXn31VQ9HIoToi4G8+SqKwthgP8YG+7FyYgxNbbZeEwmAhpY2Nh0v7nG/r0FLbKAfsYG+xJyRYHQkHG/vy+O5r4+edd5oSyYkkXCjExUH0Gp02B022mxWdFoDBq2JExUHJJEYQnv37vV0CEIID+tptkmHjscnjS1tFNU3U1jbSGFdE0W1TRTVNbV/X9dEYW0TWRX1fb7+aEomJJFwI4u1vYysVqMjwCccRQEUsFh7z4qFEEK4nytTV/2MelLC9aSEB/TYTovNTvEZyUVRbRPvHcxn96mqwQvei0gi4UZmUzAN1moAdFr9Gdv7ttS4EEII9xjI1NUORp2WhFB/EkL9O7f96rzJPVbl7O91vJUkEm6UGD6djIIt3W4XQgjhGYM1dbWv65yMVJJIuFHHOIgTFQewWGsxm4JIDJ8u4yOEEMLDPDF1dbSQRMLNooLGS+LgYfPnz/d0CEKIUWSwejy8hSQSYsR58cUXPR2CEGKUGY0JRAfXFosXQgghhOiGJBJixHnllVd45ZVXPB2GEEKMCvJoQ4w4zz//PAC33XabhyMRQoiRT3okhBBCCNFvkkgIIYQQot8kkRBCCCFEv0kiIYQQQoh+84rBlg6HA4CmpiaPxtHQ0ODR6w81b73f8ePbC4L1NX5vvd/+GE33CnK/I91Ivd+O97yO98DhSlFVVfV0EM6UlZVRWFjo6TCEEEKIIRcbG8uYMWM8HUaPvKJHIjQ0FACTyYRGI09jhBBCjHwOhwOr1dr5HjhceUWPhBBCCCGGJ/l4L4QQQoh+k0RCCCGEEP0miYQQQggh+k0SCSGEEEL0m1fM2hhqf/jDH0hPT8dms/E///M/rFixonPfsmXLiIyMRKvVAvDss88O62k5zuzatYt7772X5ORkAFJSUnjooYc692/fvp0//vGPaLVaFi9ezN133+2pUAds3bp1fPTRR50/Z2Zmsn///s6fJ0+ezMyZMzt//te//tX5e/YmWVlZ3HXXXfzkJz9h9erVlJSU8Jvf/Aa73U54eDjPPPMMBoOhyzlPPPEEGRkZKIrCgw8+yLRp0zwUfd91d78PPPAANpsNnU7HM888Q3h4eOfxzv7ND3ffv9/777+fw4cPExQUBMCtt97K0qVLu5zjrb/f79/rPffcQ01NDQC1tbVMnz6dNWvWdB6/fv16/vznPxMXFwfAggULuPPOOz0S+6iiii527Nih3nbbbaqqqmp1dbW6ZMmSLvvPO+881WKxeCCywbFz50715z//eY/7L774YrW4uFi12+3qddddp2ZnZw9hdINn165d6qOPPtpl25w5czwUjfs0Njaqq1evVn/3u9+pa9euVVVVVe+//371008/VVVVVZ977jn1jTfe6HLOrl271DvuuENVVVXNyclRr7766qENegC6u9/f/OY36oYNG1RVVdXXX39dffrpp7uc4+zf/HDW3f3+9re/Vbds2dLjOd76++3uXs90//33qxkZGV22vf/+++pTTz01VCGK0+TRxvfMnj2bP//5zwAEBATQ3NyM3W73cFSeUVBQQGBgIFFRUWg0GpYsWfL/t3d3IU2/bxzH3+bKmkk5Y1lEDwiVRYRR5ANKT1IJK4MgB2YHEvSgoliaJ7kOQgoPggrLUVFEFHi0JJgUHUTIigx6IiI8McE1tUKjWFr/A/+Nv26r33/667vZ53W2+5pw3Vz3wfW97/vraG9vNzqtCXH+/HkOHTpkdBoTbtq0aTidTqxWa2DM4/GwefNmADZu3BhUw/b2drZs2QJAWloanz59YnBw8M8lPQ6h5ltfX8/WrVsBSE5O5uPHj0alN+FCzfd3YrW+v5prZ2cnAwMDMbOzMtmpkRgjPj4es9kMQEtLC3l5eUHb2/X19djtdhobG/kxCf4Nx9u3bzlw4AB2u52HDx8Gxn0+HxaLJfDZYrHg8/mMSHFCPXv2jHnz5o3a7gbw+/1UV1dTVFTElStXDMpufEwmE9OnTx819uXLl8BRRkpKSlANe3t7SU5ODnyOpTqHmq/ZbCY+Pp7h4WFu3LiBzWYL+rtwaz7ahZovwPXr1ykpKaGqqor+/v5RsVitb7i5Aly7do3i4uKQsUePHlFaWsq+fft49erVv5mi/JfuSIRx9+5dWlpauHz58qjxiooKcnNzmTVrFocPH8btdrNt2zaDshy/xYsXU1ZWxvbt2+nq6qKkpIS2tragM/TJpKWlhV27dgWN19TUsGPHDuLi4iguLmbt2rWsWrXKgAz/Pf+k8Z0MzfHw8DA1NTVkZmaSlZU1KjbZ1vzOnTuZPXs26enpNDc3c+7cOY4fPx72+7FeX7/fz5MnT3A4HEGx1atXY7FY2LBhA0+fPqW2tpbbt2//+ST/MtqRCOHBgwdcuHABp9NJUlLSqFhhYSEpKSmYTCby8vJ48+aNQVlOjLlz51JQUEBcXBwLFy5kzpw5eL1eAKxWK729vYHver3e/2tLNVp5PB4yMjKCxu12O4mJiZjNZjIzM2O+tj+ZzWa+fv0KhK7h2Dq/f/8+aLcm1tTV1bFo0SLKysqCYr9a87EoKyuL9PR0YOQy+Nh1O9nq+/jx47BHGmlpaYGLphkZGfT39/+1R9N/khqJMQYGBjh9+jQXL14M3IL+31hpaSl+vx8YWdA/b37HKpfLxaVLl4CRo4y+vr7AWygLFixgcHCQd+/eMTQ0xP3798nJyTEy3XHzer0kJiYGPX12dnZSXV3Njx8/GBoaoqOjI+Zr+1N2djZutxuAtrY2cnNzR8VzcnIC8ZcvX2K1Wpk5c+Yfz3OiuFwupk6dSkVFRdh4uDUfi8rLy+nq6gJGmuSx63ay1ff58+csX748ZMzpdNLa2gqMvPFhsVhi8s2rWKOjjTHu3LnDhw8fqKysDIytX7+eZcuWkZ+fT15eHnv27CEhIYEVK1bE9LEGjDzBHDlyhHv37vHt2zccDgetra0kJSWRn5+Pw+GguroagIKCApYsWWJwxuMz9t5Hc3Mz69atIyMjg9TUVHbv3s2UKVPYtGlTTF7kevHiBadOnaK7uxuTyYTb7aaxsZFjx45x69Yt5s+fT2FhIQBVVVU0NDSwZs0aVq5cSVFREXFxcdTX1xs8i38u1Hz7+vpISEhg7969wMhTqsPhCMw31JqPlWONUPMtLi6msrKSGTNmYDabaWhoAGK/vqHmevbsWXw+X+D1zp8OHjxIU1MTNpuNo0ePcvPmTYaGhjh58qRB2f9d9KNdIiIiEjEdbYiIiEjE1EiIiIhIxNRIiIiISMTUSIiIiEjE1EiIiIhIxNRIiEhIHo8Hu91udBoiEuXUSIiIiEjE1EiIyG+9fv0am81GT0+P0amISJRRIyEiv9TT00NtbS1nzpwhNTXV6HREJMqokRCRsD5//sz+/fspLy8nLS3N6HREJAqpkRCRsLq7u8nOzubq1at8//7d6HREJAqpkRCRsJYuXUpdXR1Wq5Wmpiaj0xGRKKRGQkR+68SJE7hcLjo6OoxORUSijH79U0RERCKmHQkRERGJmBoJERERiZgaCREREYmYGgkRERGJmBoJERERiZgaCREREYmYGgkRERGJmBoJERERidh/AGzJ2BvfwnTcAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f026c6ec610>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = df[\"Image_name\"]"
      ],
      "metadata": {
        "id": "2y8SaYPM-DBN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 7\n",
        "clusters = KMeans(k, random_state = 40)\n",
        "clusters.fit(k_list)\n",
        "image_cluster = pd.DataFrame(df_temp, columns = ['Image_name'])\n",
        "image_cluster[\"level\"] = clusters.labels_\n",
        "image_cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5xoERxPz9gqj",
        "outputId": "78c2b92d-7df1-4463-80b2-ccdf41c411d1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Image_name  level\n",
              "0        IMG_1      5\n",
              "1        IMG_2      5\n",
              "2        IMG_3      2\n",
              "3        IMG_4      3\n",
              "4        IMG_5      5\n",
              "..         ...    ...\n",
              "511    IMG_512      4\n",
              "512    IMG_513      3\n",
              "513    IMG_514      2\n",
              "514    IMG_515      3\n",
              "515    IMG_516      3\n",
              "\n",
              "[516 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6d438c82-61cb-4adc-be2d-3dff4554a720\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image_name</th>\n",
              "      <th>level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>IMG_1</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>IMG_2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IMG_3</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>IMG_4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>IMG_5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>IMG_512</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>IMG_513</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>IMG_514</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>IMG_515</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>IMG_516</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>516 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d438c82-61cb-4adc-be2d-3dff4554a720')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6d438c82-61cb-4adc-be2d-3dff4554a720 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6d438c82-61cb-4adc-be2d-3dff4554a720');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = image_cluster.copy()"
      ],
      "metadata": {
        "id": "KBazgyn3-R2m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = pd.DataFrame(columns = [\"Image_name\", \"level\"])\n",
        "for i in df[\"level\"].unique():\n",
        "  temp = df[df[\"level\"] == i]\n",
        "  f = int(len(temp)*0.7)\n",
        "  df_new = pd.concat([df_new, df.sample(n = f)])"
      ],
      "metadata": {
        "id": "ZlL5t0sSQc4X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.reset_index(inplace = True, drop = True)"
      ],
      "metadata": {
        "id": "Bvi17Q8eSE40"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_new"
      ],
      "metadata": {
        "id": "I7cbqLspSM7m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for i in df.Image_name:\n",
        "  image = cv2.imread(\"/content/drive/MyDrive/Data_set_DataMining/Images/\" + i + \".jpg\")\n",
        "  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  plt.imshow(image)\n",
        "\n",
        "  pixel_values = image.reshape((-1, 3))\n",
        "  pixel_values = np.float32(pixel_values)\n",
        "  criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.85)\n",
        "\n",
        "  k = 7\n",
        "  retval, labels, centers = cv2.kmeans(pixel_values, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
        "\n",
        "  centers = np.uint8(centers)\n",
        "  segmented_data = centers[labels.flatten()]\n",
        "\n",
        "  segmented_image = segmented_data.reshape((image.shape))\n",
        "\n",
        "  plt.imshow(segmented_image)\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.savefig('/content/drive/MyDrive/Data_set_DataMining/Segmented_Images/' + i + \".jpg\")"
      ],
      "metadata": {
        "id": "z7CjQHA0mNNB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "880bae1d-1b3c-420e-9d55-4a6b96926cdd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SENTIMENT_DATASET(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform = None, freq_threshold = 5):\n",
        "        self.df = csv_file\n",
        "        self.img = self.df['Image_name']\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(os.path.join(self.root_dir, self.df.iloc[index, 0])).convert(\"RGB\")\n",
        "        y_label = torch.tensor(int(self.df.iloc[index, 1]))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "        return (image, y_label)"
      ],
      "metadata": {
        "id": "WKObVJUitrkl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Image_name\"] = df[\"Image_name\"] + \".jpg\""
      ],
      "metadata": {
        "id": "8jdPKXzTzvgO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unsegmented_acc = []\n",
        "segmented_acc = []\n",
        "time_taken_segmented = []\n",
        "time_taken_unsegmented = []"
      ],
      "metadata": {
        "id": "XwzN_HfUwW2H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artifical Neural Network"
      ],
      "metadata": {
        "id": "qgEZ1o23nzHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.4, 0.5, 0.5]\n",
        "std = [0.4, 0.5, 0.5]\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))])\n",
        "dataset = SENTIMENT_DATASET(csv_file = df, root_dir = '/content/drive/MyDrive/Data_set_DataMining/Images/', transform = transform)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [284, 72])\n",
        "train_loader = DataLoader(dataset = train_set, batch_size = 8, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_set, batch_size = 8, shuffle = True)"
      ],
      "metadata": {
        "id": "EqdafRsvzDjD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neural_Network_Image(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Neural_Network_Image, self).__init__()\n",
        "        self.flat = nn.Flatten()\n",
        "        self.inn = nn.Linear(12288, 512)\n",
        "        self.hidden1 = nn.Linear(512, 256)\n",
        "        self.hidden2 = nn.Linear(256, 128)\n",
        "        self.hidden3 = nn.Linear(128, 64)\n",
        "        self.outt = nn.Linear(64, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flat(x)\n",
        "        x = torch.sigmoid(self.inn(x))\n",
        "        x = torch.sigmoid(self.hidden1(x))\n",
        "        x = torch.sigmoid(self.hidden2(x))\n",
        "        x = torch.sigmoid(self.hidden3(x))\n",
        "        x = self.outt(x)\n",
        "      \n",
        "        return x"
      ],
      "metadata": {
        "id": "KuTifKONzKru"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "net = Neural_Network_Image().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
        "loss_list = []\n",
        "tot_loss = 0\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        z = net(inputs)\n",
        "        loss = criterion(z, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "        tot_loss += loss.data\n",
        "        print(f'[{epoch+1}, {i+1:5d}] loss at each step: {tot_loss/len(loss_list)}')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        blank, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy is: {100 * correct // total} %')\n",
        "unsegmented_acc.append(round(100 * correct / total))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_segmented.append(TIME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_27xi7H0INV",
        "outputId": "bb964ad3-3513-47be-b071-7087b038a586"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss at each step: 1.9337384700775146\n",
            "[1,     2] loss at each step: 2.0730934143066406\n",
            "[1,     3] loss at each step: 2.105586051940918\n",
            "[1,     4] loss at each step: 2.0412213802337646\n",
            "[1,     5] loss at each step: 2.02789568901062\n",
            "[1,     6] loss at each step: 2.0208098888397217\n",
            "[1,     7] loss at each step: 2.008310079574585\n",
            "[1,     8] loss at each step: 2.024022102355957\n",
            "[1,     9] loss at each step: 2.0228607654571533\n",
            "[1,    10] loss at each step: 2.022033452987671\n",
            "[1,    11] loss at each step: 2.017922878265381\n",
            "[1,    12] loss at each step: 2.0120151042938232\n",
            "[1,    13] loss at each step: 2.002490520477295\n",
            "[1,    14] loss at each step: 2.007627248764038\n",
            "[1,    15] loss at each step: 2.005362033843994\n",
            "[1,    16] loss at each step: 2.0052566528320312\n",
            "[1,    17] loss at each step: 2.006920337677002\n",
            "[1,    18] loss at each step: 2.000955820083618\n",
            "[1,    19] loss at each step: 2.0051214694976807\n",
            "[1,    20] loss at each step: 2.0120415687561035\n",
            "[1,    21] loss at each step: 2.003371238708496\n",
            "[1,    22] loss at each step: 2.0032429695129395\n",
            "[1,    23] loss at each step: 2.0008859634399414\n",
            "[1,    24] loss at each step: 2.003894329071045\n",
            "[1,    25] loss at each step: 2.000291347503662\n",
            "[1,    26] loss at each step: 1.9947826862335205\n",
            "[1,    27] loss at each step: 1.9922047853469849\n",
            "[1,    28] loss at each step: 1.9871970415115356\n",
            "[1,    29] loss at each step: 1.9905834197998047\n",
            "[1,    30] loss at each step: 1.983188271522522\n",
            "[1,    31] loss at each step: 1.9823170900344849\n",
            "[1,    32] loss at each step: 1.9816526174545288\n",
            "[1,    33] loss at each step: 1.9783076047897339\n",
            "[1,    34] loss at each step: 1.9752318859100342\n",
            "[1,    35] loss at each step: 1.9722263813018799\n",
            "[1,    36] loss at each step: 1.97874116897583\n",
            "[2,     1] loss at each step: 1.9815113544464111\n",
            "[2,     2] loss at each step: 1.9810450077056885\n",
            "[2,     3] loss at each step: 1.974743366241455\n",
            "[2,     4] loss at each step: 1.9769392013549805\n",
            "[2,     5] loss at each step: 1.9777565002441406\n",
            "[2,     6] loss at each step: 1.977367639541626\n",
            "[2,     7] loss at each step: 1.9758199453353882\n",
            "[2,     8] loss at each step: 1.9777915477752686\n",
            "[2,     9] loss at each step: 1.97545325756073\n",
            "[2,    10] loss at each step: 1.9732327461242676\n",
            "[2,    11] loss at each step: 1.970237135887146\n",
            "[2,    12] loss at each step: 1.9702928066253662\n",
            "[2,    13] loss at each step: 1.9728444814682007\n",
            "[2,    14] loss at each step: 1.973341464996338\n",
            "[2,    15] loss at each step: 1.9732050895690918\n",
            "[2,    16] loss at each step: 1.9729399681091309\n",
            "[2,    17] loss at each step: 1.9737565517425537\n",
            "[2,    18] loss at each step: 1.9800057411193848\n",
            "[2,    19] loss at each step: 1.981089472770691\n",
            "[2,    20] loss at each step: 1.980297565460205\n",
            "[2,    21] loss at each step: 1.9827556610107422\n",
            "[2,    22] loss at each step: 1.9822382926940918\n",
            "[2,    23] loss at each step: 1.9827440977096558\n",
            "[2,    24] loss at each step: 1.9833800792694092\n",
            "[2,    25] loss at each step: 1.9818888902664185\n",
            "[2,    26] loss at each step: 1.9813357591629028\n",
            "[2,    27] loss at each step: 1.9800790548324585\n",
            "[2,    28] loss at each step: 1.978943109512329\n",
            "[2,    29] loss at each step: 1.9789987802505493\n",
            "[2,    30] loss at each step: 1.9794182777404785\n",
            "[2,    31] loss at each step: 1.9781031608581543\n",
            "[2,    32] loss at each step: 1.9749066829681396\n",
            "[2,    33] loss at each step: 1.9745475053787231\n",
            "[2,    34] loss at each step: 1.9746946096420288\n",
            "[2,    35] loss at each step: 1.9733766317367554\n",
            "[2,    36] loss at each step: 1.974449634552002\n",
            "[3,     1] loss at each step: 1.9774857759475708\n",
            "[3,     2] loss at each step: 1.9747788906097412\n",
            "[3,     3] loss at each step: 1.9735769033432007\n",
            "[3,     4] loss at each step: 1.9733366966247559\n",
            "[3,     5] loss at each step: 1.9697095155715942\n",
            "[3,     6] loss at each step: 1.971034288406372\n",
            "[3,     7] loss at each step: 1.9708151817321777\n",
            "[3,     8] loss at each step: 1.9718246459960938\n",
            "[3,     9] loss at each step: 1.9719358682632446\n",
            "[3,    10] loss at each step: 1.9726308584213257\n",
            "[3,    11] loss at each step: 1.9710569381713867\n",
            "[3,    12] loss at each step: 1.9703693389892578\n",
            "[3,    13] loss at each step: 1.969792127609253\n",
            "[3,    14] loss at each step: 1.9711343050003052\n",
            "[3,    15] loss at each step: 1.971518635749817\n",
            "[3,    16] loss at each step: 1.9706119298934937\n",
            "[3,    17] loss at each step: 1.9699275493621826\n",
            "[3,    18] loss at each step: 1.9690696001052856\n",
            "[3,    19] loss at each step: 1.9689688682556152\n",
            "[3,    20] loss at each step: 1.969320297241211\n",
            "[3,    21] loss at each step: 1.9699252843856812\n",
            "[3,    22] loss at each step: 1.9689494371414185\n",
            "[3,    23] loss at each step: 1.9684895277023315\n",
            "[3,    24] loss at each step: 1.9686825275421143\n",
            "[3,    25] loss at each step: 1.9679110050201416\n",
            "[3,    26] loss at each step: 1.9666407108306885\n",
            "[3,    27] loss at each step: 1.9655883312225342\n",
            "[3,    28] loss at each step: 1.9648029804229736\n",
            "[3,    29] loss at each step: 1.9643195867538452\n",
            "[3,    30] loss at each step: 1.963698387145996\n",
            "[3,    31] loss at each step: 1.96365225315094\n",
            "[3,    32] loss at each step: 1.9634093046188354\n",
            "[3,    33] loss at each step: 1.9635264873504639\n",
            "[3,    34] loss at each step: 1.963945984840393\n",
            "[3,    35] loss at each step: 1.963030219078064\n",
            "[3,    36] loss at each step: 1.9621284008026123\n",
            "[4,     1] loss at each step: 1.9622712135314941\n",
            "[4,     2] loss at each step: 1.962867021560669\n",
            "[4,     3] loss at each step: 1.9619154930114746\n",
            "[4,     4] loss at each step: 1.9617412090301514\n",
            "[4,     5] loss at each step: 1.9602864980697632\n",
            "[4,     6] loss at each step: 1.9610731601715088\n",
            "[4,     7] loss at each step: 1.9593603610992432\n",
            "[4,     8] loss at each step: 1.9592022895812988\n",
            "[4,     9] loss at each step: 1.9579678773880005\n",
            "[4,    10] loss at each step: 1.9581056833267212\n",
            "[4,    11] loss at each step: 1.9570889472961426\n",
            "[4,    12] loss at each step: 1.9566420316696167\n",
            "[4,    13] loss at each step: 1.9558285474777222\n",
            "[4,    14] loss at each step: 1.9561183452606201\n",
            "[4,    15] loss at each step: 1.9568933248519897\n",
            "[4,    16] loss at each step: 1.9574629068374634\n",
            "[4,    17] loss at each step: 1.957075834274292\n",
            "[4,    18] loss at each step: 1.9575831890106201\n",
            "[4,    19] loss at each step: 1.9571032524108887\n",
            "[4,    20] loss at each step: 1.9575625658035278\n",
            "[4,    21] loss at each step: 1.9571281671524048\n",
            "[4,    22] loss at each step: 1.956648588180542\n",
            "[4,    23] loss at each step: 1.9568195343017578\n",
            "[4,    24] loss at each step: 1.9572731256484985\n",
            "[4,    25] loss at each step: 1.9569865465164185\n",
            "[4,    26] loss at each step: 1.9561620950698853\n",
            "[4,    27] loss at each step: 1.955754041671753\n",
            "[4,    28] loss at each step: 1.9562162160873413\n",
            "[4,    29] loss at each step: 1.9558444023132324\n",
            "[4,    30] loss at each step: 1.9556708335876465\n",
            "[4,    31] loss at each step: 1.9548900127410889\n",
            "[4,    32] loss at each step: 1.9546397924423218\n",
            "[4,    33] loss at each step: 1.953994631767273\n",
            "[4,    34] loss at each step: 1.9533421993255615\n",
            "[4,    35] loss at each step: 1.9538708925247192\n",
            "[4,    36] loss at each step: 1.9533239603042603\n",
            "[5,     1] loss at each step: 1.9528264999389648\n",
            "[5,     2] loss at each step: 1.9527243375778198\n",
            "[5,     3] loss at each step: 1.9519034624099731\n",
            "[5,     4] loss at each step: 1.950907588005066\n",
            "[5,     5] loss at each step: 1.9503895044326782\n",
            "[5,     6] loss at each step: 1.9495278596878052\n",
            "[5,     7] loss at each step: 1.9500304460525513\n",
            "[5,     8] loss at each step: 1.9487104415893555\n",
            "[5,     9] loss at each step: 1.948194146156311\n",
            "[5,    10] loss at each step: 1.9477081298828125\n",
            "[5,    11] loss at each step: 1.9480795860290527\n",
            "[5,    12] loss at each step: 1.947731614112854\n",
            "[5,    13] loss at each step: 1.948453664779663\n",
            "[5,    14] loss at each step: 1.9486677646636963\n",
            "[5,    15] loss at each step: 1.948528528213501\n",
            "[5,    16] loss at each step: 1.9489644765853882\n",
            "[5,    17] loss at each step: 1.9488393068313599\n",
            "[5,    18] loss at each step: 1.9496053457260132\n",
            "[5,    19] loss at each step: 1.9493954181671143\n",
            "[5,    20] loss at each step: 1.9492692947387695\n",
            "[5,    21] loss at each step: 1.9486279487609863\n",
            "[5,    22] loss at each step: 1.9482239484786987\n",
            "[5,    23] loss at each step: 1.9478464126586914\n",
            "[5,    24] loss at each step: 1.9477622509002686\n",
            "[5,    25] loss at each step: 1.9487290382385254\n",
            "[5,    26] loss at each step: 1.9483935832977295\n",
            "[5,    27] loss at each step: 1.9488418102264404\n",
            "[5,    28] loss at each step: 1.9486327171325684\n",
            "[5,    29] loss at each step: 1.9486228227615356\n",
            "[5,    30] loss at each step: 1.9482786655426025\n",
            "[5,    31] loss at each step: 1.9483107328414917\n",
            "[5,    32] loss at each step: 1.9485647678375244\n",
            "[5,    33] loss at each step: 1.9481523036956787\n",
            "[5,    34] loss at each step: 1.9485446214675903\n",
            "[5,    35] loss at each step: 1.9481974840164185\n",
            "[5,    36] loss at each step: 1.9479180574417114\n",
            "[6,     1] loss at each step: 1.947298288345337\n",
            "[6,     2] loss at each step: 1.947121262550354\n",
            "[6,     3] loss at each step: 1.9467673301696777\n",
            "[6,     4] loss at each step: 1.9469037055969238\n",
            "[6,     5] loss at each step: 1.9468610286712646\n",
            "[6,     6] loss at each step: 1.9469451904296875\n",
            "[6,     7] loss at each step: 1.946785569190979\n",
            "[6,     8] loss at each step: 1.9469257593154907\n",
            "[6,     9] loss at each step: 1.9466091394424438\n",
            "[6,    10] loss at each step: 1.9461580514907837\n",
            "[6,    11] loss at each step: 1.9461430311203003\n",
            "[6,    12] loss at each step: 1.9459826946258545\n",
            "[6,    13] loss at each step: 1.945608139038086\n",
            "[6,    14] loss at each step: 1.945296287536621\n",
            "[6,    15] loss at each step: 1.945357322692871\n",
            "[6,    16] loss at each step: 1.9453940391540527\n",
            "[6,    17] loss at each step: 1.945183515548706\n",
            "[6,    18] loss at each step: 1.9448111057281494\n",
            "[6,    19] loss at each step: 1.9442399740219116\n",
            "[6,    20] loss at each step: 1.94412100315094\n",
            "[6,    21] loss at each step: 1.9442081451416016\n",
            "[6,    22] loss at each step: 1.944374918937683\n",
            "[6,    23] loss at each step: 1.9439473152160645\n",
            "[6,    24] loss at each step: 1.9433046579360962\n",
            "[6,    25] loss at each step: 1.943930983543396\n",
            "[6,    26] loss at each step: 1.943381428718567\n",
            "[6,    27] loss at each step: 1.943052887916565\n",
            "[6,    28] loss at each step: 1.9426286220550537\n",
            "[6,    29] loss at each step: 1.943519949913025\n",
            "[6,    30] loss at each step: 1.9436341524124146\n",
            "[6,    31] loss at each step: 1.943464756011963\n",
            "[6,    32] loss at each step: 1.9433913230895996\n",
            "[6,    33] loss at each step: 1.944130778312683\n",
            "[6,    34] loss at each step: 1.9442070722579956\n",
            "[6,    35] loss at each step: 1.9445019960403442\n",
            "[6,    36] loss at each step: 1.9437536001205444\n",
            "[7,     1] loss at each step: 1.9431345462799072\n",
            "[7,     2] loss at each step: 1.9429502487182617\n",
            "[7,     3] loss at each step: 1.9428130388259888\n",
            "[7,     4] loss at each step: 1.9425722360610962\n",
            "[7,     5] loss at each step: 1.9421881437301636\n",
            "[7,     6] loss at each step: 1.9427891969680786\n",
            "[7,     7] loss at each step: 1.9430898427963257\n",
            "[7,     8] loss at each step: 1.9426188468933105\n",
            "[7,     9] loss at each step: 1.942413330078125\n",
            "[7,    10] loss at each step: 1.942551851272583\n",
            "[7,    11] loss at each step: 1.9424805641174316\n",
            "[7,    12] loss at each step: 1.9428354501724243\n",
            "[7,    13] loss at each step: 1.9426158666610718\n",
            "[7,    14] loss at each step: 1.9417622089385986\n",
            "[7,    15] loss at each step: 1.9421240091323853\n",
            "[7,    16] loss at each step: 1.9418612718582153\n",
            "[7,    17] loss at each step: 1.9416828155517578\n",
            "[7,    18] loss at each step: 1.9412938356399536\n",
            "[7,    19] loss at each step: 1.9409692287445068\n",
            "[7,    20] loss at each step: 1.9411797523498535\n",
            "[7,    21] loss at each step: 1.9417691230773926\n",
            "[7,    22] loss at each step: 1.9420645236968994\n",
            "[7,    23] loss at each step: 1.942276954650879\n",
            "[7,    24] loss at each step: 1.9422266483306885\n",
            "[7,    25] loss at each step: 1.942078948020935\n",
            "[7,    26] loss at each step: 1.9420056343078613\n",
            "[7,    27] loss at each step: 1.9419020414352417\n",
            "[7,    28] loss at each step: 1.94157075881958\n",
            "[7,    29] loss at each step: 1.9413074254989624\n",
            "[7,    30] loss at each step: 1.9414992332458496\n",
            "[7,    31] loss at each step: 1.9414571523666382\n",
            "[7,    32] loss at each step: 1.9420799016952515\n",
            "[7,    33] loss at each step: 1.9420706033706665\n",
            "[7,    34] loss at each step: 1.9420171976089478\n",
            "[7,    35] loss at each step: 1.941523551940918\n",
            "[7,    36] loss at each step: 1.941298484802246\n",
            "[8,     1] loss at each step: 1.9409093856811523\n",
            "[8,     2] loss at each step: 1.9407232999801636\n",
            "[8,     3] loss at each step: 1.9405421018600464\n",
            "[8,     4] loss at each step: 1.9404879808425903\n",
            "[8,     5] loss at each step: 1.940198302268982\n",
            "[8,     6] loss at each step: 1.9403541088104248\n",
            "[8,     7] loss at each step: 1.9401062726974487\n",
            "[8,     8] loss at each step: 1.9403458833694458\n",
            "[8,     9] loss at each step: 1.9397072792053223\n",
            "[8,    10] loss at each step: 1.9394681453704834\n",
            "[8,    11] loss at each step: 1.9396229982376099\n",
            "[8,    12] loss at each step: 1.9395248889923096\n",
            "[8,    13] loss at each step: 1.9392783641815186\n",
            "[8,    14] loss at each step: 1.9397801160812378\n",
            "[8,    15] loss at each step: 1.9394186735153198\n",
            "[8,    16] loss at each step: 1.939480185508728\n",
            "[8,    17] loss at each step: 1.9395232200622559\n",
            "[8,    18] loss at each step: 1.9391618967056274\n",
            "[8,    19] loss at each step: 1.9392173290252686\n",
            "[8,    20] loss at each step: 1.93909752368927\n",
            "[8,    21] loss at each step: 1.9392452239990234\n",
            "[8,    22] loss at each step: 1.9393987655639648\n",
            "[8,    23] loss at each step: 1.9398304224014282\n",
            "[8,    24] loss at each step: 1.9398202896118164\n",
            "[8,    25] loss at each step: 1.9399299621582031\n",
            "[8,    26] loss at each step: 1.9399666786193848\n",
            "[8,    27] loss at each step: 1.9396634101867676\n",
            "[8,    28] loss at each step: 1.9393622875213623\n",
            "[8,    29] loss at each step: 1.9392139911651611\n",
            "[8,    30] loss at each step: 1.9390727281570435\n",
            "[8,    31] loss at each step: 1.9393134117126465\n",
            "[8,    32] loss at each step: 1.9390957355499268\n",
            "[8,    33] loss at each step: 1.9390575885772705\n",
            "[8,    34] loss at each step: 1.9391590356826782\n",
            "[8,    35] loss at each step: 1.9389797449111938\n",
            "[8,    36] loss at each step: 1.9389386177062988\n",
            "[9,     1] loss at each step: 1.9391812086105347\n",
            "[9,     2] loss at each step: 1.9390077590942383\n",
            "[9,     3] loss at each step: 1.9388844966888428\n",
            "[9,     4] loss at each step: 1.9391040802001953\n",
            "[9,     5] loss at each step: 1.9388066530227661\n",
            "[9,     6] loss at each step: 1.9387304782867432\n",
            "[9,     7] loss at each step: 1.9388781785964966\n",
            "[9,     8] loss at each step: 1.9392105340957642\n",
            "[9,     9] loss at each step: 1.9390733242034912\n",
            "[9,    10] loss at each step: 1.9385803937911987\n",
            "[9,    11] loss at each step: 1.938483715057373\n",
            "[9,    12] loss at each step: 1.9379942417144775\n",
            "[9,    13] loss at each step: 1.9382649660110474\n",
            "[9,    14] loss at each step: 1.9383138418197632\n",
            "[9,    15] loss at each step: 1.9380626678466797\n",
            "[9,    16] loss at each step: 1.9376758337020874\n",
            "[9,    17] loss at each step: 1.9376155138015747\n",
            "[9,    18] loss at each step: 1.9379007816314697\n",
            "[9,    19] loss at each step: 1.9375929832458496\n",
            "[9,    20] loss at each step: 1.9373366832733154\n",
            "[9,    21] loss at each step: 1.9367831945419312\n",
            "[9,    22] loss at each step: 1.9367352724075317\n",
            "[9,    23] loss at each step: 1.936977744102478\n",
            "[9,    24] loss at each step: 1.9369810819625854\n",
            "[9,    25] loss at each step: 1.9374561309814453\n",
            "[9,    26] loss at each step: 1.9375150203704834\n",
            "[9,    27] loss at each step: 1.9376980066299438\n",
            "[9,    28] loss at each step: 1.9376870393753052\n",
            "[9,    29] loss at each step: 1.9378011226654053\n",
            "[9,    30] loss at each step: 1.9380041360855103\n",
            "[9,    31] loss at each step: 1.9373533725738525\n",
            "[9,    32] loss at each step: 1.9373619556427002\n",
            "[9,    33] loss at each step: 1.9374020099639893\n",
            "[9,    34] loss at each step: 1.9372719526290894\n",
            "[9,    35] loss at each step: 1.9369943141937256\n",
            "[9,    36] loss at each step: 1.9369003772735596\n",
            "[10,     1] loss at each step: 1.9364885091781616\n",
            "[10,     2] loss at each step: 1.9367883205413818\n",
            "[10,     3] loss at each step: 1.9364839792251587\n",
            "[10,     4] loss at each step: 1.9367347955703735\n",
            "[10,     5] loss at each step: 1.9365746974945068\n",
            "[10,     6] loss at each step: 1.9365744590759277\n",
            "[10,     7] loss at each step: 1.9366395473480225\n",
            "[10,     8] loss at each step: 1.9364262819290161\n",
            "[10,     9] loss at each step: 1.936202883720398\n",
            "[10,    10] loss at each step: 1.936610460281372\n",
            "[10,    11] loss at each step: 1.9367575645446777\n",
            "[10,    12] loss at each step: 1.9368953704833984\n",
            "[10,    13] loss at each step: 1.9365545511245728\n",
            "[10,    14] loss at each step: 1.9364886283874512\n",
            "[10,    15] loss at each step: 1.936659574508667\n",
            "[10,    16] loss at each step: 1.9366567134857178\n",
            "[10,    17] loss at each step: 1.936646580696106\n",
            "[10,    18] loss at each step: 1.936856985092163\n",
            "[10,    19] loss at each step: 1.936668872833252\n",
            "[10,    20] loss at each step: 1.9367624521255493\n",
            "[10,    21] loss at each step: 1.9367696046829224\n",
            "[10,    22] loss at each step: 1.936729907989502\n",
            "[10,    23] loss at each step: 1.9368754625320435\n",
            "[10,    24] loss at each step: 1.937069296836853\n",
            "[10,    25] loss at each step: 1.937086820602417\n",
            "[10,    26] loss at each step: 1.9367085695266724\n",
            "[10,    27] loss at each step: 1.936772108078003\n",
            "[10,    28] loss at each step: 1.9367010593414307\n",
            "[10,    29] loss at each step: 1.9365500211715698\n",
            "[10,    30] loss at each step: 1.9363387823104858\n",
            "[10,    31] loss at each step: 1.9362577199935913\n",
            "[10,    32] loss at each step: 1.9362832307815552\n",
            "[10,    33] loss at each step: 1.935840129852295\n",
            "[10,    34] loss at each step: 1.9355133771896362\n",
            "[10,    35] loss at each step: 1.935524344444275\n",
            "[10,    36] loss at each step: 1.9355303049087524\n",
            "[11,     1] loss at each step: 1.9356085062026978\n",
            "[11,     2] loss at each step: 1.9356228113174438\n",
            "[11,     3] loss at each step: 1.9351593255996704\n",
            "[11,     4] loss at each step: 1.935160756111145\n",
            "[11,     5] loss at each step: 1.9350107908248901\n",
            "[11,     6] loss at each step: 1.9352580308914185\n",
            "[11,     7] loss at each step: 1.9354119300842285\n",
            "[11,     8] loss at each step: 1.9352961778640747\n",
            "[11,     9] loss at each step: 1.9349411725997925\n",
            "[11,    10] loss at each step: 1.9352962970733643\n",
            "[11,    11] loss at each step: 1.9351726770401\n",
            "[11,    12] loss at each step: 1.9355422258377075\n",
            "[11,    13] loss at each step: 1.9354193210601807\n",
            "[11,    14] loss at each step: 1.9355175495147705\n",
            "[11,    15] loss at each step: 1.935785174369812\n",
            "[11,    16] loss at each step: 1.935928463935852\n",
            "[11,    17] loss at each step: 1.9355872869491577\n",
            "[11,    18] loss at each step: 1.935604453086853\n",
            "[11,    19] loss at each step: 1.9354617595672607\n",
            "[11,    20] loss at each step: 1.935470461845398\n",
            "[11,    21] loss at each step: 1.9356341361999512\n",
            "[11,    22] loss at each step: 1.93531334400177\n",
            "[11,    23] loss at each step: 1.9353982210159302\n",
            "[11,    24] loss at each step: 1.9350488185882568\n",
            "[11,    25] loss at each step: 1.9352914094924927\n",
            "[11,    26] loss at each step: 1.9354197978973389\n",
            "[11,    27] loss at each step: 1.9351249933242798\n",
            "[11,    28] loss at each step: 1.9351600408554077\n",
            "[11,    29] loss at each step: 1.9350024461746216\n",
            "[11,    30] loss at each step: 1.9347201585769653\n",
            "[11,    31] loss at each step: 1.934517502784729\n",
            "[11,    32] loss at each step: 1.9342221021652222\n",
            "[11,    33] loss at each step: 1.9342774152755737\n",
            "[11,    34] loss at each step: 1.9341779947280884\n",
            "[11,    35] loss at each step: 1.9343160390853882\n",
            "[11,    36] loss at each step: 1.9343109130859375\n",
            "[12,     1] loss at each step: 1.9342875480651855\n",
            "[12,     2] loss at each step: 1.9343353509902954\n",
            "[12,     3] loss at each step: 1.9344933032989502\n",
            "[12,     4] loss at each step: 1.9343644380569458\n",
            "[12,     5] loss at each step: 1.934166431427002\n",
            "[12,     6] loss at each step: 1.9342135190963745\n",
            "[12,     7] loss at each step: 1.9340897798538208\n",
            "[12,     8] loss at each step: 1.9337464570999146\n",
            "[12,     9] loss at each step: 1.9340100288391113\n",
            "[12,    10] loss at each step: 1.9338946342468262\n",
            "[12,    11] loss at each step: 1.9339699745178223\n",
            "[12,    12] loss at each step: 1.934248924255371\n",
            "[12,    13] loss at each step: 1.9337968826293945\n",
            "[12,    14] loss at each step: 1.9339234828948975\n",
            "[12,    15] loss at each step: 1.9341716766357422\n",
            "[12,    16] loss at each step: 1.9342349767684937\n",
            "[12,    17] loss at each step: 1.9342689514160156\n",
            "[12,    18] loss at each step: 1.93434476852417\n",
            "[12,    19] loss at each step: 1.9338704347610474\n",
            "[12,    20] loss at each step: 1.9338968992233276\n",
            "[12,    21] loss at each step: 1.933921217918396\n",
            "[12,    22] loss at each step: 1.933473825454712\n",
            "[12,    23] loss at each step: 1.933466911315918\n",
            "[12,    24] loss at each step: 1.933536171913147\n",
            "[12,    25] loss at each step: 1.9332953691482544\n",
            "[12,    26] loss at each step: 1.9334769248962402\n",
            "[12,    27] loss at each step: 1.9338462352752686\n",
            "[12,    28] loss at each step: 1.9337981939315796\n",
            "[12,    29] loss at each step: 1.933455228805542\n",
            "[12,    30] loss at each step: 1.9332141876220703\n",
            "[12,    31] loss at each step: 1.933005928993225\n",
            "[12,    32] loss at each step: 1.9332454204559326\n",
            "[12,    33] loss at each step: 1.9331214427947998\n",
            "[12,    34] loss at each step: 1.9332444667816162\n",
            "[12,    35] loss at each step: 1.9332524538040161\n",
            "[12,    36] loss at each step: 1.9330228567123413\n",
            "[13,     1] loss at each step: 1.9326677322387695\n",
            "[13,     2] loss at each step: 1.9321883916854858\n",
            "[13,     3] loss at each step: 1.9321972131729126\n",
            "[13,     4] loss at each step: 1.9317851066589355\n",
            "[13,     5] loss at each step: 1.9318169355392456\n",
            "[13,     6] loss at each step: 1.9318896532058716\n",
            "[13,     7] loss at each step: 1.9321235418319702\n",
            "[13,     8] loss at each step: 1.9318418502807617\n",
            "[13,     9] loss at each step: 1.9317829608917236\n",
            "[13,    10] loss at each step: 1.9321595430374146\n",
            "[13,    11] loss at each step: 1.932253360748291\n",
            "[13,    12] loss at each step: 1.9316564798355103\n",
            "[13,    13] loss at each step: 1.9317678213119507\n",
            "[13,    14] loss at each step: 1.9318193197250366\n",
            "[13,    15] loss at each step: 1.931536316871643\n",
            "[13,    16] loss at each step: 1.9313240051269531\n",
            "[13,    17] loss at each step: 1.9308069944381714\n",
            "[13,    18] loss at each step: 1.931028127670288\n",
            "[13,    19] loss at each step: 1.9308218955993652\n",
            "[13,    20] loss at each step: 1.9313281774520874\n",
            "[13,    21] loss at each step: 1.931693434715271\n",
            "[13,    22] loss at each step: 1.9318060874938965\n",
            "[13,    23] loss at each step: 1.9322999715805054\n",
            "[13,    24] loss at each step: 1.9325416088104248\n",
            "[13,    25] loss at each step: 1.9322936534881592\n",
            "[13,    26] loss at each step: 1.931860327720642\n",
            "[13,    27] loss at each step: 1.93201744556427\n",
            "[13,    28] loss at each step: 1.932370662689209\n",
            "[13,    29] loss at each step: 1.9324097633361816\n",
            "[13,    30] loss at each step: 1.9321597814559937\n",
            "[13,    31] loss at each step: 1.9320213794708252\n",
            "[13,    32] loss at each step: 1.932334542274475\n",
            "[13,    33] loss at each step: 1.9324839115142822\n",
            "[13,    34] loss at each step: 1.9325084686279297\n",
            "[13,    35] loss at each step: 1.9324692487716675\n",
            "[13,    36] loss at each step: 1.9324572086334229\n",
            "[14,     1] loss at each step: 1.9320932626724243\n",
            "[14,     2] loss at each step: 1.93209707736969\n",
            "[14,     3] loss at each step: 1.9320673942565918\n",
            "[14,     4] loss at each step: 1.9318796396255493\n",
            "[14,     5] loss at each step: 1.9319441318511963\n",
            "[14,     6] loss at each step: 1.9319312572479248\n",
            "[14,     7] loss at each step: 1.9317090511322021\n",
            "[14,     8] loss at each step: 1.9315558671951294\n",
            "[14,     9] loss at each step: 1.9316000938415527\n",
            "[14,    10] loss at each step: 1.9317060708999634\n",
            "[14,    11] loss at each step: 1.931734561920166\n",
            "[14,    12] loss at each step: 1.931875228881836\n",
            "[14,    13] loss at each step: 1.9318441152572632\n",
            "[14,    14] loss at each step: 1.9316281080245972\n",
            "[14,    15] loss at each step: 1.9314851760864258\n",
            "[14,    16] loss at each step: 1.9312639236450195\n",
            "[14,    17] loss at each step: 1.9311847686767578\n",
            "[14,    18] loss at each step: 1.931355595588684\n",
            "[14,    19] loss at each step: 1.9314531087875366\n",
            "[14,    20] loss at each step: 1.9314017295837402\n",
            "[14,    21] loss at each step: 1.9315476417541504\n",
            "[14,    22] loss at each step: 1.9314855337142944\n",
            "[14,    23] loss at each step: 1.9315112829208374\n",
            "[14,    24] loss at each step: 1.9314329624176025\n",
            "[14,    25] loss at each step: 1.9315530061721802\n",
            "[14,    26] loss at each step: 1.931480884552002\n",
            "[14,    27] loss at each step: 1.9315682649612427\n",
            "[14,    28] loss at each step: 1.931718349456787\n",
            "[14,    29] loss at each step: 1.931715726852417\n",
            "[14,    30] loss at each step: 1.931886076927185\n",
            "[14,    31] loss at each step: 1.9318835735321045\n",
            "[14,    32] loss at each step: 1.9318008422851562\n",
            "[14,    33] loss at each step: 1.931892991065979\n",
            "[14,    34] loss at each step: 1.9317824840545654\n",
            "[14,    35] loss at each step: 1.9319136142730713\n",
            "[14,    36] loss at each step: 1.9316719770431519\n",
            "[15,     1] loss at each step: 1.9316540956497192\n",
            "[15,     2] loss at each step: 1.931683897972107\n",
            "[15,     3] loss at each step: 1.9317481517791748\n",
            "[15,     4] loss at each step: 1.9316933155059814\n",
            "[15,     5] loss at each step: 1.93193781375885\n",
            "[15,     6] loss at each step: 1.931787133216858\n",
            "[15,     7] loss at each step: 1.9321002960205078\n",
            "[15,     8] loss at each step: 1.9322032928466797\n",
            "[15,     9] loss at each step: 1.9322431087493896\n",
            "[15,    10] loss at each step: 1.9320306777954102\n",
            "[15,    11] loss at each step: 1.9320615530014038\n",
            "[15,    12] loss at each step: 1.9319747686386108\n",
            "[15,    13] loss at each step: 1.9319535493850708\n",
            "[15,    14] loss at each step: 1.931760311126709\n",
            "[15,    15] loss at each step: 1.9318885803222656\n",
            "[15,    16] loss at each step: 1.9319158792495728\n",
            "[15,    17] loss at each step: 1.9318368434906006\n",
            "[15,    18] loss at each step: 1.9320694208145142\n",
            "[15,    19] loss at each step: 1.9320166110992432\n",
            "[15,    20] loss at each step: 1.9319429397583008\n",
            "[15,    21] loss at each step: 1.9317573308944702\n",
            "[15,    22] loss at each step: 1.931545376777649\n",
            "[15,    23] loss at each step: 1.9316664934158325\n",
            "[15,    24] loss at each step: 1.931376338005066\n",
            "[15,    25] loss at each step: 1.9312745332717896\n",
            "[15,    26] loss at each step: 1.9312235116958618\n",
            "[15,    27] loss at each step: 1.931215763092041\n",
            "[15,    28] loss at each step: 1.9311918020248413\n",
            "[15,    29] loss at each step: 1.9313552379608154\n",
            "[15,    30] loss at each step: 1.9313875436782837\n",
            "[15,    31] loss at each step: 1.9312561750411987\n",
            "[15,    32] loss at each step: 1.9311949014663696\n",
            "[15,    33] loss at each step: 1.931183934211731\n",
            "[15,    34] loss at each step: 1.9311761856079102\n",
            "[15,    35] loss at each step: 1.931054949760437\n",
            "[15,    36] loss at each step: 1.9309064149856567\n",
            "[16,     1] loss at each step: 1.930769681930542\n",
            "[16,     2] loss at each step: 1.9310415983200073\n",
            "[16,     3] loss at each step: 1.931011438369751\n",
            "[16,     4] loss at each step: 1.9309771060943604\n",
            "[16,     5] loss at each step: 1.9310307502746582\n",
            "[16,     6] loss at each step: 1.9309473037719727\n",
            "[16,     7] loss at each step: 1.9308116436004639\n",
            "[16,     8] loss at each step: 1.9307456016540527\n",
            "[16,     9] loss at each step: 1.9307827949523926\n",
            "[16,    10] loss at each step: 1.9310005903244019\n",
            "[16,    11] loss at each step: 1.930907964706421\n",
            "[16,    12] loss at each step: 1.9305411577224731\n",
            "[16,    13] loss at each step: 1.9304442405700684\n",
            "[16,    14] loss at each step: 1.9304277896881104\n",
            "[16,    15] loss at each step: 1.9301782846450806\n",
            "[16,    16] loss at each step: 1.9299216270446777\n",
            "[16,    17] loss at each step: 1.9298677444458008\n",
            "[16,    18] loss at each step: 1.9298375844955444\n",
            "[16,    19] loss at each step: 1.9297298192977905\n",
            "[16,    20] loss at each step: 1.9297289848327637\n",
            "[16,    21] loss at each step: 1.9296226501464844\n",
            "[16,    22] loss at each step: 1.9295567274093628\n",
            "[16,    23] loss at each step: 1.9297878742218018\n",
            "[16,    24] loss at each step: 1.929957389831543\n",
            "[16,    25] loss at each step: 1.93015718460083\n",
            "[16,    26] loss at each step: 1.929936408996582\n",
            "[16,    27] loss at each step: 1.930163025856018\n",
            "[16,    28] loss at each step: 1.9301934242248535\n",
            "[16,    29] loss at each step: 1.930077075958252\n",
            "[16,    30] loss at each step: 1.9303195476531982\n",
            "[16,    31] loss at each step: 1.9303202629089355\n",
            "[16,    32] loss at each step: 1.9302268028259277\n",
            "[16,    33] loss at each step: 1.9303609132766724\n",
            "[16,    34] loss at each step: 1.9304673671722412\n",
            "[16,    35] loss at each step: 1.930477499961853\n",
            "[16,    36] loss at each step: 1.9304548501968384\n",
            "[17,     1] loss at each step: 1.930558681488037\n",
            "[17,     2] loss at each step: 1.9306814670562744\n",
            "[17,     3] loss at each step: 1.9306153059005737\n",
            "[17,     4] loss at each step: 1.9303165674209595\n",
            "[17,     5] loss at each step: 1.9305119514465332\n",
            "[17,     6] loss at each step: 1.9305590391159058\n",
            "[17,     7] loss at each step: 1.9306451082229614\n",
            "[17,     8] loss at each step: 1.9309642314910889\n",
            "[17,     9] loss at each step: 1.9308987855911255\n",
            "[17,    10] loss at each step: 1.930890679359436\n",
            "[17,    11] loss at each step: 1.931081771850586\n",
            "[17,    12] loss at each step: 1.9310779571533203\n",
            "[17,    13] loss at each step: 1.931071400642395\n",
            "[17,    14] loss at each step: 1.9309664964675903\n",
            "[17,    15] loss at each step: 1.9307656288146973\n",
            "[17,    16] loss at each step: 1.9307653903961182\n",
            "[17,    17] loss at each step: 1.9306519031524658\n",
            "[17,    18] loss at each step: 1.9305022954940796\n",
            "[17,    19] loss at each step: 1.9304579496383667\n",
            "[17,    20] loss at each step: 1.9304581880569458\n",
            "[17,    21] loss at each step: 1.9304908514022827\n",
            "[17,    22] loss at each step: 1.930493950843811\n",
            "[17,    23] loss at each step: 1.9305967092514038\n",
            "[17,    24] loss at each step: 1.9305336475372314\n",
            "[17,    25] loss at each step: 1.9303417205810547\n",
            "[17,    26] loss at each step: 1.9302406311035156\n",
            "[17,    27] loss at each step: 1.9301464557647705\n",
            "[17,    28] loss at each step: 1.9301103353500366\n",
            "[17,    29] loss at each step: 1.929970622062683\n",
            "[17,    30] loss at each step: 1.92971932888031\n",
            "[17,    31] loss at each step: 1.9297457933425903\n",
            "[17,    32] loss at each step: 1.92973792552948\n",
            "[17,    33] loss at each step: 1.9297778606414795\n",
            "[17,    34] loss at each step: 1.9296386241912842\n",
            "[17,    35] loss at each step: 1.9299856424331665\n",
            "[17,    36] loss at each step: 1.9298169612884521\n",
            "[18,     1] loss at each step: 1.9296482801437378\n",
            "[18,     2] loss at each step: 1.9296321868896484\n",
            "[18,     3] loss at each step: 1.9293402433395386\n",
            "[18,     4] loss at each step: 1.929185390472412\n",
            "[18,     5] loss at each step: 1.929240107536316\n",
            "[18,     6] loss at each step: 1.9294854402542114\n",
            "[18,     7] loss at each step: 1.9294743537902832\n",
            "[18,     8] loss at each step: 1.929125189781189\n",
            "[18,     9] loss at each step: 1.9291189908981323\n",
            "[18,    10] loss at each step: 1.9292172193527222\n",
            "[18,    11] loss at each step: 1.929107904434204\n",
            "[18,    12] loss at each step: 1.9290775060653687\n",
            "[18,    13] loss at each step: 1.9289631843566895\n",
            "[18,    14] loss at each step: 1.9287891387939453\n",
            "[18,    15] loss at each step: 1.9287630319595337\n",
            "[18,    16] loss at each step: 1.9287687540054321\n",
            "[18,    17] loss at each step: 1.9288239479064941\n",
            "[18,    18] loss at each step: 1.928657054901123\n",
            "[18,    19] loss at each step: 1.9287614822387695\n",
            "[18,    20] loss at each step: 1.9286280870437622\n",
            "[18,    21] loss at each step: 1.9289491176605225\n",
            "[18,    22] loss at each step: 1.9289768934249878\n",
            "[18,    23] loss at each step: 1.9288973808288574\n",
            "[18,    24] loss at each step: 1.9292093515396118\n",
            "[18,    25] loss at each step: 1.929349660873413\n",
            "[18,    26] loss at each step: 1.9294898509979248\n",
            "[18,    27] loss at each step: 1.9293200969696045\n",
            "[18,    28] loss at each step: 1.9294378757476807\n",
            "[18,    29] loss at each step: 1.9293570518493652\n",
            "[18,    30] loss at each step: 1.9293211698532104\n",
            "[18,    31] loss at each step: 1.9292737245559692\n",
            "[18,    32] loss at each step: 1.9292659759521484\n",
            "[18,    33] loss at each step: 1.9292603731155396\n",
            "[18,    34] loss at each step: 1.9293681383132935\n",
            "[18,    35] loss at each step: 1.9292510747909546\n",
            "[18,    36] loss at each step: 1.9294036626815796\n",
            "[19,     1] loss at each step: 1.9293192625045776\n",
            "[19,     2] loss at each step: 1.9292418956756592\n",
            "[19,     3] loss at each step: 1.9293899536132812\n",
            "[19,     4] loss at each step: 1.9295523166656494\n",
            "[19,     5] loss at each step: 1.9296776056289673\n",
            "[19,     6] loss at each step: 1.9295963048934937\n",
            "[19,     7] loss at each step: 1.9296772480010986\n",
            "[19,     8] loss at each step: 1.9296120405197144\n",
            "[19,     9] loss at each step: 1.9294699430465698\n",
            "[19,    10] loss at each step: 1.9296194314956665\n",
            "[19,    11] loss at each step: 1.9297428131103516\n",
            "[19,    12] loss at each step: 1.9298094511032104\n",
            "[19,    13] loss at each step: 1.9297595024108887\n",
            "[19,    14] loss at each step: 1.929733395576477\n",
            "[19,    15] loss at each step: 1.9297068119049072\n",
            "[19,    16] loss at each step: 1.9297873973846436\n",
            "[19,    17] loss at each step: 1.9299204349517822\n",
            "[19,    18] loss at each step: 1.9299007654190063\n",
            "[19,    19] loss at each step: 1.92994225025177\n",
            "[19,    20] loss at each step: 1.929714560508728\n",
            "[19,    21] loss at each step: 1.9296236038208008\n",
            "[19,    22] loss at each step: 1.9297091960906982\n",
            "[19,    23] loss at each step: 1.9296175241470337\n",
            "[19,    24] loss at each step: 1.9294524192810059\n",
            "[19,    25] loss at each step: 1.9291836023330688\n",
            "[19,    26] loss at each step: 1.929235577583313\n",
            "[19,    27] loss at each step: 1.92923903465271\n",
            "[19,    28] loss at each step: 1.9291470050811768\n",
            "[19,    29] loss at each step: 1.9291936159133911\n",
            "[19,    30] loss at each step: 1.9294087886810303\n",
            "[19,    31] loss at each step: 1.9292718172073364\n",
            "[19,    32] loss at each step: 1.9292399883270264\n",
            "[19,    33] loss at each step: 1.9292277097702026\n",
            "[19,    34] loss at each step: 1.9291393756866455\n",
            "[19,    35] loss at each step: 1.9289687871932983\n",
            "[19,    36] loss at each step: 1.928827166557312\n",
            "[20,     1] loss at each step: 1.9289934635162354\n",
            "[20,     2] loss at each step: 1.9287906885147095\n",
            "[20,     3] loss at each step: 1.928849220275879\n",
            "[20,     4] loss at each step: 1.9288500547409058\n",
            "[20,     5] loss at each step: 1.9290367364883423\n",
            "[20,     6] loss at each step: 1.9289852380752563\n",
            "[20,     7] loss at each step: 1.9289305210113525\n",
            "[20,     8] loss at each step: 1.9288886785507202\n",
            "[20,     9] loss at each step: 1.9288256168365479\n",
            "[20,    10] loss at each step: 1.9287819862365723\n",
            "[20,    11] loss at each step: 1.9287734031677246\n",
            "[20,    12] loss at each step: 1.9286518096923828\n",
            "[20,    13] loss at each step: 1.9287317991256714\n",
            "[20,    14] loss at each step: 1.9287385940551758\n",
            "[20,    15] loss at each step: 1.928676962852478\n",
            "[20,    16] loss at each step: 1.9288462400436401\n",
            "[20,    17] loss at each step: 1.9287437200546265\n",
            "[20,    18] loss at each step: 1.928684115409851\n",
            "[20,    19] loss at each step: 1.928633451461792\n",
            "[20,    20] loss at each step: 1.9286553859710693\n",
            "[20,    21] loss at each step: 1.9287221431732178\n",
            "[20,    22] loss at each step: 1.928523302078247\n",
            "[20,    23] loss at each step: 1.9284473657608032\n",
            "[20,    24] loss at each step: 1.9284087419509888\n",
            "[20,    25] loss at each step: 1.9284096956253052\n",
            "[20,    26] loss at each step: 1.92851722240448\n",
            "[20,    27] loss at each step: 1.9284201860427856\n",
            "[20,    28] loss at each step: 1.9284443855285645\n",
            "[20,    29] loss at each step: 1.9283510446548462\n",
            "[20,    30] loss at each step: 1.9283746480941772\n",
            "[20,    31] loss at each step: 1.9284991025924683\n",
            "[20,    32] loss at each step: 1.9287934303283691\n",
            "[20,    33] loss at each step: 1.9288394451141357\n",
            "[20,    34] loss at each step: 1.92865788936615\n",
            "[20,    35] loss at each step: 1.9285424947738647\n",
            "[20,    36] loss at each step: 1.9281812906265259\n",
            "Accuracy is: 22 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.4, 0.5, 0.5]\n",
        "std = [0.4, 0.5, 0.5]\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))])\n",
        "dataset = SENTIMENT_DATASET(csv_file = df, root_dir = '/content/drive/MyDrive/Data_set_DataMining/Segmented_Images/', transform = transform)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [284, 72])\n",
        "train_loader = DataLoader(dataset = train_set, batch_size = 8, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_set, batch_size = 8, shuffle = True)"
      ],
      "metadata": {
        "id": "wUd60mmx0IPr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "net = Neural_Network_Image().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
        "loss_list = []\n",
        "tot_loss = 0\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        z = net(inputs)\n",
        "        loss = criterion(z, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "        tot_loss += loss.data\n",
        "        print(f'[{epoch+1}, {i+1:5d}] loss at each step: {tot_loss/len(loss_list)}')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(inputs)\n",
        "        blank, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy is: {100 * correct // total} %')\n",
        "segmented_acc.append(round(100 * correct / total))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_unsegmented.append(TIME)"
      ],
      "metadata": {
        "id": "At-_NBKj2ibW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "910bdf2d-b070-4436-e1fa-2ef2c55a8085"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss at each step: 1.9714561700820923\n",
            "[1,     2] loss at each step: 1.924705982208252\n",
            "[1,     3] loss at each step: 2.106468915939331\n",
            "[1,     4] loss at each step: 2.2393546104431152\n",
            "[1,     5] loss at each step: 2.1825482845306396\n",
            "[1,     6] loss at each step: 2.203906536102295\n",
            "[1,     7] loss at each step: 2.160160541534424\n",
            "[1,     8] loss at each step: 2.160579204559326\n",
            "[1,     9] loss at each step: 2.1190834045410156\n",
            "[1,    10] loss at each step: 2.1064679622650146\n",
            "[1,    11] loss at each step: 2.099177837371826\n",
            "[1,    12] loss at each step: 2.082050085067749\n",
            "[1,    13] loss at each step: 2.066539764404297\n",
            "[1,    14] loss at each step: 2.0494611263275146\n",
            "[1,    15] loss at each step: 2.06323504447937\n",
            "[1,    16] loss at each step: 2.0646073818206787\n",
            "[1,    17] loss at each step: 2.087690591812134\n",
            "[1,    18] loss at each step: 2.090770721435547\n",
            "[1,    19] loss at each step: 2.08620023727417\n",
            "[1,    20] loss at each step: 2.0699591636657715\n",
            "[1,    21] loss at each step: 2.072902202606201\n",
            "[1,    22] loss at each step: 2.056436061859131\n",
            "[1,    23] loss at each step: 2.0627129077911377\n",
            "[1,    24] loss at each step: 2.0533626079559326\n",
            "[1,    25] loss at each step: 2.0443451404571533\n",
            "[1,    26] loss at each step: 2.0483343601226807\n",
            "[1,    27] loss at each step: 2.037755250930786\n",
            "[1,    28] loss at each step: 2.029991388320923\n",
            "[1,    29] loss at each step: 2.039364814758301\n",
            "[1,    30] loss at each step: 2.0361263751983643\n",
            "[1,    31] loss at each step: 2.034243583679199\n",
            "[1,    32] loss at each step: 2.028796434402466\n",
            "[1,    33] loss at each step: 2.029196262359619\n",
            "[1,    34] loss at each step: 2.023988962173462\n",
            "[1,    35] loss at each step: 2.022559881210327\n",
            "[1,    36] loss at each step: 2.0163681507110596\n",
            "[2,     1] loss at each step: 2.010530471801758\n",
            "[2,     2] loss at each step: 2.008493423461914\n",
            "[2,     3] loss at each step: 2.0097527503967285\n",
            "[2,     4] loss at each step: 2.005455493927002\n",
            "[2,     5] loss at each step: 2.0019237995147705\n",
            "[2,     6] loss at each step: 2.0032241344451904\n",
            "[2,     7] loss at each step: 2.0042574405670166\n",
            "[2,     8] loss at each step: 2.0050392150878906\n",
            "[2,     9] loss at each step: 2.001532793045044\n",
            "[2,    10] loss at each step: 2.0057122707366943\n",
            "[2,    11] loss at each step: 2.0047121047973633\n",
            "[2,    12] loss at each step: 2.0049424171447754\n",
            "[2,    13] loss at each step: 2.002091646194458\n",
            "[2,    14] loss at each step: 1.9999951124191284\n",
            "[2,    15] loss at each step: 1.9975154399871826\n",
            "[2,    16] loss at each step: 1.995364785194397\n",
            "[2,    17] loss at each step: 1.9936307668685913\n",
            "[2,    18] loss at each step: 1.9920064210891724\n",
            "[2,    19] loss at each step: 1.9903701543807983\n",
            "[2,    20] loss at each step: 1.98886239528656\n",
            "[2,    21] loss at each step: 1.9880943298339844\n",
            "[2,    22] loss at each step: 1.9875301122665405\n",
            "[2,    23] loss at each step: 1.9873614311218262\n",
            "[2,    24] loss at each step: 1.9878965616226196\n",
            "[2,    25] loss at each step: 1.9860241413116455\n",
            "[2,    26] loss at each step: 1.9855756759643555\n",
            "[2,    27] loss at each step: 1.986049771308899\n",
            "[2,    28] loss at each step: 1.9874305725097656\n",
            "[2,    29] loss at each step: 1.9852778911590576\n",
            "[2,    30] loss at each step: 1.983787178993225\n",
            "[2,    31] loss at each step: 1.9818834066390991\n",
            "[2,    32] loss at each step: 1.978825330734253\n",
            "[2,    33] loss at each step: 1.9780415296554565\n",
            "[2,    34] loss at each step: 1.9779942035675049\n",
            "[2,    35] loss at each step: 1.977571964263916\n",
            "[2,    36] loss at each step: 1.9766172170639038\n",
            "[3,     1] loss at each step: 1.9758329391479492\n",
            "[3,     2] loss at each step: 1.9728777408599854\n",
            "[3,     3] loss at each step: 1.971433162689209\n",
            "[3,     4] loss at each step: 1.97043776512146\n",
            "[3,     5] loss at each step: 1.969451904296875\n",
            "[3,     6] loss at each step: 1.9697864055633545\n",
            "[3,     7] loss at each step: 1.9688512086868286\n",
            "[3,     8] loss at each step: 1.968125343322754\n",
            "[3,     9] loss at each step: 1.9650652408599854\n",
            "[3,    10] loss at each step: 1.962214708328247\n",
            "[3,    11] loss at each step: 1.9623034000396729\n",
            "[3,    12] loss at each step: 1.9607903957366943\n",
            "[3,    13] loss at each step: 1.9613231420516968\n",
            "[3,    14] loss at each step: 1.9616175889968872\n",
            "[3,    15] loss at each step: 1.9625006914138794\n",
            "[3,    16] loss at each step: 1.9628329277038574\n",
            "[3,    17] loss at each step: 1.9635448455810547\n",
            "[3,    18] loss at each step: 1.9621292352676392\n",
            "[3,    19] loss at each step: 1.963178038597107\n",
            "[3,    20] loss at each step: 1.9651297330856323\n",
            "[3,    21] loss at each step: 1.9652273654937744\n",
            "[3,    22] loss at each step: 1.9653693437576294\n",
            "[3,    23] loss at each step: 1.9647741317749023\n",
            "[3,    24] loss at each step: 1.9645512104034424\n",
            "[3,    25] loss at each step: 1.9639149904251099\n",
            "[3,    26] loss at each step: 1.9637528657913208\n",
            "[3,    27] loss at each step: 1.9629967212677002\n",
            "[3,    28] loss at each step: 1.9625811576843262\n",
            "[3,    29] loss at each step: 1.962070345878601\n",
            "[3,    30] loss at each step: 1.962791919708252\n",
            "[3,    31] loss at each step: 1.9623522758483887\n",
            "[3,    32] loss at each step: 1.9619646072387695\n",
            "[3,    33] loss at each step: 1.9620143175125122\n",
            "[3,    34] loss at each step: 1.9615553617477417\n",
            "[3,    35] loss at each step: 1.96103036403656\n",
            "[3,    36] loss at each step: 1.960858702659607\n",
            "[4,     1] loss at each step: 1.960463285446167\n",
            "[4,     2] loss at each step: 1.9599987268447876\n",
            "[4,     3] loss at each step: 1.9597183465957642\n",
            "[4,     4] loss at each step: 1.9589533805847168\n",
            "[4,     5] loss at each step: 1.9592218399047852\n",
            "[4,     6] loss at each step: 1.9589331150054932\n",
            "[4,     7] loss at each step: 1.9581621885299683\n",
            "[4,     8] loss at each step: 1.9576549530029297\n",
            "[4,     9] loss at each step: 1.9573255777359009\n",
            "[4,    10] loss at each step: 1.9567104578018188\n",
            "[4,    11] loss at each step: 1.9561140537261963\n",
            "[4,    12] loss at each step: 1.9561161994934082\n",
            "[4,    13] loss at each step: 1.9559407234191895\n",
            "[4,    14] loss at each step: 1.9551236629486084\n",
            "[4,    15] loss at each step: 1.955248475074768\n",
            "[4,    16] loss at each step: 1.955216646194458\n",
            "[4,    17] loss at each step: 1.9545868635177612\n",
            "[4,    18] loss at each step: 1.9546340703964233\n",
            "[4,    19] loss at each step: 1.9542920589447021\n",
            "[4,    20] loss at each step: 1.954646348953247\n",
            "[4,    21] loss at each step: 1.95401930809021\n",
            "[4,    22] loss at each step: 1.9535133838653564\n",
            "[4,    23] loss at each step: 1.9535456895828247\n",
            "[4,    24] loss at each step: 1.9531909227371216\n",
            "[4,    25] loss at each step: 1.9536519050598145\n",
            "[4,    26] loss at each step: 1.9536186456680298\n",
            "[4,    27] loss at each step: 1.9537187814712524\n",
            "[4,    28] loss at each step: 1.953168511390686\n",
            "[4,    29] loss at each step: 1.953026294708252\n",
            "[4,    30] loss at each step: 1.9534510374069214\n",
            "[4,    31] loss at each step: 1.9524420499801636\n",
            "[4,    32] loss at each step: 1.9523472785949707\n",
            "[4,    33] loss at each step: 1.9526386260986328\n",
            "[4,    34] loss at each step: 1.953083872795105\n",
            "[4,    35] loss at each step: 1.9525549411773682\n",
            "[4,    36] loss at each step: 1.9522522687911987\n",
            "[5,     1] loss at each step: 1.952568531036377\n",
            "[5,     2] loss at each step: 1.9528498649597168\n",
            "[5,     3] loss at each step: 1.9529335498809814\n",
            "[5,     4] loss at each step: 1.9521971940994263\n",
            "[5,     5] loss at each step: 1.9509819746017456\n",
            "[5,     6] loss at each step: 1.9507558345794678\n",
            "[5,     7] loss at each step: 1.9506022930145264\n",
            "[5,     8] loss at each step: 1.9507927894592285\n",
            "[5,     9] loss at each step: 1.9493094682693481\n",
            "[5,    10] loss at each step: 1.9485652446746826\n",
            "[5,    11] loss at each step: 1.9485979080200195\n",
            "[5,    12] loss at each step: 1.9481139183044434\n",
            "[5,    13] loss at each step: 1.9482029676437378\n",
            "[5,    14] loss at each step: 1.9483212232589722\n",
            "[5,    15] loss at each step: 1.9480189085006714\n",
            "[5,    16] loss at each step: 1.947654366493225\n",
            "[5,    17] loss at each step: 1.947695016860962\n",
            "[5,    18] loss at each step: 1.9474866390228271\n",
            "[5,    19] loss at each step: 1.9478319883346558\n",
            "[5,    20] loss at each step: 1.9484930038452148\n",
            "[5,    21] loss at each step: 1.9484261274337769\n",
            "[5,    22] loss at each step: 1.9484996795654297\n",
            "[5,    23] loss at each step: 1.9478782415390015\n",
            "[5,    24] loss at each step: 1.9476664066314697\n",
            "[5,    25] loss at each step: 1.9473772048950195\n",
            "[5,    26] loss at each step: 1.9476821422576904\n",
            "[5,    27] loss at each step: 1.9467307329177856\n",
            "[5,    28] loss at each step: 1.9470281600952148\n",
            "[5,    29] loss at each step: 1.9468432664871216\n",
            "[5,    30] loss at each step: 1.945984959602356\n",
            "[5,    31] loss at each step: 1.945866346359253\n",
            "[5,    32] loss at each step: 1.9465243816375732\n",
            "[5,    33] loss at each step: 1.9465073347091675\n",
            "[5,    34] loss at each step: 1.9461818933486938\n",
            "[5,    35] loss at each step: 1.9461654424667358\n",
            "[5,    36] loss at each step: 1.9470852613449097\n",
            "[6,     1] loss at each step: 1.9465464353561401\n",
            "[6,     2] loss at each step: 1.9462922811508179\n",
            "[6,     3] loss at each step: 1.9456889629364014\n",
            "[6,     4] loss at each step: 1.9452760219573975\n",
            "[6,     5] loss at each step: 1.9446550607681274\n",
            "[6,     6] loss at each step: 1.9441004991531372\n",
            "[6,     7] loss at each step: 1.9436544179916382\n",
            "[6,     8] loss at each step: 1.944052815437317\n",
            "[6,     9] loss at each step: 1.9447120428085327\n",
            "[6,    10] loss at each step: 1.9447505474090576\n",
            "[6,    11] loss at each step: 1.9440805912017822\n",
            "[6,    12] loss at each step: 1.9440500736236572\n",
            "[6,    13] loss at each step: 1.9441806077957153\n",
            "[6,    14] loss at each step: 1.9440933465957642\n",
            "[6,    15] loss at each step: 1.944381594657898\n",
            "[6,    16] loss at each step: 1.9441251754760742\n",
            "[6,    17] loss at each step: 1.943910002708435\n",
            "[6,    18] loss at each step: 1.9438449144363403\n",
            "[6,    19] loss at each step: 1.94462251663208\n",
            "[6,    20] loss at each step: 1.944548487663269\n",
            "[6,    21] loss at each step: 1.9437766075134277\n",
            "[6,    22] loss at each step: 1.943935751914978\n",
            "[6,    23] loss at each step: 1.944011926651001\n",
            "[6,    24] loss at each step: 1.9442558288574219\n",
            "[6,    25] loss at each step: 1.943848967552185\n",
            "[6,    26] loss at each step: 1.9444023370742798\n",
            "[6,    27] loss at each step: 1.9444358348846436\n",
            "[6,    28] loss at each step: 1.9443159103393555\n",
            "[6,    29] loss at each step: 1.944278597831726\n",
            "[6,    30] loss at each step: 1.9438618421554565\n",
            "[6,    31] loss at each step: 1.9433776140213013\n",
            "[6,    32] loss at each step: 1.943464756011963\n",
            "[6,    33] loss at each step: 1.943274974822998\n",
            "[6,    34] loss at each step: 1.9435967206954956\n",
            "[6,    35] loss at each step: 1.9433988332748413\n",
            "[6,    36] loss at each step: 1.9431015253067017\n",
            "[7,     1] loss at each step: 1.9432709217071533\n",
            "[7,     2] loss at each step: 1.942945122718811\n",
            "[7,     3] loss at each step: 1.9430581331253052\n",
            "[7,     4] loss at each step: 1.9432235956192017\n",
            "[7,     5] loss at each step: 1.9427992105484009\n",
            "[7,     6] loss at each step: 1.9431337118148804\n",
            "[7,     7] loss at each step: 1.9433813095092773\n",
            "[7,     8] loss at each step: 1.9432061910629272\n",
            "[7,     9] loss at each step: 1.9431509971618652\n",
            "[7,    10] loss at each step: 1.9431264400482178\n",
            "[7,    11] loss at each step: 1.943324089050293\n",
            "[7,    12] loss at each step: 1.9430760145187378\n",
            "[7,    13] loss at each step: 1.9429948329925537\n",
            "[7,    14] loss at each step: 1.942999005317688\n",
            "[7,    15] loss at each step: 1.942968726158142\n",
            "[7,    16] loss at each step: 1.9430856704711914\n",
            "[7,    17] loss at each step: 1.942557692527771\n",
            "[7,    18] loss at each step: 1.9424110651016235\n",
            "[7,    19] loss at each step: 1.942708969116211\n",
            "[7,    20] loss at each step: 1.9425469636917114\n",
            "[7,    21] loss at each step: 1.9423413276672363\n",
            "[7,    22] loss at each step: 1.9418193101882935\n",
            "[7,    23] loss at each step: 1.9420602321624756\n",
            "[7,    24] loss at each step: 1.9416648149490356\n",
            "[7,    25] loss at each step: 1.9413580894470215\n",
            "[7,    26] loss at each step: 1.9409449100494385\n",
            "[7,    27] loss at each step: 1.941104531288147\n",
            "[7,    28] loss at each step: 1.941404938697815\n",
            "[7,    29] loss at each step: 1.9414663314819336\n",
            "[7,    30] loss at each step: 1.9412730932235718\n",
            "[7,    31] loss at each step: 1.9409799575805664\n",
            "[7,    32] loss at each step: 1.9410059452056885\n",
            "[7,    33] loss at each step: 1.9408479928970337\n",
            "[7,    34] loss at each step: 1.9406042098999023\n",
            "[7,    35] loss at each step: 1.9401118755340576\n",
            "[7,    36] loss at each step: 1.9397802352905273\n",
            "[8,     1] loss at each step: 1.9399237632751465\n",
            "[8,     2] loss at each step: 1.9399480819702148\n",
            "[8,     3] loss at each step: 1.939941167831421\n",
            "[8,     4] loss at each step: 1.9400125741958618\n",
            "[8,     5] loss at each step: 1.940466284751892\n",
            "[8,     6] loss at each step: 1.9404207468032837\n",
            "[8,     7] loss at each step: 1.9400882720947266\n",
            "[8,     8] loss at each step: 1.940172791481018\n",
            "[8,     9] loss at each step: 1.9399877786636353\n",
            "[8,    10] loss at each step: 1.940017580986023\n",
            "[8,    11] loss at each step: 1.9396820068359375\n",
            "[8,    12] loss at each step: 1.9396777153015137\n",
            "[8,    13] loss at each step: 1.9397422075271606\n",
            "[8,    14] loss at each step: 1.9393455982208252\n",
            "[8,    15] loss at each step: 1.9394789934158325\n",
            "[8,    16] loss at each step: 1.9391276836395264\n",
            "[8,    17] loss at each step: 1.938988447189331\n",
            "[8,    18] loss at each step: 1.9388556480407715\n",
            "[8,    19] loss at each step: 1.9383385181427002\n",
            "[8,    20] loss at each step: 1.9384081363677979\n",
            "[8,    21] loss at each step: 1.9387857913970947\n",
            "[8,    22] loss at each step: 1.939073920249939\n",
            "[8,    23] loss at each step: 1.9394391775131226\n",
            "[8,    24] loss at each step: 1.9396374225616455\n",
            "[8,    25] loss at each step: 1.9396934509277344\n",
            "[8,    26] loss at each step: 1.9399288892745972\n",
            "[8,    27] loss at each step: 1.9397392272949219\n",
            "[8,    28] loss at each step: 1.9390329122543335\n",
            "[8,    29] loss at each step: 1.939253330230713\n",
            "[8,    30] loss at each step: 1.9386814832687378\n",
            "[8,    31] loss at each step: 1.9384323358535767\n",
            "[8,    32] loss at each step: 1.9383738040924072\n",
            "[8,    33] loss at each step: 1.9381028413772583\n",
            "[8,    34] loss at each step: 1.9383864402770996\n",
            "[8,    35] loss at each step: 1.938169240951538\n",
            "[8,    36] loss at each step: 1.9379098415374756\n",
            "[9,     1] loss at each step: 1.9378464221954346\n",
            "[9,     2] loss at each step: 1.9375407695770264\n",
            "[9,     3] loss at each step: 1.9374769926071167\n",
            "[9,     4] loss at each step: 1.9374600648880005\n",
            "[9,     5] loss at each step: 1.9375957250595093\n",
            "[9,     6] loss at each step: 1.937723994255066\n",
            "[9,     7] loss at each step: 1.937813401222229\n",
            "[9,     8] loss at each step: 1.9372212886810303\n",
            "[9,     9] loss at each step: 1.9371471405029297\n",
            "[9,    10] loss at each step: 1.9371047019958496\n",
            "[9,    11] loss at each step: 1.9373923540115356\n",
            "[9,    12] loss at each step: 1.9375940561294556\n",
            "[9,    13] loss at each step: 1.9373456239700317\n",
            "[9,    14] loss at each step: 1.9372614622116089\n",
            "[9,    15] loss at each step: 1.9374029636383057\n",
            "[9,    16] loss at each step: 1.9372111558914185\n",
            "[9,    17] loss at each step: 1.937422513961792\n",
            "[9,    18] loss at each step: 1.9367936849594116\n",
            "[9,    19] loss at each step: 1.936645269393921\n",
            "[9,    20] loss at each step: 1.937168836593628\n",
            "[9,    21] loss at each step: 1.9364508390426636\n",
            "[9,    22] loss at each step: 1.9365202188491821\n",
            "[9,    23] loss at each step: 1.9372055530548096\n",
            "[9,    24] loss at each step: 1.936855435371399\n",
            "[9,    25] loss at each step: 1.9366739988327026\n",
            "[9,    26] loss at each step: 1.9362980127334595\n",
            "[9,    27] loss at each step: 1.9366405010223389\n",
            "[9,    28] loss at each step: 1.9368896484375\n",
            "[9,    29] loss at each step: 1.9366569519042969\n",
            "[9,    30] loss at each step: 1.9368177652359009\n",
            "[9,    31] loss at each step: 1.9363293647766113\n",
            "[9,    32] loss at each step: 1.9361189603805542\n",
            "[9,    33] loss at each step: 1.9362465143203735\n",
            "[9,    34] loss at each step: 1.9362863302230835\n",
            "[9,    35] loss at each step: 1.9366278648376465\n",
            "[9,    36] loss at each step: 1.9363155364990234\n",
            "[10,     1] loss at each step: 1.935967206954956\n",
            "[10,     2] loss at each step: 1.935829520225525\n",
            "[10,     3] loss at each step: 1.9353986978530884\n",
            "[10,     4] loss at each step: 1.9356074333190918\n",
            "[10,     5] loss at each step: 1.9357541799545288\n",
            "[10,     6] loss at each step: 1.9360371828079224\n",
            "[10,     7] loss at each step: 1.9359750747680664\n",
            "[10,     8] loss at each step: 1.9359793663024902\n",
            "[10,     9] loss at each step: 1.9358346462249756\n",
            "[10,    10] loss at each step: 1.9356715679168701\n",
            "[10,    11] loss at each step: 1.9353785514831543\n",
            "[10,    12] loss at each step: 1.9356228113174438\n",
            "[10,    13] loss at each step: 1.9355409145355225\n",
            "[10,    14] loss at each step: 1.9356309175491333\n",
            "[10,    15] loss at each step: 1.935129165649414\n",
            "[10,    16] loss at each step: 1.9349157810211182\n",
            "[10,    17] loss at each step: 1.9348375797271729\n",
            "[10,    18] loss at each step: 1.9350887537002563\n",
            "[10,    19] loss at each step: 1.9353679418563843\n",
            "[10,    20] loss at each step: 1.9355087280273438\n",
            "[10,    21] loss at each step: 1.9357247352600098\n",
            "[10,    22] loss at each step: 1.9359561204910278\n",
            "[10,    23] loss at each step: 1.935672640800476\n",
            "[10,    24] loss at each step: 1.9359958171844482\n",
            "[10,    25] loss at each step: 1.9360476732254028\n",
            "[10,    26] loss at each step: 1.9355043172836304\n",
            "[10,    27] loss at each step: 1.9352799654006958\n",
            "[10,    28] loss at each step: 1.935236930847168\n",
            "[10,    29] loss at each step: 1.9354171752929688\n",
            "[10,    30] loss at each step: 1.9355276823043823\n",
            "[10,    31] loss at each step: 1.9355055093765259\n",
            "[10,    32] loss at each step: 1.9353442192077637\n",
            "[10,    33] loss at each step: 1.9349604845046997\n",
            "[10,    34] loss at each step: 1.9349383115768433\n",
            "[10,    35] loss at each step: 1.9349303245544434\n",
            "[10,    36] loss at each step: 1.9349523782730103\n",
            "[11,     1] loss at each step: 1.93509840965271\n",
            "[11,     2] loss at each step: 1.9352176189422607\n",
            "[11,     3] loss at each step: 1.9349696636199951\n",
            "[11,     4] loss at each step: 1.935104250907898\n",
            "[11,     5] loss at each step: 1.9347766637802124\n",
            "[11,     6] loss at each step: 1.934722900390625\n",
            "[11,     7] loss at each step: 1.9346246719360352\n",
            "[11,     8] loss at each step: 1.934532880783081\n",
            "[11,     9] loss at each step: 1.9344627857208252\n",
            "[11,    10] loss at each step: 1.9341273307800293\n",
            "[11,    11] loss at each step: 1.9340171813964844\n",
            "[11,    12] loss at each step: 1.933545470237732\n",
            "[11,    13] loss at each step: 1.933372139930725\n",
            "[11,    14] loss at each step: 1.9335455894470215\n",
            "[11,    15] loss at each step: 1.933761715888977\n",
            "[11,    16] loss at each step: 1.9338895082473755\n",
            "[11,    17] loss at each step: 1.9337565898895264\n",
            "[11,    18] loss at each step: 1.9335980415344238\n",
            "[11,    19] loss at each step: 1.9330763816833496\n",
            "[11,    20] loss at each step: 1.9329044818878174\n",
            "[11,    21] loss at each step: 1.9328975677490234\n",
            "[11,    22] loss at each step: 1.9331477880477905\n",
            "[11,    23] loss at each step: 1.9331883192062378\n",
            "[11,    24] loss at each step: 1.9334931373596191\n",
            "[11,    25] loss at each step: 1.9332842826843262\n",
            "[11,    26] loss at each step: 1.9332246780395508\n",
            "[11,    27] loss at each step: 1.9331870079040527\n",
            "[11,    28] loss at each step: 1.933429479598999\n",
            "[11,    29] loss at each step: 1.9334652423858643\n",
            "[11,    30] loss at each step: 1.9335724115371704\n",
            "[11,    31] loss at each step: 1.9331797361373901\n",
            "[11,    32] loss at each step: 1.9338874816894531\n",
            "[11,    33] loss at each step: 1.933666467666626\n",
            "[11,    34] loss at each step: 1.9337579011917114\n",
            "[11,    35] loss at each step: 1.933774709701538\n",
            "[11,    36] loss at each step: 1.9339733123779297\n",
            "[12,     1] loss at each step: 1.9339234828948975\n",
            "[12,     2] loss at each step: 1.933703064918518\n",
            "[12,     3] loss at each step: 1.9340131282806396\n",
            "[12,     4] loss at each step: 1.9340308904647827\n",
            "[12,     5] loss at each step: 1.9336638450622559\n",
            "[12,     6] loss at each step: 1.9336875677108765\n",
            "[12,     7] loss at each step: 1.933868646621704\n",
            "[12,     8] loss at each step: 1.9340726137161255\n",
            "[12,     9] loss at each step: 1.934300184249878\n",
            "[12,    10] loss at each step: 1.9339910745620728\n",
            "[12,    11] loss at each step: 1.9340711832046509\n",
            "[12,    12] loss at each step: 1.9342279434204102\n",
            "[12,    13] loss at each step: 1.9342857599258423\n",
            "[12,    14] loss at each step: 1.9344267845153809\n",
            "[12,    15] loss at each step: 1.9341861009597778\n",
            "[12,    16] loss at each step: 1.933918833732605\n",
            "[12,    17] loss at each step: 1.9337302446365356\n",
            "[12,    18] loss at each step: 1.9337027072906494\n",
            "[12,    19] loss at each step: 1.933619499206543\n",
            "[12,    20] loss at each step: 1.933414340019226\n",
            "[12,    21] loss at each step: 1.9335297346115112\n",
            "[12,    22] loss at each step: 1.933897852897644\n",
            "[12,    23] loss at each step: 1.9338520765304565\n",
            "[12,    24] loss at each step: 1.9338760375976562\n",
            "[12,    25] loss at each step: 1.933611512184143\n",
            "[12,    26] loss at each step: 1.9335508346557617\n",
            "[12,    27] loss at each step: 1.9335356950759888\n",
            "[12,    28] loss at each step: 1.9336788654327393\n",
            "[12,    29] loss at each step: 1.933512568473816\n",
            "[12,    30] loss at each step: 1.9332308769226074\n",
            "[12,    31] loss at each step: 1.9331964254379272\n",
            "[12,    32] loss at each step: 1.9332109689712524\n",
            "[12,    33] loss at each step: 1.9331800937652588\n",
            "[12,    34] loss at each step: 1.9332557916641235\n",
            "[12,    35] loss at each step: 1.933074712753296\n",
            "[12,    36] loss at each step: 1.9329643249511719\n",
            "[13,     1] loss at each step: 1.9330520629882812\n",
            "[13,     2] loss at each step: 1.933234691619873\n",
            "[13,     3] loss at each step: 1.9329723119735718\n",
            "[13,     4] loss at each step: 1.9330374002456665\n",
            "[13,     5] loss at each step: 1.9327722787857056\n",
            "[13,     6] loss at each step: 1.9326977729797363\n",
            "[13,     7] loss at each step: 1.9325852394104004\n",
            "[13,     8] loss at each step: 1.9327460527420044\n",
            "[13,     9] loss at each step: 1.9325671195983887\n",
            "[13,    10] loss at each step: 1.932481288909912\n",
            "[13,    11] loss at each step: 1.932706356048584\n",
            "[13,    12] loss at each step: 1.9327278137207031\n",
            "[13,    13] loss at each step: 1.9326282739639282\n",
            "[13,    14] loss at each step: 1.9328060150146484\n",
            "[13,    15] loss at each step: 1.9326822757720947\n",
            "[13,    16] loss at each step: 1.9327659606933594\n",
            "[13,    17] loss at each step: 1.932840347290039\n",
            "[13,    18] loss at each step: 1.9328584671020508\n",
            "[13,    19] loss at each step: 1.933042287826538\n",
            "[13,    20] loss at each step: 1.932975172996521\n",
            "[13,    21] loss at each step: 1.9330369234085083\n",
            "[13,    22] loss at each step: 1.9327378273010254\n",
            "[13,    23] loss at each step: 1.9323885440826416\n",
            "[13,    24] loss at each step: 1.9322808980941772\n",
            "[13,    25] loss at each step: 1.932080864906311\n",
            "[13,    26] loss at each step: 1.9325149059295654\n",
            "[13,    27] loss at each step: 1.9324970245361328\n",
            "[13,    28] loss at each step: 1.932371973991394\n",
            "[13,    29] loss at each step: 1.9323222637176514\n",
            "[13,    30] loss at each step: 1.9326810836791992\n",
            "[13,    31] loss at each step: 1.9325355291366577\n",
            "[13,    32] loss at each step: 1.9326092004776\n",
            "[13,    33] loss at each step: 1.9324945211410522\n",
            "[13,    34] loss at each step: 1.9322043657302856\n",
            "[13,    35] loss at each step: 1.9321035146713257\n",
            "[13,    36] loss at each step: 1.93184232711792\n",
            "[14,     1] loss at each step: 1.931875467300415\n",
            "[14,     2] loss at each step: 1.9317294359207153\n",
            "[14,     3] loss at each step: 1.931882381439209\n",
            "[14,     4] loss at each step: 1.9317678213119507\n",
            "[14,     5] loss at each step: 1.9319111108779907\n",
            "[14,     6] loss at each step: 1.9321174621582031\n",
            "[14,     7] loss at each step: 1.9322130680084229\n",
            "[14,     8] loss at each step: 1.9320497512817383\n",
            "[14,     9] loss at each step: 1.9322327375411987\n",
            "[14,    10] loss at each step: 1.9320710897445679\n",
            "[14,    11] loss at each step: 1.9319735765457153\n",
            "[14,    12] loss at each step: 1.9321188926696777\n",
            "[14,    13] loss at each step: 1.9324734210968018\n",
            "[14,    14] loss at each step: 1.9323385953903198\n",
            "[14,    15] loss at each step: 1.9325202703475952\n",
            "[14,    16] loss at each step: 1.932349681854248\n",
            "[14,    17] loss at each step: 1.932279109954834\n",
            "[14,    18] loss at each step: 1.9322439432144165\n",
            "[14,    19] loss at each step: 1.9322844743728638\n",
            "[14,    20] loss at each step: 1.932295560836792\n",
            "[14,    21] loss at each step: 1.932237982749939\n",
            "[14,    22] loss at each step: 1.9321645498275757\n",
            "[14,    23] loss at each step: 1.9321620464324951\n",
            "[14,    24] loss at each step: 1.9320380687713623\n",
            "[14,    25] loss at each step: 1.9319928884506226\n",
            "[14,    26] loss at each step: 1.9317247867584229\n",
            "[14,    27] loss at each step: 1.9314030408859253\n",
            "[14,    28] loss at each step: 1.9315412044525146\n",
            "[14,    29] loss at each step: 1.931767225265503\n",
            "[14,    30] loss at each step: 1.9317610263824463\n",
            "[14,    31] loss at each step: 1.9317009449005127\n",
            "[14,    32] loss at each step: 1.9315502643585205\n",
            "[14,    33] loss at each step: 1.9313338994979858\n",
            "[14,    34] loss at each step: 1.9314274787902832\n",
            "[14,    35] loss at each step: 1.9312275648117065\n",
            "[14,    36] loss at each step: 1.930815577507019\n",
            "[15,     1] loss at each step: 1.9307589530944824\n",
            "[15,     2] loss at each step: 1.9306912422180176\n",
            "[15,     3] loss at each step: 1.930524230003357\n",
            "[15,     4] loss at each step: 1.9304949045181274\n",
            "[15,     5] loss at each step: 1.9305580854415894\n",
            "[15,     6] loss at each step: 1.9304953813552856\n",
            "[15,     7] loss at each step: 1.9305468797683716\n",
            "[15,     8] loss at each step: 1.9302356243133545\n",
            "[15,     9] loss at each step: 1.9302551746368408\n",
            "[15,    10] loss at each step: 1.9301111698150635\n",
            "[15,    11] loss at each step: 1.9302886724472046\n",
            "[15,    12] loss at each step: 1.9302237033843994\n",
            "[15,    13] loss at each step: 1.930242657661438\n",
            "[15,    14] loss at each step: 1.9304006099700928\n",
            "[15,    15] loss at each step: 1.930354118347168\n",
            "[15,    16] loss at each step: 1.9304100275039673\n",
            "[15,    17] loss at each step: 1.930190086364746\n",
            "[15,    18] loss at each step: 1.9304020404815674\n",
            "[15,    19] loss at each step: 1.930263638496399\n",
            "[15,    20] loss at each step: 1.9304674863815308\n",
            "[15,    21] loss at each step: 1.9305771589279175\n",
            "[15,    22] loss at each step: 1.9305850267410278\n",
            "[15,    23] loss at each step: 1.9306130409240723\n",
            "[15,    24] loss at each step: 1.9305217266082764\n",
            "[15,    25] loss at each step: 1.9302724599838257\n",
            "[15,    26] loss at each step: 1.9302159547805786\n",
            "[15,    27] loss at each step: 1.930539608001709\n",
            "[15,    28] loss at each step: 1.930593490600586\n",
            "[15,    29] loss at each step: 1.9306946992874146\n",
            "[15,    30] loss at each step: 1.9305330514907837\n",
            "[15,    31] loss at each step: 1.9305189847946167\n",
            "[15,    32] loss at each step: 1.9302879571914673\n",
            "[15,    33] loss at each step: 1.9301503896713257\n",
            "[15,    34] loss at each step: 1.9302858114242554\n",
            "[15,    35] loss at each step: 1.9303141832351685\n",
            "[15,    36] loss at each step: 1.9300568103790283\n",
            "[16,     1] loss at each step: 1.9302053451538086\n",
            "[16,     2] loss at each step: 1.930020809173584\n",
            "[16,     3] loss at each step: 1.9299957752227783\n",
            "[16,     4] loss at each step: 1.9300355911254883\n",
            "[16,     5] loss at each step: 1.9299527406692505\n",
            "[16,     6] loss at each step: 1.9300960302352905\n",
            "[16,     7] loss at each step: 1.9299410581588745\n",
            "[16,     8] loss at each step: 1.9295016527175903\n",
            "[16,     9] loss at each step: 1.929233193397522\n",
            "[16,    10] loss at each step: 1.9292380809783936\n",
            "[16,    11] loss at each step: 1.9291354417800903\n",
            "[16,    12] loss at each step: 1.9293991327285767\n",
            "[16,    13] loss at each step: 1.9295742511749268\n",
            "[16,    14] loss at each step: 1.9294935464859009\n",
            "[16,    15] loss at each step: 1.9294432401657104\n",
            "[16,    16] loss at each step: 1.929744839668274\n",
            "[16,    17] loss at each step: 1.929874300956726\n",
            "[16,    18] loss at each step: 1.9299309253692627\n",
            "[16,    19] loss at each step: 1.9298955202102661\n",
            "[16,    20] loss at each step: 1.9300323724746704\n",
            "[16,    21] loss at each step: 1.9299196004867554\n",
            "[16,    22] loss at each step: 1.9297696352005005\n",
            "[16,    23] loss at each step: 1.9297939538955688\n",
            "[16,    24] loss at each step: 1.9295943975448608\n",
            "[16,    25] loss at each step: 1.9294250011444092\n",
            "[16,    26] loss at each step: 1.929439902305603\n",
            "[16,    27] loss at each step: 1.9294573068618774\n",
            "[16,    28] loss at each step: 1.9296844005584717\n",
            "[16,    29] loss at each step: 1.929938554763794\n",
            "[16,    30] loss at each step: 1.9298533201217651\n",
            "[16,    31] loss at each step: 1.9298651218414307\n",
            "[16,    32] loss at each step: 1.9295908212661743\n",
            "[16,    33] loss at each step: 1.9295610189437866\n",
            "[16,    34] loss at each step: 1.9296345710754395\n",
            "[16,    35] loss at each step: 1.9295295476913452\n",
            "[16,    36] loss at each step: 1.929533839225769\n",
            "[17,     1] loss at each step: 1.9296470880508423\n",
            "[17,     2] loss at each step: 1.9298700094223022\n",
            "[17,     3] loss at each step: 1.929964303970337\n",
            "[17,     4] loss at each step: 1.9298150539398193\n",
            "[17,     5] loss at each step: 1.9294816255569458\n",
            "[17,     6] loss at each step: 1.9294203519821167\n",
            "[17,     7] loss at each step: 1.9294848442077637\n",
            "[17,     8] loss at each step: 1.9295566082000732\n",
            "[17,     9] loss at each step: 1.9294759035110474\n",
            "[17,    10] loss at each step: 1.9293051958084106\n",
            "[17,    11] loss at each step: 1.929261326789856\n",
            "[17,    12] loss at each step: 1.9291527271270752\n",
            "[17,    13] loss at each step: 1.929355502128601\n",
            "[17,    14] loss at each step: 1.9293755292892456\n",
            "[17,    15] loss at each step: 1.929253339767456\n",
            "[17,    16] loss at each step: 1.9293392896652222\n",
            "[17,    17] loss at each step: 1.9292410612106323\n",
            "[17,    18] loss at each step: 1.9290337562561035\n",
            "[17,    19] loss at each step: 1.9291692972183228\n",
            "[17,    20] loss at each step: 1.9293729066848755\n",
            "[17,    21] loss at each step: 1.9291208982467651\n",
            "[17,    22] loss at each step: 1.9291491508483887\n",
            "[17,    23] loss at each step: 1.9291237592697144\n",
            "[17,    24] loss at each step: 1.9292423725128174\n",
            "[17,    25] loss at each step: 1.9290716648101807\n",
            "[17,    26] loss at each step: 1.92900550365448\n",
            "[17,    27] loss at each step: 1.9290313720703125\n",
            "[17,    28] loss at each step: 1.9291881322860718\n",
            "[17,    29] loss at each step: 1.9292385578155518\n",
            "[17,    30] loss at each step: 1.9292683601379395\n",
            "[17,    31] loss at each step: 1.9289665222167969\n",
            "[17,    32] loss at each step: 1.9288151264190674\n",
            "[17,    33] loss at each step: 1.9289709329605103\n",
            "[17,    34] loss at each step: 1.9290181398391724\n",
            "[17,    35] loss at each step: 1.9290475845336914\n",
            "[17,    36] loss at each step: 1.9289476871490479\n",
            "[18,     1] loss at each step: 1.9289354085922241\n",
            "[18,     2] loss at each step: 1.9289283752441406\n",
            "[18,     3] loss at each step: 1.9289556741714478\n",
            "[18,     4] loss at each step: 1.9288811683654785\n",
            "[18,     5] loss at each step: 1.9289041757583618\n",
            "[18,     6] loss at each step: 1.9290337562561035\n",
            "[18,     7] loss at each step: 1.9289908409118652\n",
            "[18,     8] loss at each step: 1.9291566610336304\n",
            "[18,     9] loss at each step: 1.929255485534668\n",
            "[18,    10] loss at each step: 1.9290577173233032\n",
            "[18,    11] loss at each step: 1.9289578199386597\n",
            "[18,    12] loss at each step: 1.928888201713562\n",
            "[18,    13] loss at each step: 1.9289652109146118\n",
            "[18,    14] loss at each step: 1.928757667541504\n",
            "[18,    15] loss at each step: 1.9285842180252075\n",
            "[18,    16] loss at each step: 1.9285882711410522\n",
            "[18,    17] loss at each step: 1.9286422729492188\n",
            "[18,    18] loss at each step: 1.928733229637146\n",
            "[18,    19] loss at each step: 1.9288331270217896\n",
            "[18,    20] loss at each step: 1.9288419485092163\n",
            "[18,    21] loss at each step: 1.928799033164978\n",
            "[18,    22] loss at each step: 1.928662896156311\n",
            "[18,    23] loss at each step: 1.9284037351608276\n",
            "[18,    24] loss at each step: 1.928511619567871\n",
            "[18,    25] loss at each step: 1.9285540580749512\n",
            "[18,    26] loss at each step: 1.928170919418335\n",
            "[18,    27] loss at each step: 1.9282904863357544\n",
            "[18,    28] loss at each step: 1.9282753467559814\n",
            "[18,    29] loss at each step: 1.9284613132476807\n",
            "[18,    30] loss at each step: 1.9285666942596436\n",
            "[18,    31] loss at each step: 1.9287108182907104\n",
            "[18,    32] loss at each step: 1.9285244941711426\n",
            "[18,    33] loss at each step: 1.9285820722579956\n",
            "[18,    34] loss at each step: 1.9284998178482056\n",
            "[18,    35] loss at each step: 1.9285686016082764\n",
            "[18,    36] loss at each step: 1.928478717803955\n",
            "[19,     1] loss at each step: 1.9284169673919678\n",
            "[19,     2] loss at each step: 1.9284371137619019\n",
            "[19,     3] loss at each step: 1.9284979104995728\n",
            "[19,     4] loss at each step: 1.928331971168518\n",
            "[19,     5] loss at each step: 1.92836332321167\n",
            "[19,     6] loss at each step: 1.928328514099121\n",
            "[19,     7] loss at each step: 1.9283440113067627\n",
            "[19,     8] loss at each step: 1.9282755851745605\n",
            "[19,     9] loss at each step: 1.9281219244003296\n",
            "[19,    10] loss at each step: 1.9283173084259033\n",
            "[19,    11] loss at each step: 1.928216576576233\n",
            "[19,    12] loss at each step: 1.928016185760498\n",
            "[19,    13] loss at each step: 1.9281891584396362\n",
            "[19,    14] loss at each step: 1.9282934665679932\n",
            "[19,    15] loss at each step: 1.9284257888793945\n",
            "[19,    16] loss at each step: 1.928317666053772\n",
            "[19,    17] loss at each step: 1.9281057119369507\n",
            "[19,    18] loss at each step: 1.9282270669937134\n",
            "[19,    19] loss at each step: 1.9280915260314941\n",
            "[19,    20] loss at each step: 1.9282221794128418\n",
            "[19,    21] loss at each step: 1.9280483722686768\n",
            "[19,    22] loss at each step: 1.9280502796173096\n",
            "[19,    23] loss at each step: 1.928287386894226\n",
            "[19,    24] loss at each step: 1.9282722473144531\n",
            "[19,    25] loss at each step: 1.9283851385116577\n",
            "[19,    26] loss at each step: 1.9284744262695312\n",
            "[19,    27] loss at each step: 1.9284335374832153\n",
            "[19,    28] loss at each step: 1.9281997680664062\n",
            "[19,    29] loss at each step: 1.9280439615249634\n",
            "[19,    30] loss at each step: 1.9279171228408813\n",
            "[19,    31] loss at each step: 1.927858591079712\n",
            "[19,    32] loss at each step: 1.927964210510254\n",
            "[19,    33] loss at each step: 1.9280132055282593\n",
            "[19,    34] loss at each step: 1.927880048751831\n",
            "[19,    35] loss at each step: 1.9279890060424805\n",
            "[19,    36] loss at each step: 1.9279705286026\n",
            "[20,     1] loss at each step: 1.9281147718429565\n",
            "[20,     2] loss at each step: 1.9281294345855713\n",
            "[20,     3] loss at each step: 1.9279812574386597\n",
            "[20,     4] loss at each step: 1.9280452728271484\n",
            "[20,     5] loss at each step: 1.9282010793685913\n",
            "[20,     6] loss at each step: 1.9282690286636353\n",
            "[20,     7] loss at each step: 1.9281482696533203\n",
            "[20,     8] loss at each step: 1.9280517101287842\n",
            "[20,     9] loss at each step: 1.9282892942428589\n",
            "[20,    10] loss at each step: 1.9283027648925781\n",
            "[20,    11] loss at each step: 1.9282649755477905\n",
            "[20,    12] loss at each step: 1.9282997846603394\n",
            "[20,    13] loss at each step: 1.928121566772461\n",
            "[20,    14] loss at each step: 1.9280513525009155\n",
            "[20,    15] loss at each step: 1.9281091690063477\n",
            "[20,    16] loss at each step: 1.9282747507095337\n",
            "[20,    17] loss at each step: 1.9282625913619995\n",
            "[20,    18] loss at each step: 1.9283690452575684\n",
            "[20,    19] loss at each step: 1.9283901453018188\n",
            "[20,    20] loss at each step: 1.9282535314559937\n",
            "[20,    21] loss at each step: 1.9283926486968994\n",
            "[20,    22] loss at each step: 1.9282338619232178\n",
            "[20,    23] loss at each step: 1.9279611110687256\n",
            "[20,    24] loss at each step: 1.928052544593811\n",
            "[20,    25] loss at each step: 1.9278899431228638\n",
            "[20,    26] loss at each step: 1.9279882907867432\n",
            "[20,    27] loss at each step: 1.9279594421386719\n",
            "[20,    28] loss at each step: 1.9278517961502075\n",
            "[20,    29] loss at each step: 1.9279874563217163\n",
            "[20,    30] loss at each step: 1.9278682470321655\n",
            "[20,    31] loss at each step: 1.9278039932250977\n",
            "[20,    32] loss at each step: 1.9277186393737793\n",
            "[20,    33] loss at each step: 1.9277881383895874\n",
            "[20,    34] loss at each step: 1.9276753664016724\n",
            "[20,    35] loss at each step: 1.9275355339050293\n",
            "[20,    36] loss at each step: 1.9275916814804077\n",
            "Accuracy is: 20 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network"
      ],
      "metadata": {
        "id": "YwjafUaln4z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.4, 0.5, 0.5]\n",
        "std = [0.4, 0.5, 0.5]\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))])\n",
        "dataset = SENTIMENT_DATASET(csv_file = df, root_dir = '/content/drive/MyDrive/Data_set_DataMining/Images/', transform = transform)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [284, 72])\n",
        "train_loader = DataLoader(dataset = train_set, batch_size = 8, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_set, batch_size = 8, shuffle = True)"
      ],
      "metadata": {
        "id": "1mEoR5Gkye0D"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolutional_NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride =1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride = 1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(2048, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 9)\n",
        "            )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ],
      "metadata": {
        "id": "gOJ3uclD4Ozt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "model = Convolutional_NN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "n_epochs = 20\n",
        "loss_list = []\n",
        "tot_loss = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    z = net(inputs)\n",
        "    loss=criterion(z,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_list.append(loss.data)\n",
        "    tot_loss+=loss.data\n",
        "    print(f'[{epoch+1}, {i+1:5d}] loss: {tot_loss/len(loss_list)}')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on test images: {100 * correct // total} %')\n",
        "unsegmented_acc.append(round(100 * correct / total))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_segmented.append(TIME)"
      ],
      "metadata": {
        "id": "W5ATI4Wku7hK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dd4de26-4e0e-40a7-b489-4635e6c80576"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 1.9643968343734741\n",
            "[1,     2] loss: 1.923698902130127\n",
            "[1,     3] loss: 1.9312413930892944\n",
            "[1,     4] loss: 1.9768762588500977\n",
            "[1,     5] loss: 1.994981050491333\n",
            "[1,     6] loss: 1.9642324447631836\n",
            "[1,     7] loss: 1.9484100341796875\n",
            "[1,     8] loss: 1.9535751342773438\n",
            "[1,     9] loss: 1.9441263675689697\n",
            "[1,    10] loss: 1.9430248737335205\n",
            "[1,    11] loss: 1.949042797088623\n",
            "[1,    12] loss: 1.9493496417999268\n",
            "[1,    13] loss: 1.9429333209991455\n",
            "[1,    14] loss: 1.931662917137146\n",
            "[1,    15] loss: 1.9260910749435425\n",
            "[1,    16] loss: 1.9320602416992188\n",
            "[1,    17] loss: 1.9353121519088745\n",
            "[1,    18] loss: 1.9332311153411865\n",
            "[1,    19] loss: 1.922839879989624\n",
            "[1,    20] loss: 1.927183985710144\n",
            "[1,    21] loss: 1.9251437187194824\n",
            "[1,    22] loss: 1.9218844175338745\n",
            "[1,    23] loss: 1.9229921102523804\n",
            "[1,    24] loss: 1.925154447555542\n",
            "[1,    25] loss: 1.9285305738449097\n",
            "[1,    26] loss: 1.9273587465286255\n",
            "[1,    27] loss: 1.9302406311035156\n",
            "[1,    28] loss: 1.925346851348877\n",
            "[1,    29] loss: 1.9313123226165771\n",
            "[1,    30] loss: 1.9312256574630737\n",
            "[1,    31] loss: 1.9294840097427368\n",
            "[1,    32] loss: 1.924377679824829\n",
            "[1,    33] loss: 1.924713134765625\n",
            "[1,    34] loss: 1.9271211624145508\n",
            "[1,    35] loss: 1.927150011062622\n",
            "[1,    36] loss: 1.929152011871338\n",
            "[2,     1] loss: 1.9307360649108887\n",
            "[2,     2] loss: 1.9281548261642456\n",
            "[2,     3] loss: 1.930957317352295\n",
            "[2,     4] loss: 1.9354923963546753\n",
            "[2,     5] loss: 1.9344722032546997\n",
            "[2,     6] loss: 1.935916543006897\n",
            "[2,     7] loss: 1.9355233907699585\n",
            "[2,     8] loss: 1.9326190948486328\n",
            "[2,     9] loss: 1.930790901184082\n",
            "[2,    10] loss: 1.9324005842208862\n",
            "[2,    11] loss: 1.934616208076477\n",
            "[2,    12] loss: 1.9323725700378418\n",
            "[2,    13] loss: 1.9311619997024536\n",
            "[2,    14] loss: 1.932729959487915\n",
            "[2,    15] loss: 1.9327136278152466\n",
            "[2,    16] loss: 1.9338977336883545\n",
            "[2,    17] loss: 1.9309710264205933\n",
            "[2,    18] loss: 1.929396629333496\n",
            "[2,    19] loss: 1.9272913932800293\n",
            "[2,    20] loss: 1.9271399974822998\n",
            "[2,    21] loss: 1.9294157028198242\n",
            "[2,    22] loss: 1.9291887283325195\n",
            "[2,    23] loss: 1.930694818496704\n",
            "[2,    24] loss: 1.9311540126800537\n",
            "[2,    25] loss: 1.9312344789505005\n",
            "[2,    26] loss: 1.932608962059021\n",
            "[2,    27] loss: 1.9345475435256958\n",
            "[2,    28] loss: 1.9340949058532715\n",
            "[2,    29] loss: 1.9329371452331543\n",
            "[2,    30] loss: 1.9348629713058472\n",
            "[2,    31] loss: 1.932417869567871\n",
            "[2,    32] loss: 1.931256890296936\n",
            "[2,    33] loss: 1.9323763847351074\n",
            "[2,    34] loss: 1.9308645725250244\n",
            "[2,    35] loss: 1.929707407951355\n",
            "[2,    36] loss: 1.9276305437088013\n",
            "[3,     1] loss: 1.9292272329330444\n",
            "[3,     2] loss: 1.9279357194900513\n",
            "[3,     3] loss: 1.9281423091888428\n",
            "[3,     4] loss: 1.9296878576278687\n",
            "[3,     5] loss: 1.9287623167037964\n",
            "[3,     6] loss: 1.928210973739624\n",
            "[3,     7] loss: 1.927540898323059\n",
            "[3,     8] loss: 1.9267927408218384\n",
            "[3,     9] loss: 1.926750898361206\n",
            "[3,    10] loss: 1.9268620014190674\n",
            "[3,    11] loss: 1.923667550086975\n",
            "[3,    12] loss: 1.9241998195648193\n",
            "[3,    13] loss: 1.9244743585586548\n",
            "[3,    14] loss: 1.9258688688278198\n",
            "[3,    15] loss: 1.9257968664169312\n",
            "[3,    16] loss: 1.9262758493423462\n",
            "[3,    17] loss: 1.9264259338378906\n",
            "[3,    18] loss: 1.9253114461898804\n",
            "[3,    19] loss: 1.9254710674285889\n",
            "[3,    20] loss: 1.9260663986206055\n",
            "[3,    21] loss: 1.9255772829055786\n",
            "[3,    22] loss: 1.926890254020691\n",
            "[3,    23] loss: 1.9275206327438354\n",
            "[3,    24] loss: 1.9281814098358154\n",
            "[3,    25] loss: 1.9283647537231445\n",
            "[3,    26] loss: 1.927651047706604\n",
            "[3,    27] loss: 1.928118109703064\n",
            "[3,    28] loss: 1.927083969116211\n",
            "[3,    29] loss: 1.926572561264038\n",
            "[3,    30] loss: 1.927011489868164\n",
            "[3,    31] loss: 1.9277263879776\n",
            "[3,    32] loss: 1.9274275302886963\n",
            "[3,    33] loss: 1.9284929037094116\n",
            "[3,    34] loss: 1.9286965131759644\n",
            "[3,    35] loss: 1.9278007745742798\n",
            "[3,    36] loss: 1.927813172340393\n",
            "[4,     1] loss: 1.927954077720642\n",
            "[4,     2] loss: 1.9293891191482544\n",
            "[4,     3] loss: 1.9288926124572754\n",
            "[4,     4] loss: 1.9285781383514404\n",
            "[4,     5] loss: 1.928672194480896\n",
            "[4,     6] loss: 1.928876280784607\n",
            "[4,     7] loss: 1.928757667541504\n",
            "[4,     8] loss: 1.929787039756775\n",
            "[4,     9] loss: 1.9298748970031738\n",
            "[4,    10] loss: 1.9296051263809204\n",
            "[4,    11] loss: 1.9307982921600342\n",
            "[4,    12] loss: 1.9311232566833496\n",
            "[4,    13] loss: 1.9306548833847046\n",
            "[4,    14] loss: 1.9299145936965942\n",
            "[4,    15] loss: 1.929293155670166\n",
            "[4,    16] loss: 1.9302055835723877\n",
            "[4,    17] loss: 1.9299215078353882\n",
            "[4,    18] loss: 1.931401014328003\n",
            "[4,    19] loss: 1.9314064979553223\n",
            "[4,    20] loss: 1.9305901527404785\n",
            "[4,    21] loss: 1.9311271905899048\n",
            "[4,    22] loss: 1.9314544200897217\n",
            "[4,    23] loss: 1.9317876100540161\n",
            "[4,    24] loss: 1.9319655895233154\n",
            "[4,    25] loss: 1.932112693786621\n",
            "[4,    26] loss: 1.9311721324920654\n",
            "[4,    27] loss: 1.9302986860275269\n",
            "[4,    28] loss: 1.9302507638931274\n",
            "[4,    29] loss: 1.9309712648391724\n",
            "[4,    30] loss: 1.9300782680511475\n",
            "[4,    31] loss: 1.9296752214431763\n",
            "[4,    32] loss: 1.9291844367980957\n",
            "[4,    33] loss: 1.928320050239563\n",
            "[4,    34] loss: 1.9277454614639282\n",
            "[4,    35] loss: 1.9281511306762695\n",
            "[4,    36] loss: 1.9276515245437622\n",
            "[5,     1] loss: 1.9276647567749023\n",
            "[5,     2] loss: 1.9278823137283325\n",
            "[5,     3] loss: 1.927087664604187\n",
            "[5,     4] loss: 1.9271725416183472\n",
            "[5,     5] loss: 1.9265978336334229\n",
            "[5,     6] loss: 1.9274097681045532\n",
            "[5,     7] loss: 1.927099585533142\n",
            "[5,     8] loss: 1.9259647130966187\n",
            "[5,     9] loss: 1.9268310070037842\n",
            "[5,    10] loss: 1.9273269176483154\n",
            "[5,    11] loss: 1.9272962808609009\n",
            "[5,    12] loss: 1.9275341033935547\n",
            "[5,    13] loss: 1.9282593727111816\n",
            "[5,    14] loss: 1.9283452033996582\n",
            "[5,    15] loss: 1.927940011024475\n",
            "[5,    16] loss: 1.9280840158462524\n",
            "[5,    17] loss: 1.9281002283096313\n",
            "[5,    18] loss: 1.9283658266067505\n",
            "[5,    19] loss: 1.9285465478897095\n",
            "[5,    20] loss: 1.928207278251648\n",
            "[5,    21] loss: 1.928757905960083\n",
            "[5,    22] loss: 1.9279438257217407\n",
            "[5,    23] loss: 1.9279842376708984\n",
            "[5,    24] loss: 1.9288305044174194\n",
            "[5,    25] loss: 1.9287821054458618\n",
            "[5,    26] loss: 1.9286373853683472\n",
            "[5,    27] loss: 1.928533911705017\n",
            "[5,    28] loss: 1.9278154373168945\n",
            "[5,    29] loss: 1.9278366565704346\n",
            "[5,    30] loss: 1.9279431104660034\n",
            "[5,    31] loss: 1.9274518489837646\n",
            "[5,    32] loss: 1.9267873764038086\n",
            "[5,    33] loss: 1.9270211458206177\n",
            "[5,    34] loss: 1.9275615215301514\n",
            "[5,    35] loss: 1.927032470703125\n",
            "[5,    36] loss: 1.9284703731536865\n",
            "[6,     1] loss: 1.9284217357635498\n",
            "[6,     2] loss: 1.9277865886688232\n",
            "[6,     3] loss: 1.9282808303833008\n",
            "[6,     4] loss: 1.928720235824585\n",
            "[6,     5] loss: 1.9284473657608032\n",
            "[6,     6] loss: 1.927790641784668\n",
            "[6,     7] loss: 1.9278675317764282\n",
            "[6,     8] loss: 1.9275811910629272\n",
            "[6,     9] loss: 1.9277299642562866\n",
            "[6,    10] loss: 1.9271681308746338\n",
            "[6,    11] loss: 1.9267395734786987\n",
            "[6,    12] loss: 1.9267882108688354\n",
            "[6,    13] loss: 1.926818609237671\n",
            "[6,    14] loss: 1.92694890499115\n",
            "[6,    15] loss: 1.926593542098999\n",
            "[6,    16] loss: 1.927327036857605\n",
            "[6,    17] loss: 1.927727222442627\n",
            "[6,    18] loss: 1.9277405738830566\n",
            "[6,    19] loss: 1.927858829498291\n",
            "[6,    20] loss: 1.9277671575546265\n",
            "[6,    21] loss: 1.9281988143920898\n",
            "[6,    22] loss: 1.9281296730041504\n",
            "[6,    23] loss: 1.928476095199585\n",
            "[6,    24] loss: 1.9281402826309204\n",
            "[6,    25] loss: 1.9282352924346924\n",
            "[6,    26] loss: 1.928087830543518\n",
            "[6,    27] loss: 1.9286001920700073\n",
            "[6,    28] loss: 1.9287792444229126\n",
            "[6,    29] loss: 1.9284652471542358\n",
            "[6,    30] loss: 1.929320216178894\n",
            "[6,    31] loss: 1.9281299114227295\n",
            "[6,    32] loss: 1.928171992301941\n",
            "[6,    33] loss: 1.9284993410110474\n",
            "[6,    34] loss: 1.928038477897644\n",
            "[6,    35] loss: 1.9282242059707642\n",
            "[6,    36] loss: 1.9286153316497803\n",
            "[7,     1] loss: 1.9288238286972046\n",
            "[7,     2] loss: 1.9290887117385864\n",
            "[7,     3] loss: 1.9286212921142578\n",
            "[7,     4] loss: 1.9286723136901855\n",
            "[7,     5] loss: 1.9292428493499756\n",
            "[7,     6] loss: 1.9293876886367798\n",
            "[7,     7] loss: 1.929516315460205\n",
            "[7,     8] loss: 1.930113434791565\n",
            "[7,     9] loss: 1.9298783540725708\n",
            "[7,    10] loss: 1.929741621017456\n",
            "[7,    11] loss: 1.9297398328781128\n",
            "[7,    12] loss: 1.9298170804977417\n",
            "[7,    13] loss: 1.9295918941497803\n",
            "[7,    14] loss: 1.9296051263809204\n",
            "[7,    15] loss: 1.9300611019134521\n",
            "[7,    16] loss: 1.9304200410842896\n",
            "[7,    17] loss: 1.9303592443466187\n",
            "[7,    18] loss: 1.9301427602767944\n",
            "[7,    19] loss: 1.9297698736190796\n",
            "[7,    20] loss: 1.9301332235336304\n",
            "[7,    21] loss: 1.9302995204925537\n",
            "[7,    22] loss: 1.930456280708313\n",
            "[7,    23] loss: 1.9301769733428955\n",
            "[7,    24] loss: 1.930418848991394\n",
            "[7,    25] loss: 1.9304282665252686\n",
            "[7,    26] loss: 1.9308074712753296\n",
            "[7,    27] loss: 1.9305250644683838\n",
            "[7,    28] loss: 1.9300858974456787\n",
            "[7,    29] loss: 1.9302057027816772\n",
            "[7,    30] loss: 1.9295064210891724\n",
            "[7,    31] loss: 1.929399013519287\n",
            "[7,    32] loss: 1.9288579225540161\n",
            "[7,    33] loss: 1.9284555912017822\n",
            "[7,    34] loss: 1.9289236068725586\n",
            "[7,    35] loss: 1.9281809329986572\n",
            "[7,    36] loss: 1.9289219379425049\n",
            "[8,     1] loss: 1.92878258228302\n",
            "[8,     2] loss: 1.9290653467178345\n",
            "[8,     3] loss: 1.929131031036377\n",
            "[8,     4] loss: 1.9293452501296997\n",
            "[8,     5] loss: 1.929781436920166\n",
            "[8,     6] loss: 1.9299142360687256\n",
            "[8,     7] loss: 1.9295891523361206\n",
            "[8,     8] loss: 1.9291142225265503\n",
            "[8,     9] loss: 1.9293519258499146\n",
            "[8,    10] loss: 1.9295765161514282\n",
            "[8,    11] loss: 1.9297194480895996\n",
            "[8,    12] loss: 1.9297164678573608\n",
            "[8,    13] loss: 1.9295547008514404\n",
            "[8,    14] loss: 1.9296836853027344\n",
            "[8,    15] loss: 1.9295841455459595\n",
            "[8,    16] loss: 1.9302558898925781\n",
            "[8,    17] loss: 1.9306957721710205\n",
            "[8,    18] loss: 1.930044412612915\n",
            "[8,    19] loss: 1.9301222562789917\n",
            "[8,    20] loss: 1.9301396608352661\n",
            "[8,    21] loss: 1.9301111698150635\n",
            "[8,    22] loss: 1.929274082183838\n",
            "[8,    23] loss: 1.9292864799499512\n",
            "[8,    24] loss: 1.9289685487747192\n",
            "[8,    25] loss: 1.9289934635162354\n",
            "[8,    26] loss: 1.9293233156204224\n",
            "[8,    27] loss: 1.9293558597564697\n",
            "[8,    28] loss: 1.9295850992202759\n",
            "[8,    29] loss: 1.929801106452942\n",
            "[8,    30] loss: 1.9297298192977905\n",
            "[8,    31] loss: 1.9295164346694946\n",
            "[8,    32] loss: 1.9292200803756714\n",
            "[8,    33] loss: 1.9287242889404297\n",
            "[8,    34] loss: 1.9286383390426636\n",
            "[8,    35] loss: 1.9284594058990479\n",
            "[8,    36] loss: 1.9291962385177612\n",
            "[9,     1] loss: 1.9289747476577759\n",
            "[9,     2] loss: 1.929060459136963\n",
            "[9,     3] loss: 1.9293326139450073\n",
            "[9,     4] loss: 1.929094672203064\n",
            "[9,     5] loss: 1.9288620948791504\n",
            "[9,     6] loss: 1.9287574291229248\n",
            "[9,     7] loss: 1.9286291599273682\n",
            "[9,     8] loss: 1.9285353422164917\n",
            "[9,     9] loss: 1.9283465147018433\n",
            "[9,    10] loss: 1.9280461072921753\n",
            "[9,    11] loss: 1.928039789199829\n",
            "[9,    12] loss: 1.9284416437149048\n",
            "[9,    13] loss: 1.928709626197815\n",
            "[9,    14] loss: 1.9284272193908691\n",
            "[9,    15] loss: 1.9283740520477295\n",
            "[9,    16] loss: 1.9285374879837036\n",
            "[9,    17] loss: 1.9284237623214722\n",
            "[9,    18] loss: 1.9283794164657593\n",
            "[9,    19] loss: 1.9285922050476074\n",
            "[9,    20] loss: 1.9284101724624634\n",
            "[9,    21] loss: 1.9283149242401123\n",
            "[9,    22] loss: 1.928654432296753\n",
            "[9,    23] loss: 1.9288636445999146\n",
            "[9,    24] loss: 1.9289584159851074\n",
            "[9,    25] loss: 1.9290424585342407\n",
            "[9,    26] loss: 1.9289346933364868\n",
            "[9,    27] loss: 1.9292911291122437\n",
            "[9,    28] loss: 1.929294466972351\n",
            "[9,    29] loss: 1.9292564392089844\n",
            "[9,    30] loss: 1.929409146308899\n",
            "[9,    31] loss: 1.9292678833007812\n",
            "[9,    32] loss: 1.9296995401382446\n",
            "[9,    33] loss: 1.92971670627594\n",
            "[9,    34] loss: 1.9293550252914429\n",
            "[9,    35] loss: 1.9290916919708252\n",
            "[9,    36] loss: 1.9290748834609985\n",
            "[10,     1] loss: 1.9292991161346436\n",
            "[10,     2] loss: 1.9292912483215332\n",
            "[10,     3] loss: 1.9296059608459473\n",
            "[10,     4] loss: 1.9295568466186523\n",
            "[10,     5] loss: 1.9296205043792725\n",
            "[10,     6] loss: 1.929404616355896\n",
            "[10,     7] loss: 1.929305076599121\n",
            "[10,     8] loss: 1.9296890497207642\n",
            "[10,     9] loss: 1.9295892715454102\n",
            "[10,    10] loss: 1.9295790195465088\n",
            "[10,    11] loss: 1.9297325611114502\n",
            "[10,    12] loss: 1.9292815923690796\n",
            "[10,    13] loss: 1.929813027381897\n",
            "[10,    14] loss: 1.9299284219741821\n",
            "[10,    15] loss: 1.9303027391433716\n",
            "[10,    16] loss: 1.9301949739456177\n",
            "[10,    17] loss: 1.9298920631408691\n",
            "[10,    18] loss: 1.9296767711639404\n",
            "[10,    19] loss: 1.929772973060608\n",
            "[10,    20] loss: 1.9300631284713745\n",
            "[10,    21] loss: 1.9299209117889404\n",
            "[10,    22] loss: 1.929584264755249\n",
            "[10,    23] loss: 1.9292216300964355\n",
            "[10,    24] loss: 1.9290227890014648\n",
            "[10,    25] loss: 1.9288313388824463\n",
            "[10,    26] loss: 1.928916335105896\n",
            "[10,    27] loss: 1.9284852743148804\n",
            "[10,    28] loss: 1.9284427165985107\n",
            "[10,    29] loss: 1.9286000728607178\n",
            "[10,    30] loss: 1.9288299083709717\n",
            "[10,    31] loss: 1.9285774230957031\n",
            "[10,    32] loss: 1.9286564588546753\n",
            "[10,    33] loss: 1.9288049936294556\n",
            "[10,    34] loss: 1.9289624691009521\n",
            "[10,    35] loss: 1.9288394451141357\n",
            "[10,    36] loss: 1.9291309118270874\n",
            "[11,     1] loss: 1.929355263710022\n",
            "[11,     2] loss: 1.9294073581695557\n",
            "[11,     3] loss: 1.9294849634170532\n",
            "[11,     4] loss: 1.9294606447219849\n",
            "[11,     5] loss: 1.9292302131652832\n",
            "[11,     6] loss: 1.929097056388855\n",
            "[11,     7] loss: 1.928917646408081\n",
            "[11,     8] loss: 1.9289638996124268\n",
            "[11,     9] loss: 1.9289677143096924\n",
            "[11,    10] loss: 1.929311752319336\n",
            "[11,    11] loss: 1.929335117340088\n",
            "[11,    12] loss: 1.9292999505996704\n",
            "[11,    13] loss: 1.9294440746307373\n",
            "[11,    14] loss: 1.9293172359466553\n",
            "[11,    15] loss: 1.929303526878357\n",
            "[11,    16] loss: 1.9296101331710815\n",
            "[11,    17] loss: 1.929626226425171\n",
            "[11,    18] loss: 1.9294949769973755\n",
            "[11,    19] loss: 1.9293149709701538\n",
            "[11,    20] loss: 1.9293389320373535\n",
            "[11,    21] loss: 1.929478645324707\n",
            "[11,    22] loss: 1.9298131465911865\n",
            "[11,    23] loss: 1.9296603202819824\n",
            "[11,    24] loss: 1.9293841123580933\n",
            "[11,    25] loss: 1.9292141199111938\n",
            "[11,    26] loss: 1.9291733503341675\n",
            "[11,    27] loss: 1.9288597106933594\n",
            "[11,    28] loss: 1.9289385080337524\n",
            "[11,    29] loss: 1.9287079572677612\n",
            "[11,    30] loss: 1.9288071393966675\n",
            "[11,    31] loss: 1.9289888143539429\n",
            "[11,    32] loss: 1.9291417598724365\n",
            "[11,    33] loss: 1.9286683797836304\n",
            "[11,    34] loss: 1.9287649393081665\n",
            "[11,    35] loss: 1.9289941787719727\n",
            "[11,    36] loss: 1.929094672203064\n",
            "[12,     1] loss: 1.9290623664855957\n",
            "[12,     2] loss: 1.92924964427948\n",
            "[12,     3] loss: 1.9290298223495483\n",
            "[12,     4] loss: 1.9291894435882568\n",
            "[12,     5] loss: 1.929221510887146\n",
            "[12,     6] loss: 1.9294428825378418\n",
            "[12,     7] loss: 1.9295655488967896\n",
            "[12,     8] loss: 1.9294620752334595\n",
            "[12,     9] loss: 1.9296022653579712\n",
            "[12,    10] loss: 1.930006980895996\n",
            "[12,    11] loss: 1.9297999143600464\n",
            "[12,    12] loss: 1.9296706914901733\n",
            "[12,    13] loss: 1.9297202825546265\n",
            "[12,    14] loss: 1.9295008182525635\n",
            "[12,    15] loss: 1.9295095205307007\n",
            "[12,    16] loss: 1.9292384386062622\n",
            "[12,    17] loss: 1.9293365478515625\n",
            "[12,    18] loss: 1.9294283390045166\n",
            "[12,    19] loss: 1.9294835329055786\n",
            "[12,    20] loss: 1.9292290210723877\n",
            "[12,    21] loss: 1.9294487237930298\n",
            "[12,    22] loss: 1.9294488430023193\n",
            "[12,    23] loss: 1.92922043800354\n",
            "[12,    24] loss: 1.9293965101242065\n",
            "[12,    25] loss: 1.9294824600219727\n",
            "[12,    26] loss: 1.9294871091842651\n",
            "[12,    27] loss: 1.9294997453689575\n",
            "[12,    28] loss: 1.9293643236160278\n",
            "[12,    29] loss: 1.9290755987167358\n",
            "[12,    30] loss: 1.9289530515670776\n",
            "[12,    31] loss: 1.9287586212158203\n",
            "[12,    32] loss: 1.9288575649261475\n",
            "[12,    33] loss: 1.9289718866348267\n",
            "[12,    34] loss: 1.9289970397949219\n",
            "[12,    35] loss: 1.9290119409561157\n",
            "[12,    36] loss: 1.9290242195129395\n",
            "[13,     1] loss: 1.9288371801376343\n",
            "[13,     2] loss: 1.928976058959961\n",
            "[13,     3] loss: 1.9289851188659668\n",
            "[13,     4] loss: 1.9290634393692017\n",
            "[13,     5] loss: 1.929101824760437\n",
            "[13,     6] loss: 1.9292218685150146\n",
            "[13,     7] loss: 1.9292813539505005\n",
            "[13,     8] loss: 1.9292012453079224\n",
            "[13,     9] loss: 1.929275393486023\n",
            "[13,    10] loss: 1.9295296669006348\n",
            "[13,    11] loss: 1.9296618700027466\n",
            "[13,    12] loss: 1.9293104410171509\n",
            "[13,    13] loss: 1.929382562637329\n",
            "[13,    14] loss: 1.9291489124298096\n",
            "[13,    15] loss: 1.928888201713562\n",
            "[13,    16] loss: 1.928869366645813\n",
            "[13,    17] loss: 1.9291311502456665\n",
            "[13,    18] loss: 1.9292478561401367\n",
            "[13,    19] loss: 1.9290190935134888\n",
            "[13,    20] loss: 1.9289847612380981\n",
            "[13,    21] loss: 1.9290887117385864\n",
            "[13,    22] loss: 1.928952932357788\n",
            "[13,    23] loss: 1.9289745092391968\n",
            "[13,    24] loss: 1.9290664196014404\n",
            "[13,    25] loss: 1.9289244413375854\n",
            "[13,    26] loss: 1.9290473461151123\n",
            "[13,    27] loss: 1.9289219379425049\n",
            "[13,    28] loss: 1.9285451173782349\n",
            "[13,    29] loss: 1.9283303022384644\n",
            "[13,    30] loss: 1.9282677173614502\n",
            "[13,    31] loss: 1.92850661277771\n",
            "[13,    32] loss: 1.928335428237915\n",
            "[13,    33] loss: 1.9284435510635376\n",
            "[13,    34] loss: 1.9287956953048706\n",
            "[13,    35] loss: 1.9290742874145508\n",
            "[13,    36] loss: 1.9288443326950073\n",
            "[14,     1] loss: 1.9285893440246582\n",
            "[14,     2] loss: 1.9282615184783936\n",
            "[14,     3] loss: 1.9282320737838745\n",
            "[14,     4] loss: 1.9280098676681519\n",
            "[14,     5] loss: 1.9278404712677002\n",
            "[14,     6] loss: 1.9279558658599854\n",
            "[14,     7] loss: 1.9280885457992554\n",
            "[14,     8] loss: 1.9281409978866577\n",
            "[14,     9] loss: 1.9280245304107666\n",
            "[14,    10] loss: 1.9280400276184082\n",
            "[14,    11] loss: 1.9280915260314941\n",
            "[14,    12] loss: 1.9281219244003296\n",
            "[14,    13] loss: 1.927808403968811\n",
            "[14,    14] loss: 1.9276386499404907\n",
            "[14,    15] loss: 1.927822470664978\n",
            "[14,    16] loss: 1.9278123378753662\n",
            "[14,    17] loss: 1.9278614521026611\n",
            "[14,    18] loss: 1.9279886484146118\n",
            "[14,    19] loss: 1.9280121326446533\n",
            "[14,    20] loss: 1.9278100728988647\n",
            "[14,    21] loss: 1.9279335737228394\n",
            "[14,    22] loss: 1.9278424978256226\n",
            "[14,    23] loss: 1.928083896636963\n",
            "[14,    24] loss: 1.9280146360397339\n",
            "[14,    25] loss: 1.928168535232544\n",
            "[14,    26] loss: 1.9280539751052856\n",
            "[14,    27] loss: 1.9279825687408447\n",
            "[14,    28] loss: 1.9280818700790405\n",
            "[14,    29] loss: 1.9281487464904785\n",
            "[14,    30] loss: 1.928032398223877\n",
            "[14,    31] loss: 1.9282029867172241\n",
            "[14,    32] loss: 1.9282552003860474\n",
            "[14,    33] loss: 1.9284734725952148\n",
            "[14,    34] loss: 1.9284437894821167\n",
            "[14,    35] loss: 1.9286556243896484\n",
            "[14,    36] loss: 1.9289374351501465\n",
            "[15,     1] loss: 1.9290263652801514\n",
            "[15,     2] loss: 1.9290168285369873\n",
            "[15,     3] loss: 1.9291943311691284\n",
            "[15,     4] loss: 1.9290533065795898\n",
            "[15,     5] loss: 1.9291980266571045\n",
            "[15,     6] loss: 1.9289474487304688\n",
            "[15,     7] loss: 1.9290988445281982\n",
            "[15,     8] loss: 1.9291937351226807\n",
            "[15,     9] loss: 1.9291220903396606\n",
            "[15,    10] loss: 1.9288904666900635\n",
            "[15,    11] loss: 1.9289770126342773\n",
            "[15,    12] loss: 1.9290144443511963\n",
            "[15,    13] loss: 1.9290118217468262\n",
            "[15,    14] loss: 1.9292539358139038\n",
            "[15,    15] loss: 1.9291892051696777\n",
            "[15,    16] loss: 1.9292649030685425\n",
            "[15,    17] loss: 1.9291342496871948\n",
            "[15,    18] loss: 1.9290097951889038\n",
            "[15,    19] loss: 1.9291929006576538\n",
            "[15,    20] loss: 1.9294908046722412\n",
            "[15,    21] loss: 1.929510474205017\n",
            "[15,    22] loss: 1.9294670820236206\n",
            "[15,    23] loss: 1.9294424057006836\n",
            "[15,    24] loss: 1.929449439048767\n",
            "[15,    25] loss: 1.9292141199111938\n",
            "[15,    26] loss: 1.9290764331817627\n",
            "[15,    27] loss: 1.9292093515396118\n",
            "[15,    28] loss: 1.9291932582855225\n",
            "[15,    29] loss: 1.9293056726455688\n",
            "[15,    30] loss: 1.9295108318328857\n",
            "[15,    31] loss: 1.9292166233062744\n",
            "[15,    32] loss: 1.9290311336517334\n",
            "[15,    33] loss: 1.9289671182632446\n",
            "[15,    34] loss: 1.9291728734970093\n",
            "[15,    35] loss: 1.9289225339889526\n",
            "[15,    36] loss: 1.9288501739501953\n",
            "[16,     1] loss: 1.9287834167480469\n",
            "[16,     2] loss: 1.928704023361206\n",
            "[16,     3] loss: 1.92838454246521\n",
            "[16,     4] loss: 1.9285913705825806\n",
            "[16,     5] loss: 1.9285330772399902\n",
            "[16,     6] loss: 1.9286749362945557\n",
            "[16,     7] loss: 1.9286565780639648\n",
            "[16,     8] loss: 1.9285709857940674\n",
            "[16,     9] loss: 1.9285410642623901\n",
            "[16,    10] loss: 1.9286441802978516\n",
            "[16,    11] loss: 1.9287636280059814\n",
            "[16,    12] loss: 1.9286103248596191\n",
            "[16,    13] loss: 1.9286489486694336\n",
            "[16,    14] loss: 1.928637981414795\n",
            "[16,    15] loss: 1.9285825490951538\n",
            "[16,    16] loss: 1.9286350011825562\n",
            "[16,    17] loss: 1.9282838106155396\n",
            "[16,    18] loss: 1.9282805919647217\n",
            "[16,    19] loss: 1.9284135103225708\n",
            "[16,    20] loss: 1.9280461072921753\n",
            "[16,    21] loss: 1.9279658794403076\n",
            "[16,    22] loss: 1.9277493953704834\n",
            "[16,    23] loss: 1.927819013595581\n",
            "[16,    24] loss: 1.928139090538025\n",
            "[16,    25] loss: 1.928113579750061\n",
            "[16,    26] loss: 1.9280877113342285\n",
            "[16,    27] loss: 1.9280381202697754\n",
            "[16,    28] loss: 1.9283629655838013\n",
            "[16,    29] loss: 1.9283809661865234\n",
            "[16,    30] loss: 1.9284077882766724\n",
            "[16,    31] loss: 1.9284735918045044\n",
            "[16,    32] loss: 1.9284052848815918\n",
            "[16,    33] loss: 1.9286218881607056\n",
            "[16,    34] loss: 1.928737759590149\n",
            "[16,    35] loss: 1.9288073778152466\n",
            "[16,    36] loss: 1.9288086891174316\n",
            "[17,     1] loss: 1.9287893772125244\n",
            "[17,     2] loss: 1.9288294315338135\n",
            "[17,     3] loss: 1.9285359382629395\n",
            "[17,     4] loss: 1.928504467010498\n",
            "[17,     5] loss: 1.928559422492981\n",
            "[17,     6] loss: 1.928619146347046\n",
            "[17,     7] loss: 1.92869234085083\n",
            "[17,     8] loss: 1.9286308288574219\n",
            "[17,     9] loss: 1.9284824132919312\n",
            "[17,    10] loss: 1.928521752357483\n",
            "[17,    11] loss: 1.9287208318710327\n",
            "[17,    12] loss: 1.9288394451141357\n",
            "[17,    13] loss: 1.9288063049316406\n",
            "[17,    14] loss: 1.9289441108703613\n",
            "[17,    15] loss: 1.9290425777435303\n",
            "[17,    16] loss: 1.9291976690292358\n",
            "[17,    17] loss: 1.929120659828186\n",
            "[17,    18] loss: 1.9291915893554688\n",
            "[17,    19] loss: 1.9292796850204468\n",
            "[17,    20] loss: 1.9292882680892944\n",
            "[17,    21] loss: 1.929220199584961\n",
            "[17,    22] loss: 1.9292612075805664\n",
            "[17,    23] loss: 1.9292081594467163\n",
            "[17,    24] loss: 1.9292141199111938\n",
            "[17,    25] loss: 1.9292312860488892\n",
            "[17,    26] loss: 1.9293382167816162\n",
            "[17,    27] loss: 1.9295895099639893\n",
            "[17,    28] loss: 1.9294116497039795\n",
            "[17,    29] loss: 1.929124355316162\n",
            "[17,    30] loss: 1.9289839267730713\n",
            "[17,    31] loss: 1.9290201663970947\n",
            "[17,    32] loss: 1.928702473640442\n",
            "[17,    33] loss: 1.9287333488464355\n",
            "[17,    34] loss: 1.9286248683929443\n",
            "[17,    35] loss: 1.928659439086914\n",
            "[17,    36] loss: 1.9288816452026367\n",
            "[18,     1] loss: 1.9287949800491333\n",
            "[18,     2] loss: 1.9287500381469727\n",
            "[18,     3] loss: 1.928436040878296\n",
            "[18,     4] loss: 1.9282923936843872\n",
            "[18,     5] loss: 1.9284859895706177\n",
            "[18,     6] loss: 1.928467869758606\n",
            "[18,     7] loss: 1.928302526473999\n",
            "[18,     8] loss: 1.9283647537231445\n",
            "[18,     9] loss: 1.9284400939941406\n",
            "[18,    10] loss: 1.9282433986663818\n",
            "[18,    11] loss: 1.928428053855896\n",
            "[18,    12] loss: 1.9283145666122437\n",
            "[18,    13] loss: 1.9282982349395752\n",
            "[18,    14] loss: 1.9284628629684448\n",
            "[18,    15] loss: 1.9283912181854248\n",
            "[18,    16] loss: 1.9285138845443726\n",
            "[18,    17] loss: 1.9285248517990112\n",
            "[18,    18] loss: 1.9282747507095337\n",
            "[18,    19] loss: 1.9281644821166992\n",
            "[18,    20] loss: 1.9283424615859985\n",
            "[18,    21] loss: 1.9283602237701416\n",
            "[18,    22] loss: 1.9282563924789429\n",
            "[18,    23] loss: 1.9284602403640747\n",
            "[18,    24] loss: 1.9284734725952148\n",
            "[18,    25] loss: 1.9284926652908325\n",
            "[18,    26] loss: 1.9288400411605835\n",
            "[18,    27] loss: 1.9289084672927856\n",
            "[18,    28] loss: 1.9288088083267212\n",
            "[18,    29] loss: 1.9287196397781372\n",
            "[18,    30] loss: 1.928779125213623\n",
            "[18,    31] loss: 1.9288694858551025\n",
            "[18,    32] loss: 1.9288458824157715\n",
            "[18,    33] loss: 1.928825855255127\n",
            "[18,    34] loss: 1.9288601875305176\n",
            "[18,    35] loss: 1.9288684129714966\n",
            "[18,    36] loss: 1.9288161993026733\n",
            "[19,     1] loss: 1.9289257526397705\n",
            "[19,     2] loss: 1.9287726879119873\n",
            "[19,     3] loss: 1.929004430770874\n",
            "[19,     4] loss: 1.9290746450424194\n",
            "[19,     5] loss: 1.9291408061981201\n",
            "[19,     6] loss: 1.9291555881500244\n",
            "[19,     7] loss: 1.9290275573730469\n",
            "[19,     8] loss: 1.9290932416915894\n",
            "[19,     9] loss: 1.9289608001708984\n",
            "[19,    10] loss: 1.9290506839752197\n",
            "[19,    11] loss: 1.929158091545105\n",
            "[19,    12] loss: 1.929215669631958\n",
            "[19,    13] loss: 1.9293605089187622\n",
            "[19,    14] loss: 1.9292645454406738\n",
            "[19,    15] loss: 1.929057240486145\n",
            "[19,    16] loss: 1.928850531578064\n",
            "[19,    17] loss: 1.928707242012024\n",
            "[19,    18] loss: 1.928910732269287\n",
            "[19,    19] loss: 1.928868055343628\n",
            "[19,    20] loss: 1.9285502433776855\n",
            "[19,    21] loss: 1.9286961555480957\n",
            "[19,    22] loss: 1.9287787675857544\n",
            "[19,    23] loss: 1.9287649393081665\n",
            "[19,    24] loss: 1.928737998008728\n",
            "[19,    25] loss: 1.9287075996398926\n",
            "[19,    26] loss: 1.9290387630462646\n",
            "[19,    27] loss: 1.9289907217025757\n",
            "[19,    28] loss: 1.92902410030365\n",
            "[19,    29] loss: 1.929016351699829\n",
            "[19,    30] loss: 1.9289268255233765\n",
            "[19,    31] loss: 1.928877592086792\n",
            "[19,    32] loss: 1.9288833141326904\n",
            "[19,    33] loss: 1.928860068321228\n",
            "[19,    34] loss: 1.9286556243896484\n",
            "[19,    35] loss: 1.9287307262420654\n",
            "[19,    36] loss: 1.9288344383239746\n",
            "[20,     1] loss: 1.9287734031677246\n",
            "[20,     2] loss: 1.928930401802063\n",
            "[20,     3] loss: 1.928761601448059\n",
            "[20,     4] loss: 1.9286117553710938\n",
            "[20,     5] loss: 1.928342580795288\n",
            "[20,     6] loss: 1.9284340143203735\n",
            "[20,     7] loss: 1.9283353090286255\n",
            "[20,     8] loss: 1.9282664060592651\n",
            "[20,     9] loss: 1.9283485412597656\n",
            "[20,    10] loss: 1.9284241199493408\n",
            "[20,    11] loss: 1.9284101724624634\n",
            "[20,    12] loss: 1.9282854795455933\n",
            "[20,    13] loss: 1.9283188581466675\n",
            "[20,    14] loss: 1.9282618761062622\n",
            "[20,    15] loss: 1.9282784461975098\n",
            "[20,    16] loss: 1.9283082485198975\n",
            "[20,    17] loss: 1.9285081624984741\n",
            "[20,    18] loss: 1.928625226020813\n",
            "[20,    19] loss: 1.9285026788711548\n",
            "[20,    20] loss: 1.928436517715454\n",
            "[20,    21] loss: 1.9285062551498413\n",
            "[20,    22] loss: 1.9287762641906738\n",
            "[20,    23] loss: 1.9289227724075317\n",
            "[20,    24] loss: 1.9288592338562012\n",
            "[20,    25] loss: 1.928795337677002\n",
            "[20,    26] loss: 1.9287937879562378\n",
            "[20,    27] loss: 1.9289207458496094\n",
            "[20,    28] loss: 1.9287261962890625\n",
            "[20,    29] loss: 1.9287028312683105\n",
            "[20,    30] loss: 1.928806185722351\n",
            "[20,    31] loss: 1.9287811517715454\n",
            "[20,    32] loss: 1.9288105964660645\n",
            "[20,    33] loss: 1.928753137588501\n",
            "[20,    34] loss: 1.928723692893982\n",
            "[20,    35] loss: 1.928650140762329\n",
            "[20,    36] loss: 1.928952932357788\n",
            "Accuracy of the network on test images: 30 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = [0.4, 0.5, 0.5]\n",
        "std = [0.4, 0.5, 0.5]\n",
        "transform = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor(), transforms.Normalize(torch.Tensor(mean), torch.Tensor(std))])\n",
        "dataset = SENTIMENT_DATASET(csv_file = df, root_dir = '/content/drive/MyDrive/Data_set_DataMining/Segmented_Images/', transform = transform)\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [284, 72])\n",
        "train_loader = DataLoader(dataset = train_set, batch_size = 8, shuffle = True)\n",
        "test_loader = DataLoader(dataset = test_set, batch_size = 8, shuffle = True)"
      ],
      "metadata": {
        "id": "SbdJAvOa08av"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "model = Convolutional_NN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "n_epochs = 20\n",
        "loss_list = []\n",
        "tot_loss = 0\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for i, data in enumerate(train_loader, 0):\n",
        "    inputs, labels = data\n",
        "    model = Convolutional_NN()    \n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    z=net(inputs)\n",
        "    loss=criterion(z,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_list.append(loss.data)\n",
        "    tot_loss+=loss.data\n",
        "    print(f'[{epoch+1}, {i+1:5d}] loss: {tot_loss/len(loss_list)}')\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    inputs, labels = data\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = net(inputs)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on test images: {100 * correct // total} %')\n",
        "segmented_acc.append(round(100 * correct / total))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_unsegmented.append(TIME)"
      ],
      "metadata": {
        "id": "2UJExNOJ0_zl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1f3d8b-fd96-4f71-bf59-87eff493dabb"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 2.0338892936706543\n",
            "[1,     2] loss: 1.8939677476882935\n",
            "[1,     3] loss: 1.9483191967010498\n",
            "[1,     4] loss: 1.9301972389221191\n",
            "[1,     5] loss: 1.9535980224609375\n",
            "[1,     6] loss: 1.9440702199935913\n",
            "[1,     7] loss: 1.9305024147033691\n",
            "[1,     8] loss: 1.912876009941101\n",
            "[1,     9] loss: 1.9092241525650024\n",
            "[1,    10] loss: 1.907549500465393\n",
            "[1,    11] loss: 1.9124789237976074\n",
            "[1,    12] loss: 1.9101178646087646\n",
            "[1,    13] loss: 1.9091918468475342\n",
            "[1,    14] loss: 1.914833426475525\n",
            "[1,    15] loss: 1.9100167751312256\n",
            "[1,    16] loss: 1.9167462587356567\n",
            "[1,    17] loss: 1.911625623703003\n",
            "[1,    18] loss: 1.9072628021240234\n",
            "[1,    19] loss: 1.9091222286224365\n",
            "[1,    20] loss: 1.9101721048355103\n",
            "[1,    21] loss: 1.9138758182525635\n",
            "[1,    22] loss: 1.9009225368499756\n",
            "[1,    23] loss: 1.9016693830490112\n",
            "[1,    24] loss: 1.8937867879867554\n",
            "[1,    25] loss: 1.8926249742507935\n",
            "[1,    26] loss: 1.8962913751602173\n",
            "[1,    27] loss: 1.9036096334457397\n",
            "[1,    28] loss: 1.907082200050354\n",
            "[1,    29] loss: 1.91009521484375\n",
            "[1,    30] loss: 1.9088945388793945\n",
            "[1,    31] loss: 1.90977144241333\n",
            "[1,    32] loss: 1.9111442565917969\n",
            "[1,    33] loss: 1.907488226890564\n",
            "[1,    34] loss: 1.9041281938552856\n",
            "[1,    35] loss: 1.903603434562683\n",
            "[1,    36] loss: 1.9046865701675415\n",
            "[2,     1] loss: 1.9082794189453125\n",
            "[2,     2] loss: 1.9056342840194702\n",
            "[2,     3] loss: 1.9015454053878784\n",
            "[2,     4] loss: 1.906982421875\n",
            "[2,     5] loss: 1.9039925336837769\n",
            "[2,     6] loss: 1.9056462049484253\n",
            "[2,     7] loss: 1.9028384685516357\n",
            "[2,     8] loss: 1.9029409885406494\n",
            "[2,     9] loss: 1.9041036367416382\n",
            "[2,    10] loss: 1.906438946723938\n",
            "[2,    11] loss: 1.903200626373291\n",
            "[2,    12] loss: 1.9042232036590576\n",
            "[2,    13] loss: 1.903050422668457\n",
            "[2,    14] loss: 1.9037350416183472\n",
            "[2,    15] loss: 1.9047175645828247\n",
            "[2,    16] loss: 1.9042727947235107\n",
            "[2,    17] loss: 1.9034866094589233\n",
            "[2,    18] loss: 1.9030624628067017\n",
            "[2,    19] loss: 1.9020670652389526\n",
            "[2,    20] loss: 1.9024518728256226\n",
            "[2,    21] loss: 1.901196837425232\n",
            "[2,    22] loss: 1.898940086364746\n",
            "[2,    23] loss: 1.9007644653320312\n",
            "[2,    24] loss: 1.9017884731292725\n",
            "[2,    25] loss: 1.9006716012954712\n",
            "[2,    26] loss: 1.9008748531341553\n",
            "[2,    27] loss: 1.9019775390625\n",
            "[2,    28] loss: 1.9026707410812378\n",
            "[2,    29] loss: 1.9028218984603882\n",
            "[2,    30] loss: 1.900580644607544\n",
            "[2,    31] loss: 1.9010330438613892\n",
            "[2,    32] loss: 1.9014203548431396\n",
            "[2,    33] loss: 1.9032796621322632\n",
            "[2,    34] loss: 1.903931975364685\n",
            "[2,    35] loss: 1.904872179031372\n",
            "[2,    36] loss: 1.9039769172668457\n",
            "[3,     1] loss: 1.9043564796447754\n",
            "[3,     2] loss: 1.904900312423706\n",
            "[3,     3] loss: 1.9026802778244019\n",
            "[3,     4] loss: 1.9025955200195312\n",
            "[3,     5] loss: 1.9033256769180298\n",
            "[3,     6] loss: 1.9042781591415405\n",
            "[3,     7] loss: 1.9049584865570068\n",
            "[3,     8] loss: 1.904292106628418\n",
            "[3,     9] loss: 1.9042878150939941\n",
            "[3,    10] loss: 1.9051563739776611\n",
            "[3,    11] loss: 1.9046717882156372\n",
            "[3,    12] loss: 1.9043598175048828\n",
            "[3,    13] loss: 1.9027453660964966\n",
            "[3,    14] loss: 1.9016265869140625\n",
            "[3,    15] loss: 1.902557373046875\n",
            "[3,    16] loss: 1.9032825231552124\n",
            "[3,    17] loss: 1.9036222696304321\n",
            "[3,    18] loss: 1.90326726436615\n",
            "[3,    19] loss: 1.9041327238082886\n",
            "[3,    20] loss: 1.904296875\n",
            "[3,    21] loss: 1.9055029153823853\n",
            "[3,    22] loss: 1.9059966802597046\n",
            "[3,    23] loss: 1.9064550399780273\n",
            "[3,    24] loss: 1.9062931537628174\n",
            "[3,    25] loss: 1.9046422243118286\n",
            "[3,    26] loss: 1.904045820236206\n",
            "[3,    27] loss: 1.9048813581466675\n",
            "[3,    28] loss: 1.9037691354751587\n",
            "[3,    29] loss: 1.904937505722046\n",
            "[3,    30] loss: 1.9059611558914185\n",
            "[3,    31] loss: 1.9047211408615112\n",
            "[3,    32] loss: 1.903748631477356\n",
            "[3,    33] loss: 1.9028304815292358\n",
            "[3,    34] loss: 1.9042088985443115\n",
            "[3,    35] loss: 1.9040753841400146\n",
            "[3,    36] loss: 1.903995394706726\n",
            "[4,     1] loss: 1.903133749961853\n",
            "[4,     2] loss: 1.9023151397705078\n",
            "[4,     3] loss: 1.903344988822937\n",
            "[4,     4] loss: 1.904304027557373\n",
            "[4,     5] loss: 1.9047234058380127\n",
            "[4,     6] loss: 1.904380202293396\n",
            "[4,     7] loss: 1.9032210111618042\n",
            "[4,     8] loss: 1.9028245210647583\n",
            "[4,     9] loss: 1.902309775352478\n",
            "[4,    10] loss: 1.903119444847107\n",
            "[4,    11] loss: 1.9024728536605835\n",
            "[4,    12] loss: 1.90242600440979\n",
            "[4,    13] loss: 1.9022059440612793\n",
            "[4,    14] loss: 1.9026994705200195\n",
            "[4,    15] loss: 1.9029179811477661\n",
            "[4,    16] loss: 1.9020450115203857\n",
            "[4,    17] loss: 1.902305006980896\n",
            "[4,    18] loss: 1.9018466472625732\n",
            "[4,    19] loss: 1.9014607667922974\n",
            "[4,    20] loss: 1.901857852935791\n",
            "[4,    21] loss: 1.9025049209594727\n",
            "[4,    22] loss: 1.9029732942581177\n",
            "[4,    23] loss: 1.9037096500396729\n",
            "[4,    24] loss: 1.9036335945129395\n",
            "[4,    25] loss: 1.9038476943969727\n",
            "[4,    26] loss: 1.9046528339385986\n",
            "[4,    27] loss: 1.9048956632614136\n",
            "[4,    28] loss: 1.9036513566970825\n",
            "[4,    29] loss: 1.9035165309906006\n",
            "[4,    30] loss: 1.903531551361084\n",
            "[4,    31] loss: 1.9032070636749268\n",
            "[4,    32] loss: 1.9028431177139282\n",
            "[4,    33] loss: 1.9022064208984375\n",
            "[4,    34] loss: 1.9023613929748535\n",
            "[4,    35] loss: 1.9037086963653564\n",
            "[4,    36] loss: 1.904358148574829\n",
            "[5,     1] loss: 1.9042229652404785\n",
            "[5,     2] loss: 1.9045021533966064\n",
            "[5,     3] loss: 1.9045186042785645\n",
            "[5,     4] loss: 1.904710292816162\n",
            "[5,     5] loss: 1.9055507183074951\n",
            "[5,     6] loss: 1.9058122634887695\n",
            "[5,     7] loss: 1.9065043926239014\n",
            "[5,     8] loss: 1.9059911966323853\n",
            "[5,     9] loss: 1.9059287309646606\n",
            "[5,    10] loss: 1.905806303024292\n",
            "[5,    11] loss: 1.9062082767486572\n",
            "[5,    12] loss: 1.9067513942718506\n",
            "[5,    13] loss: 1.9066087007522583\n",
            "[5,    14] loss: 1.9069666862487793\n",
            "[5,    15] loss: 1.9068282842636108\n",
            "[5,    16] loss: 1.9066345691680908\n",
            "[5,    17] loss: 1.9063440561294556\n",
            "[5,    18] loss: 1.9057525396347046\n",
            "[5,    19] loss: 1.9044368267059326\n",
            "[5,    20] loss: 1.904391884803772\n",
            "[5,    21] loss: 1.9036222696304321\n",
            "[5,    22] loss: 1.9039345979690552\n",
            "[5,    23] loss: 1.9038174152374268\n",
            "[5,    24] loss: 1.9044264554977417\n",
            "[5,    25] loss: 1.904593586921692\n",
            "[5,    26] loss: 1.9054092168807983\n",
            "[5,    27] loss: 1.90529465675354\n",
            "[5,    28] loss: 1.905449390411377\n",
            "[5,    29] loss: 1.9055131673812866\n",
            "[5,    30] loss: 1.9046653509140015\n",
            "[5,    31] loss: 1.9049159288406372\n",
            "[5,    32] loss: 1.9048014879226685\n",
            "[5,    33] loss: 1.9049029350280762\n",
            "[5,    34] loss: 1.9053137302398682\n",
            "[5,    35] loss: 1.904766321182251\n",
            "[5,    36] loss: 1.9038699865341187\n",
            "[6,     1] loss: 1.9037448167800903\n",
            "[6,     2] loss: 1.9041330814361572\n",
            "[6,     3] loss: 1.9049220085144043\n",
            "[6,     4] loss: 1.9049009084701538\n",
            "[6,     5] loss: 1.9050713777542114\n",
            "[6,     6] loss: 1.904618501663208\n",
            "[6,     7] loss: 1.9050523042678833\n",
            "[6,     8] loss: 1.9042490720748901\n",
            "[6,     9] loss: 1.9039052724838257\n",
            "[6,    10] loss: 1.9043041467666626\n",
            "[6,    11] loss: 1.9047917127609253\n",
            "[6,    12] loss: 1.90482759475708\n",
            "[6,    13] loss: 1.9052834510803223\n",
            "[6,    14] loss: 1.9050368070602417\n",
            "[6,    15] loss: 1.9056315422058105\n",
            "[6,    16] loss: 1.9049450159072876\n",
            "[6,    17] loss: 1.9051932096481323\n",
            "[6,    18] loss: 1.9053419828414917\n",
            "[6,    19] loss: 1.9046027660369873\n",
            "[6,    20] loss: 1.9051237106323242\n",
            "[6,    21] loss: 1.904828429222107\n",
            "[6,    22] loss: 1.9049808979034424\n",
            "[6,    23] loss: 1.9045467376708984\n",
            "[6,    24] loss: 1.9044740200042725\n",
            "[6,    25] loss: 1.9044861793518066\n",
            "[6,    26] loss: 1.9049183130264282\n",
            "[6,    27] loss: 1.9054255485534668\n",
            "[6,    28] loss: 1.9051541090011597\n",
            "[6,    29] loss: 1.9053510427474976\n",
            "[6,    30] loss: 1.904810905456543\n",
            "[6,    31] loss: 1.9052791595458984\n",
            "[6,    32] loss: 1.9054961204528809\n",
            "[6,    33] loss: 1.9058526754379272\n",
            "[6,    34] loss: 1.9047889709472656\n",
            "[6,    35] loss: 1.9041082859039307\n",
            "[6,    36] loss: 1.9037253856658936\n",
            "[7,     1] loss: 1.9039628505706787\n",
            "[7,     2] loss: 1.9036123752593994\n",
            "[7,     3] loss: 1.9038174152374268\n",
            "[7,     4] loss: 1.9037660360336304\n",
            "[7,     5] loss: 1.9036802053451538\n",
            "[7,     6] loss: 1.9034843444824219\n",
            "[7,     7] loss: 1.9034863710403442\n",
            "[7,     8] loss: 1.903700828552246\n",
            "[7,     9] loss: 1.9031931161880493\n",
            "[7,    10] loss: 1.9024639129638672\n",
            "[7,    11] loss: 1.902345061302185\n",
            "[7,    12] loss: 1.902801275253296\n",
            "[7,    13] loss: 1.9024094343185425\n",
            "[7,    14] loss: 1.9025384187698364\n",
            "[7,    15] loss: 1.9023537635803223\n",
            "[7,    16] loss: 1.9031357765197754\n",
            "[7,    17] loss: 1.9036331176757812\n",
            "[7,    18] loss: 1.90348482131958\n",
            "[7,    19] loss: 1.9036394357681274\n",
            "[7,    20] loss: 1.9028613567352295\n",
            "[7,    21] loss: 1.9034126996994019\n",
            "[7,    22] loss: 1.9029268026351929\n",
            "[7,    23] loss: 1.903281569480896\n",
            "[7,    24] loss: 1.9032821655273438\n",
            "[7,    25] loss: 1.903192162513733\n",
            "[7,    26] loss: 1.9032886028289795\n",
            "[7,    27] loss: 1.9029548168182373\n",
            "[7,    28] loss: 1.902992844581604\n",
            "[7,    29] loss: 1.9029855728149414\n",
            "[7,    30] loss: 1.9030113220214844\n",
            "[7,    31] loss: 1.9031444787979126\n",
            "[7,    32] loss: 1.9028457403182983\n",
            "[7,    33] loss: 1.9026257991790771\n",
            "[7,    34] loss: 1.9030061960220337\n",
            "[7,    35] loss: 1.90339195728302\n",
            "[7,    36] loss: 1.9041777849197388\n",
            "[8,     1] loss: 1.9036448001861572\n",
            "[8,     2] loss: 1.9032771587371826\n",
            "[8,     3] loss: 1.9033461809158325\n",
            "[8,     4] loss: 1.9031155109405518\n",
            "[8,     5] loss: 1.9032158851623535\n",
            "[8,     6] loss: 1.9035649299621582\n",
            "[8,     7] loss: 1.904241919517517\n",
            "[8,     8] loss: 1.904944658279419\n",
            "[8,     9] loss: 1.904721736907959\n",
            "[8,    10] loss: 1.9050580263137817\n",
            "[8,    11] loss: 1.904984712600708\n",
            "[8,    12] loss: 1.9053349494934082\n",
            "[8,    13] loss: 1.9056683778762817\n",
            "[8,    14] loss: 1.9061048030853271\n",
            "[8,    15] loss: 1.9061094522476196\n",
            "[8,    16] loss: 1.906201958656311\n",
            "[8,    17] loss: 1.9061222076416016\n",
            "[8,    18] loss: 1.9061120748519897\n",
            "[8,    19] loss: 1.9063401222229004\n",
            "[8,    20] loss: 1.9060453176498413\n",
            "[8,    21] loss: 1.9057780504226685\n",
            "[8,    22] loss: 1.9054737091064453\n",
            "[8,    23] loss: 1.9051369428634644\n",
            "[8,    24] loss: 1.905249834060669\n",
            "[8,    25] loss: 1.9055589437484741\n",
            "[8,    26] loss: 1.9058610200881958\n",
            "[8,    27] loss: 1.9056980609893799\n",
            "[8,    28] loss: 1.9048954248428345\n",
            "[8,    29] loss: 1.9047772884368896\n",
            "[8,    30] loss: 1.9047616720199585\n",
            "[8,    31] loss: 1.9045785665512085\n",
            "[8,    32] loss: 1.9043574333190918\n",
            "[8,    33] loss: 1.9042094945907593\n",
            "[8,    34] loss: 1.9041386842727661\n",
            "[8,    35] loss: 1.9039855003356934\n",
            "[8,    36] loss: 1.9043614864349365\n",
            "[9,     1] loss: 1.9045376777648926\n",
            "[9,     2] loss: 1.904890775680542\n",
            "[9,     3] loss: 1.9051878452301025\n",
            "[9,     4] loss: 1.9046871662139893\n",
            "[9,     5] loss: 1.9040987491607666\n",
            "[9,     6] loss: 1.9039636850357056\n",
            "[9,     7] loss: 1.9039649963378906\n",
            "[9,     8] loss: 1.9036513566970825\n",
            "[9,     9] loss: 1.9037061929702759\n",
            "[9,    10] loss: 1.9034571647644043\n",
            "[9,    11] loss: 1.9038006067276\n",
            "[9,    12] loss: 1.9038560390472412\n",
            "[9,    13] loss: 1.903639793395996\n",
            "[9,    14] loss: 1.9036206007003784\n",
            "[9,    15] loss: 1.9038912057876587\n",
            "[9,    16] loss: 1.9044086933135986\n",
            "[9,    17] loss: 1.9042612314224243\n",
            "[9,    18] loss: 1.9046467542648315\n",
            "[9,    19] loss: 1.9050288200378418\n",
            "[9,    20] loss: 1.9052338600158691\n",
            "[9,    21] loss: 1.904862403869629\n",
            "[9,    22] loss: 1.9050277471542358\n",
            "[9,    23] loss: 1.9048995971679688\n",
            "[9,    24] loss: 1.9052120447158813\n",
            "[9,    25] loss: 1.9052504301071167\n",
            "[9,    26] loss: 1.904752492904663\n",
            "[9,    27] loss: 1.9050260782241821\n",
            "[9,    28] loss: 1.9052752256393433\n",
            "[9,    29] loss: 1.9053184986114502\n",
            "[9,    30] loss: 1.9045974016189575\n",
            "[9,    31] loss: 1.90446138381958\n",
            "[9,    32] loss: 1.9043458700180054\n",
            "[9,    33] loss: 1.9043298959732056\n",
            "[9,    34] loss: 1.9044532775878906\n",
            "[9,    35] loss: 1.9044982194900513\n",
            "[9,    36] loss: 1.904178261756897\n",
            "[10,     1] loss: 1.9040709733963013\n",
            "[10,     2] loss: 1.9043980836868286\n",
            "[10,     3] loss: 1.9047783613204956\n",
            "[10,     4] loss: 1.904757022857666\n",
            "[10,     5] loss: 1.9052560329437256\n",
            "[10,     6] loss: 1.905122995376587\n",
            "[10,     7] loss: 1.9052153825759888\n",
            "[10,     8] loss: 1.9053088426589966\n",
            "[10,     9] loss: 1.9050475358963013\n",
            "[10,    10] loss: 1.9051142930984497\n",
            "[10,    11] loss: 1.9050410985946655\n",
            "[10,    12] loss: 1.9050267934799194\n",
            "[10,    13] loss: 1.904687523841858\n",
            "[10,    14] loss: 1.9050421714782715\n",
            "[10,    15] loss: 1.9050966501235962\n",
            "[10,    16] loss: 1.9046063423156738\n",
            "[10,    17] loss: 1.9044564962387085\n",
            "[10,    18] loss: 1.9043798446655273\n",
            "[10,    19] loss: 1.9041374921798706\n",
            "[10,    20] loss: 1.9043855667114258\n",
            "[10,    21] loss: 1.9045580625534058\n",
            "[10,    22] loss: 1.9043433666229248\n",
            "[10,    23] loss: 1.9040124416351318\n",
            "[10,    24] loss: 1.9040476083755493\n",
            "[10,    25] loss: 1.9041423797607422\n",
            "[10,    26] loss: 1.9044533967971802\n",
            "[10,    27] loss: 1.9041857719421387\n",
            "[10,    28] loss: 1.903862714767456\n",
            "[10,    29] loss: 1.903700828552246\n",
            "[10,    30] loss: 1.903942584991455\n",
            "[10,    31] loss: 1.9040303230285645\n",
            "[10,    32] loss: 1.9043066501617432\n",
            "[10,    33] loss: 1.9042692184448242\n",
            "[10,    34] loss: 1.9041128158569336\n",
            "[10,    35] loss: 1.9042541980743408\n",
            "[10,    36] loss: 1.9040968418121338\n",
            "[11,     1] loss: 1.9041881561279297\n",
            "[11,     2] loss: 1.9037079811096191\n",
            "[11,     3] loss: 1.9032018184661865\n",
            "[11,     4] loss: 1.902901530265808\n",
            "[11,     5] loss: 1.9028301239013672\n",
            "[11,     6] loss: 1.9027129411697388\n",
            "[11,     7] loss: 1.90235435962677\n",
            "[11,     8] loss: 1.9025068283081055\n",
            "[11,     9] loss: 1.902646541595459\n",
            "[11,    10] loss: 1.9023547172546387\n",
            "[11,    11] loss: 1.902263879776001\n",
            "[11,    12] loss: 1.9022961854934692\n",
            "[11,    13] loss: 1.902291178703308\n",
            "[11,    14] loss: 1.902475118637085\n",
            "[11,    15] loss: 1.9024944305419922\n",
            "[11,    16] loss: 1.9023672342300415\n",
            "[11,    17] loss: 1.9024934768676758\n",
            "[11,    18] loss: 1.90274977684021\n",
            "[11,    19] loss: 1.903085470199585\n",
            "[11,    20] loss: 1.903128981590271\n",
            "[11,    21] loss: 1.903409719467163\n",
            "[11,    22] loss: 1.9035687446594238\n",
            "[11,    23] loss: 1.903909683227539\n",
            "[11,    24] loss: 1.9035165309906006\n",
            "[11,    25] loss: 1.903842806816101\n",
            "[11,    26] loss: 1.9037389755249023\n",
            "[11,    27] loss: 1.9037386178970337\n",
            "[11,    28] loss: 1.9035320281982422\n",
            "[11,    29] loss: 1.9036954641342163\n",
            "[11,    30] loss: 1.9035218954086304\n",
            "[11,    31] loss: 1.9039191007614136\n",
            "[11,    32] loss: 1.903860092163086\n",
            "[11,    33] loss: 1.9041582345962524\n",
            "[11,    34] loss: 1.904098391532898\n",
            "[11,    35] loss: 1.9041593074798584\n",
            "[11,    36] loss: 1.9040443897247314\n",
            "[12,     1] loss: 1.9040825366973877\n",
            "[12,     2] loss: 1.9042414426803589\n",
            "[12,     3] loss: 1.904094934463501\n",
            "[12,     4] loss: 1.904366374015808\n",
            "[12,     5] loss: 1.9046293497085571\n",
            "[12,     6] loss: 1.9044398069381714\n",
            "[12,     7] loss: 1.904488444328308\n",
            "[12,     8] loss: 1.9040641784667969\n",
            "[12,     9] loss: 1.9040063619613647\n",
            "[12,    10] loss: 1.9042203426361084\n",
            "[12,    11] loss: 1.9043468236923218\n",
            "[12,    12] loss: 1.9043657779693604\n",
            "[12,    13] loss: 1.904334306716919\n",
            "[12,    14] loss: 1.9042835235595703\n",
            "[12,    15] loss: 1.9043748378753662\n",
            "[12,    16] loss: 1.9043461084365845\n",
            "[12,    17] loss: 1.9044157266616821\n",
            "[12,    18] loss: 1.9044241905212402\n",
            "[12,    19] loss: 1.904301643371582\n",
            "[12,    20] loss: 1.90459406375885\n",
            "[12,    21] loss: 1.9042611122131348\n",
            "[12,    22] loss: 1.9044119119644165\n",
            "[12,    23] loss: 1.9043580293655396\n",
            "[12,    24] loss: 1.9044201374053955\n",
            "[12,    25] loss: 1.9040569067001343\n",
            "[12,    26] loss: 1.9039121866226196\n",
            "[12,    27] loss: 1.9037981033325195\n",
            "[12,    28] loss: 1.9034810066223145\n",
            "[12,    29] loss: 1.903457760810852\n",
            "[12,    30] loss: 1.9034452438354492\n",
            "[12,    31] loss: 1.903172254562378\n",
            "[12,    32] loss: 1.9033665657043457\n",
            "[12,    33] loss: 1.9037721157073975\n",
            "[12,    34] loss: 1.9039373397827148\n",
            "[12,    35] loss: 1.9041450023651123\n",
            "[12,    36] loss: 1.9039617776870728\n",
            "[13,     1] loss: 1.904036283493042\n",
            "[13,     2] loss: 1.903893232345581\n",
            "[13,     3] loss: 1.9038310050964355\n",
            "[13,     4] loss: 1.903517484664917\n",
            "[13,     5] loss: 1.903630018234253\n",
            "[13,     6] loss: 1.9035955667495728\n",
            "[13,     7] loss: 1.9033890962600708\n",
            "[13,     8] loss: 1.9035404920578003\n",
            "[13,     9] loss: 1.9037257432937622\n",
            "[13,    10] loss: 1.9037870168685913\n",
            "[13,    11] loss: 1.9040883779525757\n",
            "[13,    12] loss: 1.9040979146957397\n",
            "[13,    13] loss: 1.903881311416626\n",
            "[13,    14] loss: 1.9040236473083496\n",
            "[13,    15] loss: 1.9040136337280273\n",
            "[13,    16] loss: 1.9040848016738892\n",
            "[13,    17] loss: 1.9039971828460693\n",
            "[13,    18] loss: 1.9041111469268799\n",
            "[13,    19] loss: 1.903801679611206\n",
            "[13,    20] loss: 1.9039918184280396\n",
            "[13,    21] loss: 1.9043192863464355\n",
            "[13,    22] loss: 1.9041953086853027\n",
            "[13,    23] loss: 1.9042938947677612\n",
            "[13,    24] loss: 1.9042223691940308\n",
            "[13,    25] loss: 1.9041746854782104\n",
            "[13,    26] loss: 1.9040461778640747\n",
            "[13,    27] loss: 1.9043880701065063\n",
            "[13,    28] loss: 1.9040982723236084\n",
            "[13,    29] loss: 1.9040457010269165\n",
            "[13,    30] loss: 1.9041060209274292\n",
            "[13,    31] loss: 1.9038194417953491\n",
            "[13,    32] loss: 1.9037997722625732\n",
            "[13,    33] loss: 1.903765320777893\n",
            "[13,    34] loss: 1.9037716388702393\n",
            "[13,    35] loss: 1.9039487838745117\n",
            "[13,    36] loss: 1.904003620147705\n",
            "[14,     1] loss: 1.9038516283035278\n",
            "[14,     2] loss: 1.9039369821548462\n",
            "[14,     3] loss: 1.904118299484253\n",
            "[14,     4] loss: 1.9040683507919312\n",
            "[14,     5] loss: 1.9041016101837158\n",
            "[14,     6] loss: 1.9042450189590454\n",
            "[14,     7] loss: 1.9041694402694702\n",
            "[14,     8] loss: 1.9041950702667236\n",
            "[14,     9] loss: 1.9045929908752441\n",
            "[14,    10] loss: 1.9048138856887817\n",
            "[14,    11] loss: 1.9046897888183594\n",
            "[14,    12] loss: 1.904451847076416\n",
            "[14,    13] loss: 1.9045438766479492\n",
            "[14,    14] loss: 1.9049100875854492\n",
            "[14,    15] loss: 1.9048924446105957\n",
            "[14,    16] loss: 1.904833197593689\n",
            "[14,    17] loss: 1.9046168327331543\n",
            "[14,    18] loss: 1.9046032428741455\n",
            "[14,    19] loss: 1.9048020839691162\n",
            "[14,    20] loss: 1.9050413370132446\n",
            "[14,    21] loss: 1.9044442176818848\n",
            "[14,    22] loss: 1.904515266418457\n",
            "[14,    23] loss: 1.9045062065124512\n",
            "[14,    24] loss: 1.9046306610107422\n",
            "[14,    25] loss: 1.9045573472976685\n",
            "[14,    26] loss: 1.904456377029419\n",
            "[14,    27] loss: 1.9043298959732056\n",
            "[14,    28] loss: 1.9042972326278687\n",
            "[14,    29] loss: 1.9040945768356323\n",
            "[14,    30] loss: 1.9040625095367432\n",
            "[14,    31] loss: 1.9043012857437134\n",
            "[14,    32] loss: 1.9043704271316528\n",
            "[14,    33] loss: 1.9041043519973755\n",
            "[14,    34] loss: 1.9039629697799683\n",
            "[14,    35] loss: 1.9039498567581177\n",
            "[14,    36] loss: 1.9040786027908325\n",
            "[15,     1] loss: 1.9038598537445068\n",
            "[15,     2] loss: 1.9041134119033813\n",
            "[15,     3] loss: 1.9038642644882202\n",
            "[15,     4] loss: 1.9038113355636597\n",
            "[15,     5] loss: 1.9036346673965454\n",
            "[15,     6] loss: 1.90366792678833\n",
            "[15,     7] loss: 1.9036234617233276\n",
            "[15,     8] loss: 1.9036895036697388\n",
            "[15,     9] loss: 1.9038267135620117\n",
            "[15,    10] loss: 1.9039579629898071\n",
            "[15,    11] loss: 1.903624176979065\n",
            "[15,    12] loss: 1.9036033153533936\n",
            "[15,    13] loss: 1.9036872386932373\n",
            "[15,    14] loss: 1.9035700559616089\n",
            "[15,    15] loss: 1.9034030437469482\n",
            "[15,    16] loss: 1.9036965370178223\n",
            "[15,    17] loss: 1.903622031211853\n",
            "[15,    18] loss: 1.9039111137390137\n",
            "[15,    19] loss: 1.9039627313613892\n",
            "[15,    20] loss: 1.9039862155914307\n",
            "[15,    21] loss: 1.9041485786437988\n",
            "[15,    22] loss: 1.9041110277175903\n",
            "[15,    23] loss: 1.9038885831832886\n",
            "[15,    24] loss: 1.9038021564483643\n",
            "[15,    25] loss: 1.9040039777755737\n",
            "[15,    26] loss: 1.9039978981018066\n",
            "[15,    27] loss: 1.904251217842102\n",
            "[15,    28] loss: 1.9040993452072144\n",
            "[15,    29] loss: 1.9043766260147095\n",
            "[15,    30] loss: 1.9042598009109497\n",
            "[15,    31] loss: 1.9040409326553345\n",
            "[15,    32] loss: 1.9040266275405884\n",
            "[15,    33] loss: 1.9040476083755493\n",
            "[15,    34] loss: 1.903991937637329\n",
            "[15,    35] loss: 1.9041118621826172\n",
            "[15,    36] loss: 1.9040547609329224\n",
            "[16,     1] loss: 1.9039220809936523\n",
            "[16,     2] loss: 1.9039493799209595\n",
            "[16,     3] loss: 1.904080867767334\n",
            "[16,     4] loss: 1.9041869640350342\n",
            "[16,     5] loss: 1.9044451713562012\n",
            "[16,     6] loss: 1.9045745134353638\n",
            "[16,     7] loss: 1.904561161994934\n",
            "[16,     8] loss: 1.9044541120529175\n",
            "[16,     9] loss: 1.9046757221221924\n",
            "[16,    10] loss: 1.9046038389205933\n",
            "[16,    11] loss: 1.9048305749893188\n",
            "[16,    12] loss: 1.904822587966919\n",
            "[16,    13] loss: 1.9049136638641357\n",
            "[16,    14] loss: 1.9048755168914795\n",
            "[16,    15] loss: 1.904799222946167\n",
            "[16,    16] loss: 1.904705286026001\n",
            "[16,    17] loss: 1.9045684337615967\n",
            "[16,    18] loss: 1.9044134616851807\n",
            "[16,    19] loss: 1.9043900966644287\n",
            "[16,    20] loss: 1.9042500257492065\n",
            "[16,    21] loss: 1.904266119003296\n",
            "[16,    22] loss: 1.904247760772705\n",
            "[16,    23] loss: 1.9041041135787964\n",
            "[16,    24] loss: 1.9041407108306885\n",
            "[16,    25] loss: 1.9041391611099243\n",
            "[16,    26] loss: 1.9039959907531738\n",
            "[16,    27] loss: 1.9039366245269775\n",
            "[16,    28] loss: 1.904022216796875\n",
            "[16,    29] loss: 1.904077410697937\n",
            "[16,    30] loss: 1.9042577743530273\n",
            "[16,    31] loss: 1.904244065284729\n",
            "[16,    32] loss: 1.9043587446212769\n",
            "[16,    33] loss: 1.904124140739441\n",
            "[16,    34] loss: 1.9042041301727295\n",
            "[16,    35] loss: 1.9040048122406006\n",
            "[16,    36] loss: 1.904117226600647\n",
            "[17,     1] loss: 1.9042264223098755\n",
            "[17,     2] loss: 1.904308557510376\n",
            "[17,     3] loss: 1.9045774936676025\n",
            "[17,     4] loss: 1.9042770862579346\n",
            "[17,     5] loss: 1.9041426181793213\n",
            "[17,     6] loss: 1.904222011566162\n",
            "[17,     7] loss: 1.9041486978530884\n",
            "[17,     8] loss: 1.9042069911956787\n",
            "[17,     9] loss: 1.9042612314224243\n",
            "[17,    10] loss: 1.9040789604187012\n",
            "[17,    11] loss: 1.9040324687957764\n",
            "[17,    12] loss: 1.9044243097305298\n",
            "[17,    13] loss: 1.9043843746185303\n",
            "[17,    14] loss: 1.9042760133743286\n",
            "[17,    15] loss: 1.9042912721633911\n",
            "[17,    16] loss: 1.9042820930480957\n",
            "[17,    17] loss: 1.9041558504104614\n",
            "[17,    18] loss: 1.9043395519256592\n",
            "[17,    19] loss: 1.9043816328048706\n",
            "[17,    20] loss: 1.9043710231781006\n",
            "[17,    21] loss: 1.9042565822601318\n",
            "[17,    22] loss: 1.9045525789260864\n",
            "[17,    23] loss: 1.9043302536010742\n",
            "[17,    24] loss: 1.90401291847229\n",
            "[17,    25] loss: 1.903969645500183\n",
            "[17,    26] loss: 1.9041202068328857\n",
            "[17,    27] loss: 1.903839111328125\n",
            "[17,    28] loss: 1.9040157794952393\n",
            "[17,    29] loss: 1.9041001796722412\n",
            "[17,    30] loss: 1.90414559841156\n",
            "[17,    31] loss: 1.9039268493652344\n",
            "[17,    32] loss: 1.9040905237197876\n",
            "[17,    33] loss: 1.9040457010269165\n",
            "[17,    34] loss: 1.9041252136230469\n",
            "[17,    35] loss: 1.9041788578033447\n",
            "[17,    36] loss: 1.904060959815979\n",
            "[18,     1] loss: 1.9042142629623413\n",
            "[18,     2] loss: 1.904442310333252\n",
            "[18,     3] loss: 1.9046908617019653\n",
            "[18,     4] loss: 1.9047774076461792\n",
            "[18,     5] loss: 1.9045649766921997\n",
            "[18,     6] loss: 1.9044336080551147\n",
            "[18,     7] loss: 1.904541254043579\n",
            "[18,     8] loss: 1.9047061204910278\n",
            "[18,     9] loss: 1.9045766592025757\n",
            "[18,    10] loss: 1.9045590162277222\n",
            "[18,    11] loss: 1.9046028852462769\n",
            "[18,    12] loss: 1.904677391052246\n",
            "[18,    13] loss: 1.9050159454345703\n",
            "[18,    14] loss: 1.9048566818237305\n",
            "[18,    15] loss: 1.9048606157302856\n",
            "[18,    16] loss: 1.9048292636871338\n",
            "[18,    17] loss: 1.9046951532363892\n",
            "[18,    18] loss: 1.9045664072036743\n",
            "[18,    19] loss: 1.9047011137008667\n",
            "[18,    20] loss: 1.9046635627746582\n",
            "[18,    21] loss: 1.9046883583068848\n",
            "[18,    22] loss: 1.9046229124069214\n",
            "[18,    23] loss: 1.9046577215194702\n",
            "[18,    24] loss: 1.9044476747512817\n",
            "[18,    25] loss: 1.9043927192687988\n",
            "[18,    26] loss: 1.9042302370071411\n",
            "[18,    27] loss: 1.9041690826416016\n",
            "[18,    28] loss: 1.9041286706924438\n",
            "[18,    29] loss: 1.9041177034378052\n",
            "[18,    30] loss: 1.9042110443115234\n",
            "[18,    31] loss: 1.904261589050293\n",
            "[18,    32] loss: 1.90416419506073\n",
            "[18,    33] loss: 1.904163122177124\n",
            "[18,    34] loss: 1.9039585590362549\n",
            "[18,    35] loss: 1.9040662050247192\n",
            "[18,    36] loss: 1.9040653705596924\n",
            "[19,     1] loss: 1.903975486755371\n",
            "[19,     2] loss: 1.9040967226028442\n",
            "[19,     3] loss: 1.9041017293930054\n",
            "[19,     4] loss: 1.904120683670044\n",
            "[19,     5] loss: 1.9043835401535034\n",
            "[19,     6] loss: 1.9043192863464355\n",
            "[19,     7] loss: 1.9042987823486328\n",
            "[19,     8] loss: 1.9042044878005981\n",
            "[19,     9] loss: 1.9041744470596313\n",
            "[19,    10] loss: 1.9042208194732666\n",
            "[19,    11] loss: 1.9041651487350464\n",
            "[19,    12] loss: 1.9041281938552856\n",
            "[19,    13] loss: 1.9041705131530762\n",
            "[19,    14] loss: 1.9041858911514282\n",
            "[19,    15] loss: 1.9039866924285889\n",
            "[19,    16] loss: 1.9040348529815674\n",
            "[19,    17] loss: 1.903845191001892\n",
            "[19,    18] loss: 1.903883457183838\n",
            "[19,    19] loss: 1.9039019346237183\n",
            "[19,    20] loss: 1.9041712284088135\n",
            "[19,    21] loss: 1.904050350189209\n",
            "[19,    22] loss: 1.9041023254394531\n",
            "[19,    23] loss: 1.9042787551879883\n",
            "[19,    24] loss: 1.904147982597351\n",
            "[19,    25] loss: 1.9041706323623657\n",
            "[19,    26] loss: 1.9041677713394165\n",
            "[19,    27] loss: 1.904241919517517\n",
            "[19,    28] loss: 1.9042876958847046\n",
            "[19,    29] loss: 1.9042428731918335\n",
            "[19,    30] loss: 1.9041146039962769\n",
            "[19,    31] loss: 1.9041897058486938\n",
            "[19,    32] loss: 1.9041708707809448\n",
            "[19,    33] loss: 1.9042363166809082\n",
            "[19,    34] loss: 1.9042222499847412\n",
            "[19,    35] loss: 1.9040992259979248\n",
            "[19,    36] loss: 1.9040402173995972\n",
            "[20,     1] loss: 1.9040018320083618\n",
            "[20,     2] loss: 1.904118299484253\n",
            "[20,     3] loss: 1.90408456325531\n",
            "[20,     4] loss: 1.9040050506591797\n",
            "[20,     5] loss: 1.9039900302886963\n",
            "[20,     6] loss: 1.9041388034820557\n",
            "[20,     7] loss: 1.9040595293045044\n",
            "[20,     8] loss: 1.9039713144302368\n",
            "[20,     9] loss: 1.903984546661377\n",
            "[20,    10] loss: 1.9037981033325195\n",
            "[20,    11] loss: 1.9036086797714233\n",
            "[20,    12] loss: 1.9036272764205933\n",
            "[20,    13] loss: 1.9038219451904297\n",
            "[20,    14] loss: 1.9036979675292969\n",
            "[20,    15] loss: 1.9035533666610718\n",
            "[20,    16] loss: 1.9036692380905151\n",
            "[20,    17] loss: 1.9036740064620972\n",
            "[20,    18] loss: 1.9035899639129639\n",
            "[20,    19] loss: 1.9037842750549316\n",
            "[20,    20] loss: 1.9036515951156616\n",
            "[20,    21] loss: 1.9035803079605103\n",
            "[20,    22] loss: 1.9037595987319946\n",
            "[20,    23] loss: 1.9036955833435059\n",
            "[20,    24] loss: 1.9038405418395996\n",
            "[20,    25] loss: 1.9038692712783813\n",
            "[20,    26] loss: 1.9039161205291748\n",
            "[20,    27] loss: 1.9039613008499146\n",
            "[20,    28] loss: 1.904004693031311\n",
            "[20,    29] loss: 1.9040085077285767\n",
            "[20,    30] loss: 1.9040570259094238\n",
            "[20,    31] loss: 1.9039572477340698\n",
            "[20,    32] loss: 1.9039677381515503\n",
            "[20,    33] loss: 1.9039850234985352\n",
            "[20,    34] loss: 1.9040950536727905\n",
            "[20,    35] loss: 1.9040518999099731\n",
            "[20,    36] loss: 1.9040393829345703\n",
            "Accuracy of the network on test images: 13 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Algorithms\n"
      ],
      "metadata": {
        "id": "Mh8dggvyxibT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_images = []\n",
        "for i in df['Image_name']:\n",
        "  image = cv2.imread('/content/drive/MyDrive/Data_set_DataMining/Images/' + i)\n",
        "  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  flattened_images.append(image.flatten())"
      ],
      "metadata": {
        "id": "xgpvOUACx_f_"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(flattened_images, df['level'].astype(int), test_size = 0.2, random_state = 42)\n",
        "model = GaussianNB()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(round(100*acc),\"%\")\n",
        "segmented_acc.append(round(100*acc))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_segmented.append(TIME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKDZbFwq0IJ6",
        "outputId": "fed879e2-4b52-406b-ebf4-ea6c6796b252"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(flattened_images, df['level'].astype(int), test_size = 0.2, random_state = 42)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(round(100*acc),\"%\")\n",
        "segmented_acc.append(round(100*acc))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_segmented.append(TIME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yH4FdTCE1Yrq",
        "outputId": "a626a913-2634-40dd-f1f7-7deacc5b6c6b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_images = []\n",
        "for i in df['Image_name']:\n",
        "  image = cv2.imread('/content/drive/MyDrive/Data_set_DataMining/Segmented_Images/' + i)\n",
        "  image = cv2.resize(image, (64, 64), interpolation = cv2.INTER_AREA)\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  flattened_images.append(image.flatten())"
      ],
      "metadata": {
        "id": "OSHQpZp-1l0r"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(flattened_images, df['level'].astype(int), test_size = 0.2, random_state = 42)\n",
        "model = GaussianNB()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(round(100*acc),\"%\")\n",
        "unsegmented_acc.append(round(100*acc))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_unsegmented.append(TIME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fory4Ypp1uhk",
        "outputId": "094f51f3-a148-475b-8f6a-40c821a44a11"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "85 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.process_time()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(flattened_images, df['level'].astype(int), test_size = 0.2, random_state = 42)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "acc = accuracy_score(y_test, pred)\n",
        "print(round(100*acc),\"%\")\n",
        "unsegmented_acc.append(round(100*acc))\n",
        "\n",
        "TIME = time.process_time() - start\n",
        "time_taken_unsegmented.append(TIME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-pECIDb1ukK",
        "outputId": "472f2b22-6e09-488a-c3c6-cbd65fa89fe0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "81 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\"ANN\", \"CNN\", \"GaussianNB\", \"DecisionTreeClassifier\"]"
      ],
      "metadata": {
        "id": "V9gVa6mq1-tq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15, 5))\n",
        "plt.barh(names, segmented_acc)\n",
        "plt.xlabel(\"ACCURACY SEGMENTED IMAGES(%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "6PBw3vrz2XIc",
        "outputId": "688d77d4-06db-4958-d95e-4a7eb74113c1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15, 5))\n",
        "plt.barh(names, unsegmented_acc)\n",
        "plt.xlabel(\"ACCURACY FOR UNSEGMENTED IMAGES(%)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "HdYPkAe62crt",
        "outputId": "66ad99b9-7cc8-4244-a71d-0629ed87ecde"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15, 5))\n",
        "plt.barh(names, time_taken_segmented)\n",
        "plt.xlabel(\"TIME TAKEN SEGMENTED IMAGES\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "GA_n30PD3dE5",
        "outputId": "5d90e7d2-4dfe-467d-a38b-a7f94d25edc0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAE9CAYAAAAsxb5eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfXzP9f7H8ed3V5bZCpG5CAnDEiG5yOS6IleVi0xXo7EYKRaGOkQdHFo6ialcc2yII3Slzq1mpyayynHI1QwxzPa1y+8+vz/89j3W5jLbd7097rdbt1vf9+f9+Xxen89et53z7P35fGezLMsSAAAAAACGcnN1AQAAAAAAFCeCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARvNwdQH44/Ly8mS32+Xp6SmbzebqcgAAAACgRFmWpZycHPn4+MjNrfD6LsHXAHa7XXv37nV1GQAAAADgUvXq1ZOvr2+hcYKvATw9PSVd+CF7eXm5uBrczBITExUYGOjqMnCTow9RGtCHKA3oQ5QGJdWH2dnZ2rt3rzMb/R7B1wD5jzd7eXmpTJkyLq4GNzt6EKUBfYjSgD5EaUAfojQoyT681KuffLkVAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0jytNSEpKUo8ePRQYGCjLsuTu7q7Q0FC1atXqqk8SGxsrX19fde7cudC2X375RZ9++qlGjhx51cf75ptv9N5770mSduzYofvuu0+S9Morr6hx48ZXfZzfW7dunRYvXiwvLy/l5uYqJCRE3bp1U1RUlMqXL69BgwZd97Eladq0aRo8eLB8fHwUHBysDh06yNfXVy1atFDTpk3/0LEBAAAAAEW7YvCVpNq1a2vJkiWSpMOHDys0NFSzZ89WQEDAVZ2kT58+l9zWoEEDNWjQ4KqOk69NmzZq06aNJKlly5bO2v6IhIQELVu2TB9++KH8/PyUkpKi/v37q169en/42PkmTJggSfruu+9Us2ZNjRkz5oYdGwAAAABQtKsKvhe78847FRoaquXLl6t+/frasGGD3Nzc1KlTJz333HM6d+6cXn75ZaWnp8vX11ezZ8/WokWLVL58efXs2VOjRo1Sdna2srOzNWnSJKWnp2vZsmV6++23tWnTJn344Ydyd3dXo0aNNHHiREVFRSktLU0HDhzQ4cOHNX78eAUFBRVZW3x8vBYtWqTz589r3LhxSk5O1qJFi+Th4aHAwEBFRETI4XAoMjJSR44cUW5urkaOHKlWrVpp6dKlevHFF+Xn5ydJqlixomJiYpyfJSk3N1fjxo3TiRMndP78eY0YMUIPPfSQ1q1bp6VLl8rT01MBAQGaPHlykWPBwcGKjIzU9OnTlZycrFmzZunkyZPq2rWr2rVrV2RdwcHBqlu3riRp0qRJ1/MzBgAAAICb2jUHX0kKDAzUrFmzdODAAa1YsUKSNGDAAHXr1k2rVq1S27ZtNXjwYH344YeKi4tz7hcXF6c77rhDb7zxho4cOaIDBw6oTJkykiS73a6//e1vWrdunXx8fBQaGqrt27dLko4fP64FCxbo66+/1sqVKy8ZfCVp79692rJli3JychQZGalVq1bJy8tL4eHhSkhI0JEjR1SpUiW98cYbOn36tJ5++mlt2LBBv/76a6EV7ItDrySlpqaqbdu26t27t44cOaLw8HA99NBDio6O1vvvvy9/f3/FxMQoMzOzyLF848aN07JlyzRmzBhFRERIkjZs2FBkXZJUt25dDRgw4Io/l8TExCvOAYpbQkKCq0sA6EOUCvQhSgP6EKVBaejD6wq+drtdZcuW1aFDhzR48GDn2NGjR/Xzzz8rPDxckvTMM89IuvAeryQ1adJEc+bM0aRJk9SlSxe1a9dO8fHxkqSDBw+qZs2a8vHxkSTdf//9zv3y3+GtUqWK0tLSLltb/fr15eXlpV9++UXJycl6/vnnJUlpaWlKTk7WDz/8oISEBO3YsUOSlJWVpezsbNlsNuXl5V322H5+ftq9e7dWrVolNzc3nT17VpLUvXt3hYWF6bHHHlP37t3l7e1d5NjlXKouSVf93nJgYKDzPyQArpCQkKBmzZq5ugzc5OhDlAb0IUoD+hClQUn1YVZW1mUXAq8r+CYmJiorK0vt27fX66+/XmBbdHT0JQNk5cqVtX79esXHx2vFihXauXOnWrRoIUmy2WyyLMs5NycnxxniPDyuvkwvLy9JkqenpwIDAxUdHV1g+65duxQaGqru3bsXGL/rrrv0448/yt/f3zm2f/9+ValSxfl548aNSk1N1fLly3X27Fk9/vjjkqQXXnhBPXr00JYtW/T0009r6dKlRY5djqenZ5F15W8DAAAAAFyfa/5zRocPH9aHH36opUuXKj4+XhkZGbIsS1OnTlVmZqYCAwOdjyivXLlSa9eude777bff6ttvv1Xbtm0VGRlZIJHXqlVLhw4dUnp6uiTp3//+twIDA6/7wmrXrq39+/crJSVFkvT222/rxIkTuvfee/X5559LklJSUjR79mxJ0uDBg/XOO+845588eVKjRo3SsWPHnMc8c+aMqlevLjc3N3366afKzs5WXl6e/va3v6lSpUp69tln1aRJEyUnJxc5djmXqgsAAAAA8Mdc1VLqgQMHFBwcrOzsbDkcDk2aNElVq1bV4MGD9dRTT8nd3V2dOnWSt7e3nn76aY0dO1bBwcHy8fHRzJkz9cEHH0i68MVYr7zyihYuXCibzaaRI0fK4XBIksqWLauxY8cqJCREbm5uatasmZo3b17gHeFrccstt2j8+PEaMmSIvLy81LBhQ1WuXFkPP/ywtm/frv79+8vhcOjFF1+UdOEx7NGjR+v555/XLbfcIg8PD02YMEF3332385hdunTRsGHDtHPnTvXt21dVqlTRu+++Kx8fH/Xr10++vr6qUaOGGjRooG+++abQ2OVcqi4AAAAAwB9jsy5+vhh/SvnPs/OOL1yNd4lQGtCHKA3oQ5QG9CFKg5J+x/dSmeiaH3UGAAAAAODPhOALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACM5uHqAorDoUOHNH36dKWkpEiSqlatqsmTJ6tChQo3/Fzvv/++WrRooaZNm171PklJSercubPWrl2rgIAASVJsbKwkqU+fPurQoYOqVKkid3d3nT9/Xo8//rgGDBhww2sHAAAAgJuBcSu+DodDI0aMUEhIiP7xj3/oH//4hxo1aqRp06YVy/mGDh16TaE33913361Zs2ZdcvuCBQu0ZMkSLVmyRFFRUXI4HH+kTAAAAAC4aRm34vvNN9+obt26at68uXMsJCRElmVpz549eu211+Th4SE3NzfNnTtX6enpGjlyZIEV17ffflsHDx7UnDlz5O3trYoVK2rmzJmKj48vNBYZGamuXbuqRYsWGjNmjM6fP6/MzExFRkaqcePG6ty5s/r166cvv/xS2dnZ+uCDDyRJjRo1UkZGhuLi4tSqVatLXk9qaqrKly8vd3f34r1xAAAAAGAo44Lvr7/+qvr16xcYc3O7sLCdkpKiyMhINWzYUHPnztWGDRv00EMPFXmcpUuXKiIiQs2bN9fWrVt19uzZIsfynTx5Uk888YQ6deqkuLg4LViwwLlSe9dddykkJESjR4/W9u3bnY83jx49WuPGjdMDDzxQ6PxDhgyRzWbT/v37FRkZeVXXnpiYeFXzgOKUkJDg6hIA+hClAn2I0oA+RGlQGvrQuODr5uam3Nxc5+dhw4YpPT1dx48fV1RUlGbOnKnMzEz99ttv6tGjxyWP061bN02ePFk9evTQo48+qkqVKhU5lu/222/Xu+++q+joaGVnZ6ts2bLObfmrz1WqVFFaWppzvFatWmrYsKE2bdpU6PwLFiyQj4+P0tPT9cwzzyggIEB16tS57LUHBgaqTJkyV75JQDFJSEhQs2bNXF0GbnL0IUoD+hClAX2I0qCk+jArK+uyC4HGveNbt25d7d692/n573//u5YsWSKHw6Fp06Zp8ODBWrp0qfr16ydJstlsBfbPD829evXS4sWLVb58eQ0bNkz79+8vcizfRx99pDvuuEMrVqzQlClTChzz4seULcsqsC0sLEzvv/9+gbB+sXLlyun+++/Xzp07r/1mAAAAAADMC74PPPCAjh8/ri+++MI59tNPP8lut+vEiRO68847lZ2dra+++ko5OTkqV66cUlJSZFmWTp48qSNHjkiS5s2bJw8PD/Xr10+PPPKI9u/fX+RYvjNnzujOO++UJH322WfKycm5qnpvv/12derUSStXrixyu2VZ2r17t2rXrn29twQAAAAAbmrGPepss9m0cOFCvf7665o3b548PT1VtmxZ/f3vf9d///tfhYWFqUaNGgoODtbrr7+uRx55RK1bt1bfvn0VEBCgBg0aSLrwJ5CeffZZ+fn5yc/PT88++6zsdnuhsfyA3bNnT40bN06bN2/WU089pY0bNyomJuaqan7uuee0YsWKAmNDhgyRu7u7MjMzFRQUpPvuu+/G3igAAAAAuEnYrN8/e4s/nfzn2XnHF67Gu0QoDehDlAb0IUoD+hClQUm/43upTGTco84AAAAAAFyM4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGM3D1QXgxqkzba2O2XNcXQZudst/dnUFAH2I0oE+RGlAH+IGccwKdnUJfwgrvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIJvMTp48KCGDh2qxx9/XH369NFf/vIXZWdnq0OHDlqyZIlzXlJSkiIiIiRJERERGjFiRIHjBAcHl2jdAAAAAGASgm8xcTgcGjFihEJCQrRmzRrFxMRIkubNm6eKFStq9erVSk9PL3LfQ4cOaefOnSVZLgAAAAAYi+BbTL755hvddddduv/++yVJNptNr7zyisLCwuTt7a3+/fsrOjq6yH1HjRqlWbNmlWS5AAAAAGAsD1cXYKpff/1VDRo0KDDm7e3t/Pd+/frp8ccf18CBAwvtW69ePVWrVk1ffPGFOnToUOy1AgAAAMDlJCQkuGTfG4XgW0xsNpscDsclt3t4eOiFF15QVFSUhg4dWmh7eHi4wsLCFBQUVJxlAgAAAMAVNWvW7Lr2S0hIuO59r0VWVpYSExMvuZ1HnYvJXXfdpd27dxcYy87O1t69e52fH374Ye3du1cHDx4stL+/v79atmyptWvXFnepAAAAAGA0gm8xadOmjY4ePaovvvhCkpSXl6e//vWv2rRpU4F5o0eP1uzZs4s8RmhoqD766CNlZWUVe70AAAAAYCqCbzFxc3NTdHS0Vq9erT59+mjgwIHy9fXVyJEjC8xr2bKlbr/99iKPceutt6pnz546depUSZQMAAAAAEayWZZluboI/DH5z7P3XP9fHbPnuLocAAAAAIZxzAq+rv1K+h3fwMBAlSlTptB2VnwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwmoerC8CNs39Cb5UpU8bVZeAmlpCQoGbNmrm6DNzk6EOUBvQhSgP6EPgfVnwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMJqHqwvAjVNn2lods+e4ugzc7Jb/7OoKAPoQpQN9iNKAPsQN5JgV7OoSrhsrvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIJvMdu4caMaNWqk06dPS5KioqLUt29fWZblnBMcHCxJio+PV9OmTXXy5EnntqioKMXHx5ds0QAAAABgEIJvMdu4caNq1KihLVu2OMeys7P1ySefFDm/evXqeuedd0qqPAAAAAAwHsG3GJ09e1Y//vijIiIi9M9//tM5PmzYMM2fP185OTmF9unSpYv+85//6MCBAyVZKgAAAAAYy8PVBZhs8+bNat++vR588EFNnDhRJ06ckCRVrFhRnTp10sqVK52POV9s9OjRmj17tqKiokq6ZAAAAAAoUkJCQonudyMRfIvRxo0bNXz4cLm7u6tbt27atGmTc9tzzz2n/v37q3fv3oX2a9mypRYtWqSdO3eWZLkAAAAAcEnNmjW75n0SEhKua79rlZWVpcTExEtuJ/gWk+PHj2vXrl2aMWOGbDabMjMz5evrq6CgIEmSj4+P+vfvr+jo6CL3f+mllzR16lTdf//9JVk2AAAAABiHd3yLycaNG/XUU0/p448/1vr167V582alpqbq8OHDzjlPPvmkvvjiC506darQ/vXr11e1atX05ZdflmTZAAAAAGAcgm8x+ec//6k+ffo4P9tsNvXq1avA486enp4KDQ3Vr7/+WuQxwsPDtW/fvmKvFQAAAABMZrMu/oOy+FPKf5695/r/6pi98DdFAwAAAMAf5ZhV+It5r6Sk3/ENDAxUmTJlCm1nxRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjebi6ANw4+yf0VpkyZVxdBm5iCQkJatasmavLwE2OPkRpQB+iNKAPgf9hxRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNE8XF0A/jjLsiRJ2dnZLq4EkLKyslxdAkAfolSgD1Ea0IcoDUqiD/OzUH42+j2bdakt+NNIS0vT3r17XV0GAAAAALhUvXr15OvrW2ic4GuAvLw82e12eXp6ymazubocAAAAAChRlmUpJydHPj4+cnMr/EYvwRcAAAAAYDS+3AoAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKPxd3wN8MYbb2jXrl2y2WwaP368Gjdu7OqSYLC33npLCQkJys3N1QsvvKB77rlHY8eOlcPhUKVKlfTXv/5VXl5e+vjjj/XRRx/Jzc1NTz75pJ544glXlw7DZGZmqnv37ho+fLhatWpFH6LEffzxx1q4cKE8PDw0cuRI1a9fnz5EibLb7Ro3bpxSU1OVk5OjsLAwVapUSVOmTJEk1a9fX6+99pokaeHChdq8ebNsNptefPFFBQUFubBymGLv3r0aPny4nnnmGQ0aNEjHjh276t+DOTk5ioiIUHJystzd3TV9+nTVqFGj+Iq18KcWHx9vDR061LIsy9q3b5/15JNPurgimCwuLs4KCQmxLMuyTp8+bQUFBVkRERHWpk2bLMuyrFmzZlnLli2z7Ha71aVLF+vcuXNWRkaG9eijj1pnzpxxZekw0OzZs60+ffpYMTEx9CFK3OnTp60uXbpYaWlp1okTJ6yJEyfShyhxS5YssWbOnGlZlmUdP37c6tq1qzVo0CBr165dlmVZ1ksvvWRt27bNOnz4sNW7d28rKyvLSklJsbp27Wrl5ua6snQYwG63W4MGDbImTpxoLVmyxLIs65p+D8bGxlpTpkyxLMuy/vWvf1nh4eHFWi+POv/JxcXFqVOnTpKkOnXqKDU1Venp6S6uCqZq0aKF5s6dK0ny8/NTRkaG4uPj1bFjR0nSQw89pLi4OO3atUv33HOPfH195e3trfvuu087duxwZekwzP79+7Vv3z61b99ekuhDlLi4uDi1atVK5cqVU+XKlfWXv/yFPkSJK1++vM6ePStJOnfunG677TYdPXrU+fRffh/Gx8frwQcflJeXlypUqKBq1app3759riwdBvDy8tKCBQtUuXJl59i1/B6Mi4tT586dJUmtW7cu9t+NBN8/uVOnTql8+fLOzxUqVNDJkyddWBFM5u7urrJly0qS1qxZo3bt2ikjI0NeXl6SpIoVK+rkyZM6deqUKlSo4NyPvsSN9uabbyoiIsL5mT5ESUtKSlJmZqZCQ0M1cOBAxcXF0YcocY8++qiSk5PVuXNnDRo0SGPHjpWfn59zO32I4uTh4SFvb+8CY9fye/DicTc3N9lsNmVnZxdfvcV2ZLiEZVmuLgE3gc8++0xr1qzRokWL1KVLF+f4pfqPvsSNtG7dOjVp0uSS7wHRhygpZ8+e1TvvvKPk5GQNHjy4QI/RhygJ69evV9WqVRUdHa09e/YoLCxMvr6+zu30IVzpWvuvuPuS4PsnV7lyZZ06dcr5+bffflOlSpVcWBFM969//UvvvfeeFi5cKF9fX5UtW1aZmZny9vbWiRMnVLly5SL7skmTJi6sGibZtm2bjhw5om3btun48ePy8vKiD1HiKlasqKZNm8rDw0N33nmnfHx85O7uTh+iRO3YsUNt27aVJAUEBCgrK0u5ubnO7Rf34YEDBwqNAzfatfzvceXKlXXy5EkFBAQoJydHlmU5V4uLA486/8m1adNGW7ZskST99NNPqly5ssqVK+fiqmCqtLQ0vfXWW5o/f75uu+02SRfeycjvwa1bt+rBBx/Uvffeq927d+vcuXOy2+3asWOHmjdv7srSYZA5c+YoJiZGq1ev1hNPPKHhw4fThyhxbdu21fbt25WXl6czZ87o/Pnz9CFKXM2aNbVr1y5J0tGjR+Xj46M6dero+++/l/S/PnzggQe0bds2ZWdn68SJE/rtt9909913u7J0GOpafg+2adNGmzdvliR9+eWXatmyZbHWZrN41uFPb+bMmfr+++9ls9k0efJkBQQEuLokGGrVqlWKiopS7dq1nWMzZszQxIkTlZWVpapVq2r69Ony9PTU5s2bFR0dLZvNpkGDBumxxx5zYeUwVVRUlKpVq6a2bdtq3Lhx9CFK1MqVK7VmzRpJ0rBhw3TPPffQhyhRdrtd48ePV0pKinJzcxUeHq5KlSpp0qRJysvL07333qtXX31VkrRkyRJt2LBBNptNo0aNUqtWrVxcPf7sEhMT9eabb+ro0aPy8PDQHXfcoZkzZyoiIuKqfg86HA5NnDhRBw8elJeXl2bMmCF/f/9iq5fgCwAAAAAwGo86AwAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoHq4uAACA4vLWW29p9+7dysrK0s8//6ymTZtKkvr27St/f3/NmTNHK1asUGxsrF599VVt2rRJderUce7/7rvvau7cufrPf/6j+Ph4DR8+XA0bNixwjr59+6pXr17Oz2PHjtWxY8d07tw5JSUlOeeHhoaqTQw9Z8MAAAknSURBVJs2+uSTTzRq1CitWrVKTZo0ce5Xv359/fTTT/Lw8FBWVpZCQkL08MMPa+DAgQoODlZqaqpuvfVW53w3Nzd99NFHioqK0vLly7Vlyxb5+flJkuLj47V27VrNmDGjQK2ZmZmaOnWq9u/fLw8PD9ntdoWEhOiRRx654vXZ7XbNnj1b33//vcqWLav09HR17NhRL774ojw8PG7IPaxfv75GjRqlYcOGObcFBwdr+vTpWr58+SV/lnl5eZo5c2aB8+bf85o1a6pbt27O+Tk5OWrevLnCwsJ0yy23FJiflJSkgQMH6uuvv1Z8fLwGDx6sBQsWqF27ds4569ev19ixY/X555+revXqkqTffvtN7du316hRozR06FDnXMuytHjxYq1fv15eXl7KyMhQ3bp1NX78eFWoUEFJSUkFassXFBSkkJAQHTp0SNOmTVNGRoYcDodsNpsiIyP5s4UAcB0IvgAAY40dO1bS/wLNkiVLnNvi4+MLzK1Vq5ZiYmKc+0jS1q1bVblyZefnevXqFThGUd566y3n8efMmVNo/po1a1SvXj3FxsYWCL75HA6HxowZo9atW2vgwIHO8YiICLVu3brIc1apUkVz585VZGTkZWv74IMP5O3trRUrVkiSjh07pqFDhyooKOiK1/fqq6+qVq1aWrdunWw2m9LT0zVs2DAtXrxYzz33nKQ/fg8rVqyodevWqVevXoX+luPlfpaxsbFq3bq1Zs6cWeiYSUlJqlChgnN+VlaWZsyYoTFjxujdd9+97P3Kv56Lg++6detUq1atAvPWrVunOnXqKDY2tkDwXb58ub766istXrxY5cqVcwb08ePH67333pOkArX93pQpUzRw4EB17txZkvTZZ59p3rx5ioqKumzdAIDCeNQZAABJ7dq10+bNm+VwOCRJ33//vWrWrClPT88bdo5jx47phx9+0IwZM7Rp0yZlZmYWmjNlyhRVr169wKrnlQwcOFDfffed9uzZc9l5qampstvtsixLkuTv768NGzbIx8fnsvsdPHhQu3fvVnh4uGw2mySpXLlyio6OdoZe6Y/fQ29vb40YMaLQSvWNVKZMGY0fP1579uzRvn37Ljv33nvvVWJios6ePStJSk5Olt1uLxDkJSkmJkbjx49XRkaGduzY4RyfP3++Jk2apHLlykm6sEr/8ssva968eVdVa2pqqtLT052fO3XqROgFgOtE8AUAQJKfn58CAwP11VdfSbqwitijR48beo6YmBh16dJFjRo10t13362tW7cW2D5nzhxt3769wIrp1XB3d9err76qadOmXXbe4MGDlZiYqI4dO2rChAn65JNPlJ2dfcXj79u3TwEBAXJ3dy8w7uXlVeDzjbiH3bt3V0pKiuLi4q5pv2vh6empwMBA7d2797Lz3Nzc1KVLF23YsEGStHbtWj3yyCMF5nz33XfKzc3VAw88oF69eik2NlaSlJaWJrvdXmh12M3NrdB9vJQxY8bozTffVO/evfXmm2/q3//+91VeIQDg9wi+AAD8v549eyo2NlYZGRnavn17gUdcJWnv3r0KDg4u8E9ycvJVHduyLMXGxqpv376SLrybmh+S8jkcDtWqVUvz588vtP+MGTMKnPedd94psL1Vq1aqUKGCM6QVpWrVqvr44481Z84c1axZU4sWLVKPHj2cq4qXuj53d3fnKq4kff755woODtaAAQPUr1+/Aue4Efdw4sSJmj59unJzcy95Lb/37bffFjru5UJ9Wlqa3Nyu/H+D8q9HkjZs2KDu3bsX2L5mzRr17t1bNptNffr00SeffKKMjAzZbDbl5eU55yUnJzvr6ty5s44ePSpJOn36dKG6f/zxR0lSmzZt9PXXX2v06NFyd3dXRESEXnrppau+JwCA/+EdXwAA/l+7du00efJkxcbGqm3btoVWNK/mHd9LiYuL08mTJzV16lRJF0LuwYMHdfToUVWrVk3ShRU+u92uJ554Qg0bNnS+eytd/h3fi+c888wzioiIKHJ7ZmamypQpo8aNG6tx48YaMmSIBg4cqG+//Va33nrrJa8vLy9Pv/zyi7Kzs+Xl5aWOHTuqY8eOzvdtL3Yj7mFAQIBatGihpUuXXnbexS71jm9RMjIy9Msvv6hRo0ZXnBsQECCHw6HVq1erWrVquv32253b0tPTtXXrVvn7++vTTz+VdOFebdmyRb169VKFChW0Z88eBQQEqGrVqs7r7tChg3Jzc+Xu7n7Zd3wzMjJ0yy23qF27dmrXrp1CQ0PVunVrnT17VrfddttVXSsA4AJWfAEA+H+enp7q0qWL3n77bT322GM39Nhr1qxReHi41q9fr/Xr12vjxo3q3bu31q5dW2Derbfeqrlz52rChAk6cODANZ3D399fvXr1cn5x0u89/fTTWrdunfOz3W7XmTNnVKNGjcset3r16goKCtIbb7zhXPm1LEtfffWVvL29C8y9UfcwPDxcy5YtU0pKynUfoyg5OTmaOnWq2rRpc8XrztezZ0/NmjWr0GPbGzduVIsWLbRp0ybnz/X11193rhCHh4drypQpOnPmjHOfH374QefOnSv0HwR+LzU1Ve3bt9f+/fudY8ePH1e5cuXk6+t7tZcLAPh/rPgCAHCRnj17atu2bWrWrFmhbfmP6V6sSZMmGjNmzGWPefbsWX399deaOHFigfEBAwZoxIgRCgsLKzDeoEEDvfzyywoLC9Pq1aslXXjU+eI/ZyRJr732WqFzPf/884qNjVXt2rULbZs1a5amTZumVatWycvLS1lZWRo6dKgaNGig+Pj4y17fa6+9pvnz56tPnz4qV66cMjIyVL9+fS1atKjQeW7EPfTz89PQoUML3bNLyX/U+WKdOnVSx44dnY8TOxwOnTt3Tm3atNGkSZOu6rjShfeO582b5/x25Xxr1qwp9LPr2rWrZsyYoaSkJD322GPy9vbWkCFD5OHhIYfDofLly+u9996Tv7+/kpKSnLVdrHr16po+fbrmzJmjyMhIubm5OR/Lnjdv3lW/IwwA+B+blf/VjgAAAAAAGIhHnQEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAo/0fhiX9+899YQ4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (15, 5))\n",
        "plt.barh(names, time_taken_unsegmented)\n",
        "plt.xlabel(\"TIME TAKEN UNSEGMENTED IMAGES\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "s_DWpcjI3920",
        "outputId": "7458d6ed-2ab3-46a5-b7a3-95c12e68375d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA74AAAE9CAYAAAAsxb5eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3SUZd7G8WvSISQshqp0pQSyLBoBKQuKkaJgIMDSDEWiBkIRQQglYAMjkghEsSCIBoUACXVZUV9Q98WQXcMixLJRegkYQJCENMK8f3AyL2MKGRCH3Hw/53iOz30/5TczvyNePPczY7FarVYBAAAAAGAoF2cXAAAAAADAjUTwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaG7OLgDX79KlS8rOzpa7u7ssFouzywEAAACAP5TValVBQYG8vb3l4lL8/i7B1wDZ2dlKT093dhkAAAAA4FRNmzaVj49PsXGCrwHc3d0lXf6QPTw8nFwNKoK0tDQFBAQ4uwxUEPQLHEG/wFH0DBxBv6A0+fn5Sk9Pt2Wj3yL4GqBoebOHh4c8PT2dXA0qCnoFjqBf4Aj6BY6iZ+AI+gVlKe3RT77cCgAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0N2cXgN/PnXPWKSO7wLZdGBPqxGoAAAAA4ObAHV8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMJrb1XY4evSoevfurYCAAFmtVrm6uio8PFzt27cv90WSkpLk4+Ojhx56qNjc999/r08//VTjx48v9/l27Niht956S5K0a9cu3XPPPZKkZ599Vq1atSr3eX5r/fr1+uCDD+Th4aGLFy8qLCxMPXr0UFxcnKpVq6bHHnvsms8tSXPmzNGwYcPk7e2t0NBQde3aVT4+PmrTpo3uvvvu6zo3AAAAAKBkVw2+ktSoUSPFx8dLkg4fPqzw8HDFxsaqefPm5bpISEhIqXP+/v7y9/cv13mKdOzYUR07dpQktWvXzlbb9UhNTdWHH36o5cuXy9fXV6dPn9agQYPUtGnT6z53kRkzZkiS/v3vf6tBgwaaNGnS73ZuAAAAAEDJyhV8r1S/fn2Fh4fro48+UrNmzbRp0ya5uLgoKChIjz/+uH799VdNnjxZWVlZ8vHxUWxsrJYtW6Zq1aopODhYTz/9tPLz85Wfn69Zs2YpKytLH374oRYtWqQtW7Zo+fLlcnV1VcuWLTVz5kzFxcXp/PnzOnDggA4fPqzp06erS5cuJdaWkpKiZcuW6cKFC5o6daqOHz+uZcuWyc3NTQEBAYqMjFRhYaGioqJ05MgRXbx4UePHj1f79u21YsUKjR07Vr6+vpIkPz8/JSYm2rYl6eLFi5o6dapOnjypCxcuaNy4cXrggQe0fv16rVixQu7u7mrevLlmz55d4lhoaKiioqL08ssv6/jx44qJiVFmZqa6d++uzp07l1hXaGiomjRpIkmaNWvWtXzGAAAAAHBLczj4SlJAQIBiYmJ04MABrVy5UpI0ePBg9ejRQwkJCerUqZOGDRum5cuXKzk52XZccnKyatWqpblz5+rIkSM6cOCAPD09JUnZ2dl67bXXtH79enl7eys8PFw7d+6UJJ04cUJLlizRl19+qVWrVpUafCUpPT1dW7duVUFBgaKiopSQkCAPDw9NmDBBqampOnLkiGrUqKG5c+fqzJkzGj58uDZt2qT9+/cXu4N9ZeiVpHPnzqlTp07q27evjhw5ogkTJuiBBx7Q0qVL9c4776hOnTpKTExUbm5uiWNFpk6dqg8//FCTJk1SZGSkJGnTpk0l1iVJTZo00eDBgx3+nFJTUx0+BrcO+gOOoF/gCPoFjqJn4Aj6BdfimoJvdna2KleurEOHDmnYsGG2sWPHjum7777ThAkTJEkjRoyQdPk5Xklq3bq1FixYoFmzZqlbt27q3LmzUlJSJEkHDx5UgwYN5O3tLUlq27at7biiZ3hr166t8+fPl1lbs2bN5OHhoe+//17Hjx/XqFGjJEnnz5/X8ePH9Z///EepqanatWuXJCkvL0/5+fmyWCy6dOlSmef29fXV3r17lZCQIBcXF509e1aS1KtXL0VEROjRRx9Vr1695OXlVeJYWUqrS9I1P7ccGBh4TcfBfKmpqfQHyo1+gSPoFziKnoEj6BeUJi8vT2lpaaXOX1PwTUtLU15enu6//3698MILdnNLly4tNUDWrFlTGzZsUEpKilauXKndu3erTZs2kiSLxSKr1Wrbt6CgwHY32M2t/GV6eHhIktzd3RUQEKClS5fazX/zzTcKDw9Xr1697MYbN26sPXv2qE6dOraxffv2qXbt2rbtzZs369y5c/roo4909uxZ9e/fX5L01FNPqXfv3tq6dauGDx+uFStWlDhWFnd39xLrKpoDAAAAAFwbh3/O6PDhw1q+fLlWrFihlJQU5eTkyGq16qWXXlJubq4CAgJsS5RXrVqldevW2Y796quv9NVXX6lTp06KioqyS+QNGzbUoUOHlJWVJUn617/+pYCAgGt+YY0aNdK+fft0+vRpSdKiRYt08uRJ/eUvf9H//M//SJJOnz6t2NhYSdKwYcP0+uuv2/bPzMzU008/rYyMDNs5f/nlF9WtW1cuLi769NNPlZ+fr0uXLum1115TjRo1NHLkSLVu3VrHjx8vcawspdUFAAAAALg+5bqVeuDAAYWGhio/P1+FhYWaNWuWbr/9dg0bNkxDhw6Vq6urgoKC5OXlpeHDh2vKlCkKDQ2Vt7e35s+fr/fee0/S5S/GevbZZ/Xuu+/KYrFo/PjxKiwslCRVrlxZU6ZMUVhYmFxcXBQYGKh7773X7hlhR1SqVEnTp0/XE088IQ8PD7Vo0UI1a9ZUz549tXPnTg0aNEiFhYUaO3aspMvLsCdOnKhRo0apUqVKcnNz04wZM3TXXXfZztmtWzeNHj1au3fvVr9+/VS7dm0tXrxY3t7eGjhwoHx8fFSvXj35+/trx44dxcbKUlpdAAAAAIDrY7Feub4YFVLRevbgDT8qI7vANl4YE+rEqnAz4/kYOIJ+gSPoFziKnoEj6BeUpigTBQQE2B6ZvZLDS50BAAAAAKhICL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjObm7ALw+9k3o688PT2dXQYAAAAA3FS44wsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGhuzi7gRjh06JBefvllnT59WpJ0++23a/bs2brtttt+92u98847atOmje6+++5yH3P06FE99NBDWrdunZo3by5JSkpKkiSFhISoa9euql27tlxdXXXhwgX1799fgwcP/t1rBwAAAIBbgXF3fAsLCzVu3DiFhYVpzZo1WrNmjVq2bKk5c+bckOs9+eSTDoXeInfddZdiYmJKnV+yZIni4+MVHx+vuLg4FRYWXk+ZAAAAAHDLMu6O744dO9SkSRPde++9trGwsDBZrVb98MMPev755+Xm5iYXFxctXLhQWVlZGj9+vN0d10WLFungwYNasGCBvLy85Ofnp/nz5yslJaXYWFRUlLp37642bdpo0qRJunDhgnJzcxUVFaVWrVrpoYce0sCBA7V9+3bl5+frvffekyS1bNlSOTk5Sk5OVvv27Ut9PefOnVO1atXk6up6Y984AAAAADCUccF3//79atasmd2Yi8vlG9unT59WVFSUWrRooYULF2rTpk164IEHSjzPihUrFBkZqXvvvVeffPKJzp49W+JYkczMTA0YMEBBQUFKTk7WkiVLbHdqGzdurLCwME2cOFE7d+60LW+eOHGipk6dqvvuu6/Y9Z944glZLBbt27dPUVFR5XrtaWlp5doPkKTU1FRnl4AKhH6BI+gXOIqegSPoF1wL44Kvi4uLLl68aNsePXq0srKydOLECcXFxWn+/PnKzc3Vzz//rN69e5d6nh49emj27Nnq3bu3HnnkEdWoUaPEsSLVq1fX4sWLtXTpUuXn56ty5cq2uaK7z7Vr19b58+dt4w0bNlSLFi20ZcuWYtdfsmSJvL29lZWVpREjRqh58+a68847y3ztAQEB8vT0vPqbhFteamqqAgMDnV0GKgj6BY6gX+AoegaOoF9Qmry8vDJvBBr3jG+TJk20d+9e2/abb76p+Ph4FRYWas6cORo2bJhWrFihgQMHSpIsFovd8UWhuU+fPvrggw9UrVo1jR49Wvv27StxrMj777+vWrVqaeXKlXruuefsznnlMmWr1Wo3FxERoXfeeccurF+pSpUqatu2rXbv3u34mwEAAAAAMC/43nfffTpx4oS2bdtmG/v222+VnZ2tkydPqn79+srPz9cXX3yhgoICValSRadPn5bValVmZqaOHDkiSXrjjTfk5uamgQMH6uGHH9a+fftKHCvyyy+/qH79+pKkzz77TAUFBeWqt3r16goKCtKqVatKnLdardq7d68aNWp0rW8JAAAAANzSjFvqbLFY9O677+qFF17QG2+8IXd3d1WuXFlvvvmmfvzxR0VERKhevXoKDQ3VCy+8oIcfflgdOnRQv3791Lx5c/n7+0u6/BNII0eOlK+vr3x9fTVy5EhlZ2cXGysK2MHBwZo6dao+/vhjDR06VJs3b1ZiYmK5an788ce1cuVKu7EnnnhCrq6uys3NVZcuXXTPPff8vm8UAAAAANwiLNbfrr1FhVO0np1nfFFePB8DR9AvcAT9AkfRM3AE/YLSXC0TGbfUGQAAAACAKxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjWaxWq9XZReD65OXlKS0tTcEbflRGdoGzywEAAABgoMKYUGeXUKqiTBQQECBPT89i89zxBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXxvoIMHD+rJJ59U//79FRISohdffFH5+fnq2rWr4uPjbfsdPXpUkZGRkqTIyEiNGzfO7jyhoaF/aN0AAAAAYBKC7w1SWFiocePGKSwsTGvXrlViYqIk6Y033pCfn59Wr16trKysEo89dOiQdu/e/UeWCwAAAADGIvjeIDt27FDjxo3Vtm1bSZLFYtGzzz6riIgIeXl5adCgQVq6dGmJxz799NOKiYn5I8sFAAAAAGO5ObsAU+3fv1/+/v52Y15eXrZ/HzhwoPr3768hQ4YUO7Zp06a64447tG3bNnXt2vWG1woAAAAAV5OamursEq4ZwfcGsVgsKiwsLHXezc1NTz31lOLi4vTkk08Wm58wYYIiIiLUpUuXG1kmAAAAAJRLYGCgs0soVV5entLS0kqdZ6nzDdK4cWPt3bvXbiw/P1/p6em27Z49eyo9PV0HDx4sdnydOnXUrl07rVu37kaXCgAAAABGI/jeIB07dtSxY8e0bds2SdKlS5f06quvasuWLXb7TZw4UbGxsSWeIzw8XO+//77y8vJueL0AAAAAYCqC7w3i4uKipUuXavXq1QoJCdGQIUPk4+Oj8ePH2+3Xrl07Va9evcRzVK1aVcHBwTp16tQfUTIAAAAAGMlitVqtzi4C16doPXvwhh+VkV3g7HIAAAAAGKgwJtTZJZSqKBMFBATI09Oz2Dx3fAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADCam7MLwO9n34y+8vT0dHYZqABSU1MVGBjo7DJQQdAvcAT9AkfRM3AE/YJrxR1fAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIzm5uwC8Pu5c846ZWQXOLsMVBQffefsClCR0C8oQ2FMqLNLAACgTNzxBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXxvsM2bN6tly5Y6c+aMJCkuLk79+vWT1Wq17RMaGipJSklJ0d13363MzEzbXFxcnFJSUv7YogEAAADAIATfG2zz5s2qV6+etm7dahvLz8/XP/7xjxL3r1u3rl5//fU/qjwAAAAAMB7B9wY6e/as9uzZo8jISP3973+3jY8ePVpvv/22CgoKih3TrVs3/fe//9WBAwf+yFIBAAAAwFhuzi7AZB9//LHuv/9+/fWvf9XMmTN18uRJSZKfn5+CgoK0atUq2zLnK02cOFGxsbGKi4v7o0sGAMBhqampZW4DV0PPwBH0C64FwfcG2rx5s8aMGSNXV1f16NFDW7Zssc09/vjjGjRokPr27VvsuHbt2mnZsmXavXv3H1kuAADXJDAw0PbvqampdtvA1dAzcAT9gtLk5eUpLS2t1HmC7w1y4sQJffPNN4qOjpbFYlFubq58fHzUpUsXSZK3t7cGDRqkpUuXlnj8M888o5deeklt27b9I8sGAAAAAOPwjO8NsnnzZg0dOlQbN27Uhg0b9PHHH+vcuXM6fPiwbZ+//e1v2rZtm06dOlXs+GbNmumOO+7Q9u3b/8iyAQAAAMA4BN8b5O9//7tCQkJs2xaLRX369LFb7uzu7q7w8HDt37+/xHNMmDBBP/300w2vFQAAAABMZrFe+YOyqJCK1rMHb/hRGdnFvykaAIAbqTDm/7+okefv4Ch6Bo6gX1CaokwUEBAgT0/PYvPc8QUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjEbwBQAAAAAYjeALAAAAADAawRcAAAAAYDSCLwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgNIIvAAAAAMBobs4uAL+ffTP6ytPT09lloAJITU1VYGCgs8tABUG/AACAio47vgAAAAAAoxF8AQAAAABGI/gCAAAAAIxG8AUAAAAAGI3gCwAAAAAwGsEXAAAAAGA0gi8AAAAAwGgEXwAAAACA0Qi+AAAAAACjEXwBAAAAAEYj+AIAAAAAjObm7AJw/axWqyQpPz/fyZWgIsnLy3N2CahA6Bc4gn6Bo+gZOIJ+QUmKslBRNvoti7W0GVQY58+fV3p6urPLAAAAAACnatq0qXx8fIqNE3wNcOnSJWVnZ8vd3V0Wi8XZ5QAAAADAH8pqtaqgoEDe3t5ycSn+RC/BFwAAAABgNL7cCgAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAo/E7vgaYO3euvvnmG1ksFk2fPl2tWrVydkm4yaSnp2vMmDEaMWKEHnvsMWVkZGjKlCkqLCxUjRo19Oqrr8rDw8PZZeImMW/ePKWmpurixYt66qmn9Oc//5l+QYlycnIUGRmp06dPKy8vT2PGjFHz5s3pF5QpNzdXvXr10pgxY9S+fXv6BSVKSUnRhAkT1KRJE0mXf6ImLCyMfsE1445vBfevf/1Lhw4dUkJCgubMmaM5c+Y4uyTcZC5cuKAXX3xR7du3t40tWrRIQ4YM0UcffaQGDRpo7dq1TqwQN5OdO3fqxx9/VEJCgt59913NnTuXfkGptm/froCAAK1YsUILFixQdHQ0/YKrevPNN1W1alVJ/HmEsrVt21bx8fGKj49XVFQU/YLrQvCt4JKTkxUUFCRJuvPOO3Xu3DllZWU5uSrcTDw8PLRkyRLVrFnTNpaSkqIHH3xQkvTAAw8oOTnZWeXhJtOmTRstXLhQkuTr66ucnBz6BaV6+OGH9cQTT0iSMjIyVKtWLfoFZdq3b59++ukn3X///ZL48wiOoV9wPQi+FdypU6dUrVo12/Ztt92mzMxMJ1aEm42bm5u8vLzsxnJycmxLg/z8/OgZ2Li6uqpy5cqSpLVr16pz5870C65q0KBBmjx5sqZPn06/oEyvvPKKIiMjbdv0C8ry008/KTw8XIMHD9aOHTvoF1wXnvE1jNVqdXYJqGDoGZTks88+09q1a7Vs2TJ169bNNk6/oCSrVq3S999/r2effdauR+gXXGn9+vVq3bq16tWrV+I8/YIrNWzYUGPHjlXPnj115MgRDRs2TIWFhbZ5+gWOIvhWcDVr1tSpU6ds2z///LNq1KjhxIpQEVSuXFm5ubny8vLSyZMn7ZZBA//85z/11ltv6d1335WPjw/9glKlpaXJz89PderUkb+/vwoLC+Xt7U2/oESff/65jhw5os8//1wnTpyQh4cH/31BqWrVqqWHH35YklS/fn1Vr15de/fupV9wzVjqXMF17NhRW7dulSR9++23qlmzpqpUqeLkqnCz69Chg61vPvnkE/31r391ckW4WZw/f17z5s3T22+/rT/96U+S6BeU7uuvv9ayZcskXX705sKFC/QLSrVgwQIlJiZq9erVGjBggMaMGUO/oFQbN27U0qVLJUmZmZk6ffq0QkJC6BdcM4uVdQIV3vz58/X111/LYrFo9uzZat68ubNLwk0kLS1Nr7zyio4dOyY3NzfVqlVL8+fPV2RkpPLy8nT77bfr5Zdflru7u7NLxU0gISFBcXFxatSokW0sOjpaM2fOpF9QTG5urmbMmKGMjAzl5uZq7NixCggI0NSpU+kXlCkuLk533HGHOnXqRL+gRFlZWZo8ebJ+/fVXFRQUaOzYsfL396dfcM0IvgAAAAAAo7HUGQAAAABgNIIvAAAAAMBoBF8AAAAAgNEIvgAAAAAAoxF8AQAAAABGc3N2AQAAXKt58+Zp7969ysvL03fffae7775bktSvXz/VqVNHCxYs0MqVK5WUlKRp06Zpy5YtuvPOO23HL168WAsXLtR///tfpaSkaMyYMWrRooXdNfr166c+ffrYtqdMmaKMjAz9+uuvOnr0qG3/8PBwdezYUf/4xz/09NNPKyEhQa1bt7Yd16xZM3377bdyc3NTXl6ewsLC1LNnTw0ZMkShoaE6d+6cqlatatvfxcVF77//vuLi4vTRRx9p69at8vX1lSSlpKRo3bp1io6Otqs1MjJSgYGBGjBggG1szZo1Sk1NVXR0tCIjI7Vnzx5t2LDB9hMgSUlJOnbsmMaNG6czZ87oueee0+nTp2WxWJSXl6dnnnlG7du3V1JSkubPn2/3/l35ujMzMzV//nz98MMP8vb2VnZ2tkJCQjR8+HBJl3++ZsmSJfrf//1f2+uQpBkzZig5OVnbtm0r8xoNGjTQgw8+qFdffVWPPvqoba5r167atm1bmZ/Lrl27lJSUpLp169qdd+bMmTp79qztc7darbp48aK6du2qUaNGydXV1W7/lJQUh3qqyJ49ezRgwADFxsbqkUcesY0XFBRo8eLF2r59uypVqqSsrCy1adNGkydPVuXKla/ak3v27FFMTIwuXbqkgoICeXt768UXX9Ttt98uAIA9gi8AoMKaMmWKJOno0aMaMmSI4uPjbXMpKSl2+zZs2FCJiYm2YyTpk08+Uc2aNW3bTZs2tTtHSebNm2c7/4IFC4rtv3btWjVt2prpvLwAAAgZSURBVFRJSUl2wbdIYWGhJk2apA4dOmjIkCG28cjISHXo0KHEa9auXVsLFy5UVFRUmbWVh6enp+Lj4/X4448Xm4uNjdU999yjESNGSLr8O+Avvvii7rvvPklShw4dNH/+/GLHWa1WjRkzRiEhIXrllVckSadOndKIESNUu3Ztde/eXZJUp04dbdq0SUOHDpUk5eTk6IcffrA7V2nXOHr0qBo2bKg33nhDXbt2VZUqVezmy/pcdu3apUcffVQTJ04sdt6UlBS7z/38+fOaNm2aoqOjNWPGjBLewf9Xnp6S7HviyuAbGxurM2fOaPXq1fLw8FB+fr6mTZummJgY22ddVk9OnjxZCxYssAXj+Ph4LV++XNOnTy+zbgC4FbHUGQBwS+jcubM+/vhjFRYWSpK+/vprNWjQwHbn8/eQkZGh//znP4qOjtaWLVuUm5tbbJ/nnntOdevW1ejRo8t93iFDhujf//53sZB4LSIiIrRq1SplZmYWmzt37pyysrJs2wEBAUpISJDFYinznMnJyXJ1ddXgwYNtY9WrV1dSUpIt9ErSQw89pKSkJNv21q1b1a5du3LXXrNmTYWEhGjx4sXlPsZRPj4+mjt3rtavX6/z58+XuW95eionJ0dbtmzRvHnztGvXLp04cUKSdOHCBa1du1YzZ86Uh4eHJMnDw0PR0dGaOXNmuWr97ecVGhpK6AWAUhB8AQC3BF9fXwUEBOiLL76QdHmJb+/evX/XayQmJqpbt25q2bKl7rrrLn3yySd28wsWLNDOnTvt7hCWh6urq6ZNm6Y5c+Zcd40+Pj568skn9eqrrxabGzNmjBITE9WzZ0+98MIL+uKLL3Tp0qWrnvPHH39UQEBAsfGiQFekcePGkqT09HRJ0rp16xz+DEaOHKkvvvhC+/fvd+g4R/j6+qpevXpXvUZ5emrr1q3y9/eXv7+/unbtqnXr1kmSDh8+rDp16sjHx8duf3d396v+RUORadOmafTo0Ro0aJBee+01ffvtt+V9iQBwyyH4AgBuGcHBwUpKSlJOTo527typzp07282np6crNDTU7p/jx4+X69xWq1VJSUnq16+fpMvPYV55d1O6vMy5YcOGevvtt4sdHx0dbXfd119/3W6+ffv2uu2227Rp0yZHXrIkFQtS/fr106FDh7Rr1y67cX9/f3322Wd6/vnnVa1aNc2bN09Dhw613dH86quvir0/+fn5cnV1te0jSQkJCQoNDdXf/vY3jR8/3u4awcHBSkxM1PHjx3XmzBn5+/vbzZd2jSIeHh6aMmWKw38JsHHjRrtzjh07tsz9s7Ky5OJy9f9NulpPrV271tYT/fv3twVfFxcXu/dsz549ttq6du1qmyurJ/v06aMvv/xSYWFhys7OVlhYmGJiYsr/pgDALYRnfAEAt4zOnTtr9uzZSkpKUqdOnYrdkSzPM76lSU5OVmZmpl566SVJl0PuwYMHdezYMd1xxx2SpEmTJik7O1sDBgxQixYt1KVLF9vxZT3je+U+I0aMUGRkZInz3t7eOnv2rN3YqVOnij0Pa7FYNGPGDD333HN2zxnn5OSoUqVKatu2rdq2bavw8HB1797dtsS6tOdvmzVrpsTERNv2wIEDNXDgQNvztld65JFH1LdvX1WtWlW9evUqdq7SrnGlLl26aOXKlfr000/L3O9KpT3jW5KTJ0/q1KlTuuuuu666b1k9dfDgQe3evVvnzp3Te++9J6vVqoyMDH399df685//rMzMTJ05c0a33XabWrVqZeu9Zs2ayWq1Siq7J3NycuTt7a2goCAFBQXpscceU0hIiCZNmlSu1wkAtxLu+AIAbhnu7u7q1q2bFi1aZPfNwL+HtWvXasKECdqwYYM2bNigzZs3q2/fvrY7fEWqVq2qhQsXasaMGTpw4IBD16hTp4769Omjt956q8T5bt26acuWLbbnPs+ePavNmzfbfaFSkVatWqlFixZas2aNpMtBvWfPnnZfCvbLL78oPz9ftWvXLrOuNm3a6E9/+pPdneyCggLt2LFDXl5edvv6+fnJ399fH3zwwXUtNZ8+fbpiYmLs7gb/HrKysjRr1iw99thjqlSp0lX3L6unEhMTNWDAAG3atEkbNmzQxo0bFRERocTERHl6emrkyJGKiopSTk6O7Zjt27fLw8Pjqsud9+3bp+7du+vnn3+2jR05ckQNGjRw8BUDwK2BO74AgFtKcHCwPv/8cwUGBhabK1pWeqXWrVtf9Q7a2bNn9eWXXxb7UqLBgwdr3LhxioiIsBv39/fX5MmTFRERodWrV0u6vNT5yp8zkqTnn3++2LVGjRqlpKQkNWrUqNhcu3btNHz4cI0aNUoeHh66ePGixo8fX+K3S0vSM888ox49eqhTp05ydXXV4sWLNW/ePC1cuFDu7u7Kz8/XSy+9JD8/P0n/vwz5SkFBQRo+fLjefPNNxcbGKjg4WFWqVFFOTo4CAwNLXHobHBys7OzsEn92p7RrPPjgg3Zj9evXV/fu3Uv9S4Df2rhxY7Gl3YMHD5afn5/tcy8oKFB2drZ69uyp8PDwcp236PX8tqcKCwu1bt06LVu2zG7f/v3765FHHtGFCxc0evRorVy5UkOGDJGXl5fy8/NVt25drVmzxvZTSmX1ZGRkpMaNG2cLyp6enle9Ww4AtyqLtWgtDQAAAAAABmKpMwAAAADAaARfAAAAAIDRCL4AAAAAAKMRfAEAAAAARiP4AgAAAACMRvAFAAAAABiN4AsAAAAAMBrBFwAAAABgtP8DfqYBVKo+6XgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}